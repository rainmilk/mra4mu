{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils\n",
    "from agents import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seeds',          type=int,       default=[2023, 2024, 2025])\n",
    "parser.add_argument('--dataset',        type=str,       default='svhn')\n",
    "parser.add_argument('--batch_size',     type=int,       default=512)\n",
    "parser.add_argument('--model_name',     type=str,       default='vgg11')\n",
    "parser.add_argument('--retrain',        type=bool,      default=False)\n",
    "parser.add_argument('--unlearn_class',  type=list,      default=3)\n",
    "args = parser.parse_args(\"\")\n",
    "args.time_str = time.strftime(\"%m-%d-%H-%M\", time.localtime())\n",
    "if args.dataset.lower() == 'fmnist':\n",
    "    args.n_channels = 1\n",
    "else:\n",
    "    args.n_channels = 3\n",
    "\n",
    "if args.dataset.lower() == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "else:\n",
    "    args.num_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    1: {'unlean_class': [3], 'arxiv_name': '12-19-23-47'},\n",
    "    2: {'unlean_class': [1, 3], 'arxiv_name': '12-27-22-37'},\n",
    "    3: {'unlean_class': [1, 3, 5], 'arxiv_name': '12-28-11-53'},\n",
    "    4: {'unlean_class': [1, 5, 8, 3], 'arxiv_name': '12-27-20-22'},\n",
    "    5: {'unlean_class': [1, 5, 6, 7, 9], 'arxiv_name': '12-28-00-01'},\n",
    "}\n",
    "num_unlearn = 1\n",
    "args.unlean_class = dict[num_unlearn]['unlean_class']\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "\n",
    "arxiv_name_o = '12-15-02-49'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 1 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, [0.7359], 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "0.9203, 0.8539, 0.8166, [0.5746], 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5746, Acc_r: 0.8281\n",
      "0.8481, 0.8837, 0.9371, [0.6891], 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6891, Acc_r: 0.8150\n",
      "------------ Retrained model ------------\n",
      "No retrained model\n",
      "Original model Acc_f: 66.66 \\pm 6.78\n",
      "Original model Acc_r: 82.28 \\pm 0.56\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 0.00 \\pm 0.00\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 1\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n",
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 29/576\n",
      "Layer 3 : 106/1152\n",
      "Layer 4 : 530/2304\n",
      "Layer 5 : 185/2304\n",
      "Layer 6 : 214/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 3/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mu/utils.py:111: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  accumulated_sval = (sval_total-sval_hat)/sval_total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 33/576\n",
      "Layer 3 : 136/1152\n",
      "Layer 4 : 738/2304\n",
      "Layer 5 : 352/2304\n",
      "Layer 6 : 441/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 5/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 38/576\n",
      "Layer 3 : 186/1152\n",
      "Layer 4 : 992/2304\n",
      "Layer 5 : 522/2304\n",
      "Layer 6 : 684/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 8/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1379/3904146133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.97\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcls_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmerged_feat_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_GPM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_feat_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mproj_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_basis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer_basis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged_feat_mat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mProj_mat_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mu/utils.py\u001b[0m in \u001b[0;36mupdate_GPM\u001b[0;34m(mat_list, threshold, feature_list)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mU1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVh1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0msval_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mS1\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mact_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0msval_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, 0.9712, 0.9624, [0.9167], 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9167, Acc_r: 0.9567\n",
      "[train] epoch 0, batch 14, loss 3.767608404159546\n",
      "0.9736, 0.9743, 0.9600, [0.6721], 0.9719, 0.9564, 0.9464, 0.9287, 0.9108, 0.9611, Acc_f: 0.6721, Acc_r: 0.9537\n",
      "[train] epoch 1, batch 14, loss 2.866029977798462\n",
      "0.9759, 0.9751, 0.9489, [0.2318], 0.9742, 0.9543, 0.9393, 0.9143, 0.8837, 0.9561, Acc_f: 0.2318, Acc_r: 0.9469\n",
      "[train] epoch 2, batch 14, loss 2.621795415878296\n",
      "0.9765, 0.9751, 0.9426, [0.1044], 0.9750, 0.9530, 0.9358, 0.9099, 0.8729, 0.9524, Acc_f: 0.1044, Acc_r: 0.9437\n",
      "[train] epoch 3, batch 14, loss 2.5161426067352295\n",
      "0.9765, 0.9747, 0.9397, [0.0527], 0.9750, 0.9551, 0.9358, 0.9104, 0.8687, 0.9448, Acc_f: 0.0527, Acc_r: 0.9423\n",
      "[train] epoch 4, batch 14, loss 2.4118316173553467\n",
      "0.9759, 0.9749, 0.9397, [0.0288], 0.9750, 0.9572, 0.9358, 0.9094, 0.8645, 0.9404, Acc_f: 0.0288, Acc_r: 0.9414\n",
      "[train] epoch 5, batch 14, loss 2.4532108306884766\n",
      "0.9765, 0.9747, 0.9400, [0.0167], 0.9750, 0.9581, 0.9353, 0.9108, 0.8639, 0.9304, Acc_f: 0.0167, Acc_r: 0.9405\n",
      "0.9639, 0.9722, 0.9658, [0.9129], 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9129, Acc_r: 0.9539\n",
      "[train] epoch 0, batch 14, loss 4.623106956481934\n",
      "0.9656, 0.9767, 0.9655, [0.7058], 0.9719, 0.9551, 0.9661, 0.9287, 0.9084, 0.9542, Acc_f: 0.7058, Acc_r: 0.9547\n",
      "[train] epoch 1, batch 14, loss 3.273163318634033\n",
      "0.9679, 0.9784, 0.9614, [0.3685], 0.9738, 0.9530, 0.9575, 0.9168, 0.8940, 0.9555, Acc_f: 0.3685, Acc_r: 0.9509\n",
      "[train] epoch 2, batch 14, loss 2.769610643386841\n",
      "0.9679, 0.9765, 0.9607, [0.2266], 0.9750, 0.9543, 0.9519, 0.9153, 0.8807, 0.9542, Acc_f: 0.2266, Acc_r: 0.9485\n",
      "[train] epoch 3, batch 14, loss 2.6396241188049316\n",
      "0.9679, 0.9757, 0.9600, [0.1454], 0.9754, 0.9551, 0.9494, 0.9148, 0.8651, 0.9505, Acc_f: 0.1454, Acc_r: 0.9460\n",
      "[train] epoch 4, batch 14, loss 2.5557520389556885\n",
      "0.9673, 0.9751, 0.9593, [0.0888], 0.9762, 0.9530, 0.9474, 0.9133, 0.8548, 0.9467, Acc_f: 0.0888, Acc_r: 0.9437\n",
      "[train] epoch 5, batch 14, loss 2.4527218341827393\n",
      "0.9667, 0.9747, 0.9593, [0.0593], 0.9770, 0.9547, 0.9444, 0.9158, 0.8452, 0.9442, Acc_f: 0.0593, Acc_r: 0.9424\n",
      "0.9616, 0.9645, 0.9655, [0.9094], 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9094, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 14, loss 3.657120943069458\n",
      "0.9673, 0.9704, 0.9573, [0.5160], 0.9762, 0.9543, 0.9489, 0.9262, 0.9175, 0.9492, Acc_f: 0.5160, Acc_r: 0.9519\n",
      "[train] epoch 1, batch 14, loss 2.7880451679229736\n",
      "0.9708, 0.9710, 0.9369, [0.1912], 0.9798, 0.9379, 0.9332, 0.9173, 0.8831, 0.9373, Acc_f: 0.1912, Acc_r: 0.9408\n",
      "[train] epoch 2, batch 14, loss 2.5535547733306885\n",
      "0.9708, 0.9702, 0.9337, [0.0947], 0.9814, 0.9329, 0.9256, 0.9153, 0.8602, 0.9304, Acc_f: 0.0947, Acc_r: 0.9356\n",
      "[train] epoch 3, batch 14, loss 2.489861011505127\n",
      "0.9708, 0.9700, 0.9364, [0.0430], 0.9814, 0.9316, 0.9181, 0.9163, 0.8410, 0.9223, Acc_f: 0.0430, Acc_r: 0.9320\n",
      "[train] epoch 4, batch 14, loss 2.4533355236053467\n",
      "0.9708, 0.9694, 0.9342, [0.0243], 0.9818, 0.9299, 0.9125, 0.9153, 0.8253, 0.9154, Acc_f: 0.0243, Acc_r: 0.9283\n",
      "[train] epoch 5, batch 14, loss 2.3768725395202637\n",
      "0.9725, 0.9694, 0.9335, [0.0128], 0.9822, 0.9304, 0.9090, 0.9153, 0.8127, 0.9129, Acc_f: 0.0128, Acc_r: 0.9264\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(6):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label + Subspace Acc_f: 2.96 \\pm 2.11\n",
      "Random label + Subspace Acc_r: 93.64 \\pm 0.71\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9405, 0.9424, 0.9264])\n",
    "Acc_f = 100*np.array([0.0167, 0.0593, 0.0128])\n",
    "\n",
    "print(f'Random label + Subspace Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label + Subspace Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, 0.9712, 0.9624, [0.9167], 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9167, Acc_r: 0.9567\n",
      "[train] epoch 0, batch 14, loss 0.8711209297180176\n",
      "0.9713, 0.9792, 0.9706, [0.0160], 0.9620, 0.9685, 0.9353, 0.9227, 0.9048, 0.9618, Acc_f: 0.0160, Acc_r: 0.9529\n",
      "[train] epoch 1, batch 14, loss 0.5616824626922607\n",
      "0.9719, 0.9800, 0.9723, [0.0024], 0.9600, 0.9669, 0.9393, 0.9257, 0.9193, 0.9586, Acc_f: 0.0024, Acc_r: 0.9549\n",
      "[train] epoch 2, batch 14, loss 0.47658926248550415\n",
      "0.9713, 0.9802, 0.9718, [0.0007], 0.9596, 0.9664, 0.9423, 0.9257, 0.9265, 0.9580, Acc_f: 0.0007, Acc_r: 0.9558\n",
      "[train] epoch 3, batch 14, loss 0.3886551558971405\n",
      "0.9713, 0.9804, 0.9723, [0.0000], 0.9580, 0.9656, 0.9439, 0.9277, 0.9307, 0.9580, Acc_f: 0.0000, Acc_r: 0.9564\n",
      "[train] epoch 4, batch 14, loss 0.4081674814224243\n",
      "0.9719, 0.9804, 0.9723, [0.0000], 0.9576, 0.9648, 0.9423, 0.9297, 0.9337, 0.9580, Acc_f: 0.0000, Acc_r: 0.9567\n",
      "[train] epoch 5, batch 14, loss 0.40309152007102966\n",
      "0.9719, 0.9823, 0.9689, [0.0000], 0.9552, 0.9648, 0.9423, 0.9242, 0.9325, 0.9586, Acc_f: 0.0000, Acc_r: 0.9556\n",
      "[train] epoch 6, batch 14, loss 0.3204081952571869\n",
      "0.9719, 0.9820, 0.9694, [0.0000], 0.9556, 0.9656, 0.9418, 0.9282, 0.9349, 0.9586, Acc_f: 0.0000, Acc_r: 0.9564\n",
      "[train] epoch 7, batch 14, loss 0.35515087842941284\n",
      "0.9731, 0.9800, 0.9720, [0.0000], 0.9576, 0.9656, 0.9418, 0.9351, 0.9361, 0.9592, Acc_f: 0.0000, Acc_r: 0.9578\n",
      "[train] epoch 8, batch 14, loss 0.3298690915107727\n",
      "0.9731, 0.9810, 0.9708, [0.0000], 0.9560, 0.9639, 0.9423, 0.9316, 0.9355, 0.9592, Acc_f: 0.0000, Acc_r: 0.9571\n",
      "[train] epoch 9, batch 14, loss 0.2984052002429962\n",
      "0.9725, 0.9792, 0.9730, [0.0000], 0.9576, 0.9643, 0.9423, 0.9361, 0.9361, 0.9592, Acc_f: 0.0000, Acc_r: 0.9578\n",
      "[train] epoch 10, batch 14, loss 0.25077271461486816\n",
      "0.9725, 0.9786, 0.9713, [0.0000], 0.9584, 0.9648, 0.9428, 0.9396, 0.9373, 0.9599, Acc_f: 0.0000, Acc_r: 0.9584\n",
      "[train] epoch 11, batch 14, loss 0.32129645347595215\n",
      "0.9725, 0.9774, 0.9728, [0.0000], 0.9604, 0.9652, 0.9423, 0.9416, 0.9386, 0.9599, Acc_f: 0.0000, Acc_r: 0.9590\n",
      "[train] epoch 12, batch 14, loss 0.38813281059265137\n",
      "0.9725, 0.9804, 0.9696, [0.0000], 0.9576, 0.9631, 0.9439, 0.9361, 0.9380, 0.9599, Acc_f: 0.0000, Acc_r: 0.9579\n",
      "[train] epoch 13, batch 14, loss 0.27250269055366516\n",
      "0.9719, 0.9776, 0.9716, [0.0000], 0.9600, 0.9635, 0.9428, 0.9416, 0.9380, 0.9599, Acc_f: 0.0000, Acc_r: 0.9585\n",
      "[train] epoch 14, batch 14, loss 0.29115360975265503\n",
      "0.9731, 0.9788, 0.9706, [0.0000], 0.9608, 0.9618, 0.9433, 0.9391, 0.9373, 0.9599, Acc_f: 0.0000, Acc_r: 0.9583\n",
      "0.9639, 0.9722, 0.9658, [0.9129], 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9129, Acc_r: 0.9539\n",
      "[train] epoch 0, batch 14, loss 0.6524872183799744\n",
      "0.9576, 0.9818, 0.9663, [0.0298], 0.9651, 0.9744, 0.9560, 0.9039, 0.9018, 0.9505, Acc_f: 0.0298, Acc_r: 0.9508\n",
      "[train] epoch 1, batch 14, loss 0.46918386220932007\n",
      "0.9587, 0.9814, 0.9716, [0.0080], 0.9627, 0.9694, 0.9611, 0.8990, 0.9199, 0.9473, Acc_f: 0.0080, Acc_r: 0.9523\n",
      "[train] epoch 2, batch 14, loss 0.3803301155567169\n",
      "0.9599, 0.9818, 0.9716, [0.0049], 0.9600, 0.9660, 0.9621, 0.8960, 0.9319, 0.9467, Acc_f: 0.0049, Acc_r: 0.9529\n",
      "[train] epoch 3, batch 14, loss 0.3487563133239746\n",
      "0.9604, 0.9822, 0.9723, [0.0028], 0.9608, 0.9652, 0.9616, 0.8960, 0.9355, 0.9442, Acc_f: 0.0028, Acc_r: 0.9531\n",
      "[train] epoch 4, batch 14, loss 0.35968074202537537\n",
      "0.9604, 0.9818, 0.9728, [0.0010], 0.9612, 0.9648, 0.9616, 0.8945, 0.9367, 0.9442, Acc_f: 0.0010, Acc_r: 0.9531\n",
      "[train] epoch 5, batch 14, loss 0.28122130036354065\n",
      "0.9610, 0.9820, 0.9723, [0.0003], 0.9612, 0.9648, 0.9616, 0.8940, 0.9367, 0.9442, Acc_f: 0.0003, Acc_r: 0.9531\n",
      "[train] epoch 6, batch 14, loss 0.3111727833747864\n",
      "0.9616, 0.9816, 0.9723, [0.0003], 0.9627, 0.9618, 0.9621, 0.8965, 0.9398, 0.9455, Acc_f: 0.0003, Acc_r: 0.9538\n",
      "[train] epoch 7, batch 14, loss 0.30505919456481934\n",
      "0.9616, 0.9808, 0.9723, [0.0003], 0.9647, 0.9648, 0.9626, 0.8975, 0.9392, 0.9436, Acc_f: 0.0003, Acc_r: 0.9541\n",
      "[train] epoch 8, batch 14, loss 0.2750190496444702\n",
      "0.9616, 0.9808, 0.9718, [0.0003], 0.9651, 0.9627, 0.9616, 0.8960, 0.9440, 0.9448, Acc_f: 0.0003, Acc_r: 0.9543\n",
      "[train] epoch 9, batch 14, loss 0.2611444294452667\n",
      "0.9610, 0.9798, 0.9713, [0.0003], 0.9667, 0.9631, 0.9605, 0.9000, 0.9446, 0.9442, Acc_f: 0.0003, Acc_r: 0.9546\n",
      "[train] epoch 10, batch 14, loss 0.2564266622066498\n",
      "0.9616, 0.9808, 0.9718, [0.0000], 0.9663, 0.9614, 0.9626, 0.8985, 0.9446, 0.9455, Acc_f: 0.0000, Acc_r: 0.9548\n",
      "[train] epoch 11, batch 14, loss 0.25810980796813965\n",
      "0.9633, 0.9808, 0.9713, [0.0000], 0.9675, 0.9618, 0.9605, 0.8995, 0.9446, 0.9455, Acc_f: 0.0000, Acc_r: 0.9550\n",
      "[train] epoch 12, batch 14, loss 0.1987508237361908\n",
      "0.9627, 0.9804, 0.9711, [0.0000], 0.9679, 0.9610, 0.9636, 0.8990, 0.9440, 0.9448, Acc_f: 0.0000, Acc_r: 0.9549\n",
      "[train] epoch 13, batch 14, loss 0.23071403801441193\n",
      "0.9627, 0.9812, 0.9711, [0.0000], 0.9683, 0.9597, 0.9646, 0.8955, 0.9434, 0.9436, Acc_f: 0.0000, Acc_r: 0.9544\n",
      "[train] epoch 14, batch 14, loss 0.22911785542964935\n",
      "0.9639, 0.9800, 0.9718, [0.0000], 0.9687, 0.9597, 0.9641, 0.9004, 0.9428, 0.9436, Acc_f: 0.0000, Acc_r: 0.9550\n",
      "0.9616, 0.9645, 0.9655, [0.9094], 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9094, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 14, loss 0.7691830396652222\n",
      "0.9587, 0.9667, 0.9752, [0.0180], 0.9715, 0.9740, 0.9489, 0.9208, 0.9289, 0.9536, Acc_f: 0.0180, Acc_r: 0.9554\n",
      "[train] epoch 1, batch 14, loss 0.48486486077308655\n",
      "0.9558, 0.9663, 0.9752, [0.0045], 0.9711, 0.9736, 0.9575, 0.9198, 0.9410, 0.9517, Acc_f: 0.0045, Acc_r: 0.9569\n",
      "[train] epoch 2, batch 14, loss 0.414700448513031\n",
      "0.9553, 0.9690, 0.9747, [0.0014], 0.9711, 0.9719, 0.9585, 0.9153, 0.9476, 0.9492, Acc_f: 0.0014, Acc_r: 0.9570\n",
      "[train] epoch 3, batch 14, loss 0.40690165758132935\n",
      "0.9576, 0.9690, 0.9740, [0.0014], 0.9707, 0.9719, 0.9575, 0.9183, 0.9476, 0.9473, Acc_f: 0.0014, Acc_r: 0.9571\n",
      "[train] epoch 4, batch 14, loss 0.372738242149353\n",
      "0.9581, 0.9688, 0.9737, [0.0010], 0.9711, 0.9715, 0.9555, 0.9193, 0.9482, 0.9480, Acc_f: 0.0010, Acc_r: 0.9571\n",
      "[train] epoch 5, batch 14, loss 0.3094901442527771\n",
      "0.9587, 0.9710, 0.9737, [0.0003], 0.9711, 0.9702, 0.9560, 0.9212, 0.9470, 0.9467, Acc_f: 0.0003, Acc_r: 0.9573\n",
      "[train] epoch 6, batch 14, loss 0.2594408690929413\n",
      "0.9587, 0.9708, 0.9737, [0.0003], 0.9707, 0.9711, 0.9560, 0.9217, 0.9482, 0.9448, Acc_f: 0.0003, Acc_r: 0.9573\n",
      "[train] epoch 7, batch 14, loss 0.3061419129371643\n",
      "0.9581, 0.9714, 0.9730, [0.0003], 0.9699, 0.9706, 0.9600, 0.9203, 0.9476, 0.9455, Acc_f: 0.0003, Acc_r: 0.9574\n",
      "[train] epoch 8, batch 14, loss 0.2887010872364044\n",
      "0.9599, 0.9706, 0.9735, [0.0000], 0.9723, 0.9702, 0.9600, 0.9227, 0.9500, 0.9442, Acc_f: 0.0000, Acc_r: 0.9582\n",
      "[train] epoch 9, batch 14, loss 0.264545738697052\n",
      "0.9604, 0.9708, 0.9730, [0.0000], 0.9727, 0.9711, 0.9580, 0.9252, 0.9488, 0.9448, Acc_f: 0.0000, Acc_r: 0.9583\n",
      "[train] epoch 10, batch 14, loss 0.27013471722602844\n",
      "0.9610, 0.9714, 0.9723, [0.0000], 0.9723, 0.9698, 0.9590, 0.9227, 0.9494, 0.9429, Acc_f: 0.0000, Acc_r: 0.9579\n",
      "[train] epoch 11, batch 14, loss 0.2329646497964859\n",
      "0.9599, 0.9716, 0.9716, [0.0000], 0.9727, 0.9698, 0.9590, 0.9227, 0.9494, 0.9442, Acc_f: 0.0000, Acc_r: 0.9579\n",
      "[train] epoch 12, batch 14, loss 0.26221004128456116\n",
      "0.9604, 0.9712, 0.9713, [0.0000], 0.9727, 0.9698, 0.9595, 0.9242, 0.9488, 0.9448, Acc_f: 0.0000, Acc_r: 0.9581\n",
      "[train] epoch 13, batch 14, loss 0.23307158052921295\n",
      "0.9599, 0.9712, 0.9718, [0.0000], 0.9723, 0.9694, 0.9590, 0.9252, 0.9494, 0.9448, Acc_f: 0.0000, Acc_r: 0.9581\n",
      "[train] epoch 14, batch 14, loss 0.2525005042552948\n",
      "0.9610, 0.9710, 0.9720, [0.0000], 0.9727, 0.9685, 0.9580, 0.9262, 0.9488, 0.9442, Acc_f: 0.0000, Acc_r: 0.9580\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.00 \\pm 0.00\n",
      "UNSC Acc_r: 95.74 \\pm 0.17\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9590, 0.9550, 0.9583])\n",
    "Acc_f = 100*np.array([0.0000, 0.0000, 0.0000])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2266666666666642, 0.09392668535737063)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = (100*np.array([0.9590, 0.9550, 0.9583])-100*np.array([0.9567, 0.9539, 0.9549]))\n",
    "a.mean(), a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, 0.9712, 0.9624, [0.9167], 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9167, Acc_r: 0.9567\n",
      "[train] epoch 0, batch 14, loss 5.851011276245117\n",
      "0.9731, 0.9774, 0.9605, [0.8543], 0.9671, 0.9534, 0.9433, 0.9326, 0.9187, 0.9599, Acc_f: 0.8543, Acc_r: 0.9540\n",
      "[train] epoch 1, batch 14, loss 3.5684101581573486\n",
      "0.9725, 0.9847, 0.9369, [0.6020], 0.9560, 0.9442, 0.9160, 0.8534, 0.8602, 0.9423, Acc_f: 0.6020, Acc_r: 0.9296\n",
      "[train] epoch 2, batch 14, loss 2.9119174480438232\n",
      "0.9685, 0.9867, 0.8802, [0.2623], 0.9493, 0.9023, 0.8680, 0.7504, 0.7765, 0.8489, Acc_f: 0.2623, Acc_r: 0.8812\n",
      "[train] epoch 3, batch 14, loss 2.694432258605957\n",
      "0.9644, 0.9874, 0.8260, [0.1329], 0.9465, 0.8595, 0.8235, 0.6667, 0.7078, 0.7687, Acc_f: 0.1329, Acc_r: 0.8389\n",
      "[train] epoch 4, batch 14, loss 2.5529770851135254\n",
      "0.9599, 0.9880, 0.7848, [0.0697], 0.9429, 0.8305, 0.7931, 0.6147, 0.6560, 0.7053, Acc_f: 0.0697, Acc_r: 0.8084\n",
      "[train] epoch 5, batch 14, loss 2.6163973808288574\n",
      "0.9581, 0.9878, 0.7549, [0.0423], 0.9409, 0.8100, 0.7714, 0.5740, 0.5994, 0.6571, Acc_f: 0.0423, Acc_r: 0.7837\n",
      "[train] epoch 6, batch 14, loss 2.474691152572632\n",
      "0.9576, 0.9874, 0.7317, [0.0295], 0.9370, 0.8024, 0.7521, 0.5503, 0.5747, 0.6251, Acc_f: 0.0295, Acc_r: 0.7687\n",
      "[train] epoch 7, batch 14, loss 2.4430718421936035\n",
      "0.9547, 0.9865, 0.7115, [0.0180], 0.9342, 0.7966, 0.7400, 0.5285, 0.5452, 0.5962, Acc_f: 0.0180, Acc_r: 0.7548\n",
      "[train] epoch 8, batch 14, loss 2.440791606903076\n",
      "0.9530, 0.9863, 0.6997, [0.0121], 0.9330, 0.7936, 0.7279, 0.5141, 0.5265, 0.5586, Acc_f: 0.0121, Acc_r: 0.7436\n",
      "[train] epoch 9, batch 14, loss 2.403254270553589\n",
      "0.9518, 0.9863, 0.6838, [0.0087], 0.9291, 0.7940, 0.7132, 0.4953, 0.5066, 0.5285, Acc_f: 0.0087, Acc_r: 0.7321\n",
      "0.9639, 0.9722, 0.9658, [0.9129], 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9129, Acc_r: 0.9539\n",
      "[train] epoch 0, batch 14, loss 6.748849391937256\n",
      "0.9622, 0.9757, 0.9660, [0.8539], 0.9695, 0.9564, 0.9681, 0.9267, 0.9139, 0.9524, Acc_f: 0.8539, Acc_r: 0.9545\n",
      "[train] epoch 1, batch 14, loss 4.845348358154297\n",
      "0.9627, 0.9808, 0.9629, [0.7262], 0.9699, 0.9551, 0.9600, 0.9128, 0.8958, 0.9555, Acc_f: 0.7262, Acc_r: 0.9506\n",
      "[train] epoch 2, batch 14, loss 3.6643013954162598\n",
      "0.9644, 0.9820, 0.9549, [0.4358], 0.9679, 0.9459, 0.9398, 0.8965, 0.8578, 0.9530, Acc_f: 0.4358, Acc_r: 0.9402\n",
      "[train] epoch 3, batch 14, loss 3.054218292236328\n",
      "0.9627, 0.9829, 0.9402, [0.2443], 0.9687, 0.9358, 0.9135, 0.8757, 0.8048, 0.9335, Acc_f: 0.2443, Acc_r: 0.9242\n",
      "[train] epoch 4, batch 14, loss 2.9039621353149414\n",
      "0.9604, 0.9829, 0.9277, [0.1502], 0.9695, 0.9270, 0.8897, 0.8588, 0.7488, 0.9028, Acc_f: 0.1502, Acc_r: 0.9075\n",
      "[train] epoch 5, batch 14, loss 2.8205783367156982\n",
      "0.9587, 0.9818, 0.9147, [0.0961], 0.9723, 0.9186, 0.8675, 0.8395, 0.7054, 0.8734, Acc_f: 0.0961, Acc_r: 0.8924\n",
      "[train] epoch 6, batch 14, loss 2.693566083908081\n",
      "0.9558, 0.9794, 0.9046, [0.0670], 0.9738, 0.9148, 0.8523, 0.8227, 0.6602, 0.8458, Acc_f: 0.0670, Acc_r: 0.8788\n",
      "[train] epoch 7, batch 14, loss 2.567460060119629\n",
      "0.9536, 0.9788, 0.8973, [0.0479], 0.9738, 0.9098, 0.8376, 0.8058, 0.6277, 0.8207, Acc_f: 0.0479, Acc_r: 0.8672\n",
      "[train] epoch 8, batch 14, loss 2.603320837020874\n",
      "0.9507, 0.9763, 0.8918, [0.0330], 0.9742, 0.9102, 0.8235, 0.7925, 0.5940, 0.7987, Acc_f: 0.0330, Acc_r: 0.8569\n",
      "[train] epoch 9, batch 14, loss 2.4714431762695312\n",
      "0.9461, 0.9747, 0.8884, [0.0257], 0.9750, 0.9107, 0.8113, 0.7831, 0.5723, 0.7793, Acc_f: 0.0257, Acc_r: 0.8490\n",
      "0.9616, 0.9645, 0.9655, [0.9094], 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9094, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 14, loss 5.181240558624268\n",
      "0.9679, 0.9700, 0.9602, [0.7942], 0.9715, 0.9576, 0.9474, 0.9188, 0.9066, 0.9536, Acc_f: 0.7942, Acc_r: 0.9504\n",
      "[train] epoch 1, batch 14, loss 3.252854585647583\n",
      "0.9564, 0.9788, 0.8125, [0.3279], 0.9612, 0.8863, 0.8619, 0.8598, 0.7880, 0.8727, Acc_f: 0.3279, Acc_r: 0.8864\n",
      "[train] epoch 2, batch 14, loss 2.885314702987671\n",
      "0.9438, 0.9816, 0.6549, [0.1485], 0.9524, 0.7974, 0.7719, 0.8063, 0.6633, 0.7680, Acc_f: 0.1485, Acc_r: 0.8155\n",
      "[train] epoch 3, batch 14, loss 2.567526340484619\n",
      "0.9386, 0.9818, 0.5792, [0.0874], 0.9509, 0.7525, 0.7188, 0.7613, 0.5819, 0.7129, Acc_f: 0.0874, Acc_r: 0.7753\n",
      "[train] epoch 4, batch 14, loss 2.542083978652954\n",
      "0.9358, 0.9812, 0.5406, [0.0590], 0.9520, 0.7286, 0.6773, 0.7340, 0.5229, 0.6803, Acc_f: 0.0590, Acc_r: 0.7503\n",
      "[train] epoch 5, batch 14, loss 2.502295732498169\n",
      "0.9341, 0.9806, 0.5257, [0.0392], 0.9520, 0.7156, 0.6525, 0.7147, 0.4729, 0.6721, Acc_f: 0.0392, Acc_r: 0.7356\n",
      "[train] epoch 6, batch 14, loss 2.4658751487731934\n",
      "0.9329, 0.9796, 0.5141, [0.0250], 0.9524, 0.7106, 0.6328, 0.6994, 0.4295, 0.6633, Acc_f: 0.0250, Acc_r: 0.7238\n",
      "[train] epoch 7, batch 14, loss 2.4405977725982666\n",
      "0.9335, 0.9782, 0.5081, [0.0167], 0.9524, 0.7114, 0.6141, 0.6895, 0.3928, 0.6577, Acc_f: 0.0167, Acc_r: 0.7153\n",
      "[train] epoch 8, batch 14, loss 2.398324728012085\n",
      "0.9335, 0.9767, 0.5020, [0.0115], 0.9532, 0.7164, 0.6029, 0.6795, 0.3687, 0.6558, Acc_f: 0.0115, Acc_r: 0.7099\n",
      "[train] epoch 9, batch 14, loss 2.371405601501465\n",
      "0.9323, 0.9755, 0.4951, [0.0087], 0.9532, 0.7219, 0.5893, 0.6691, 0.3512, 0.6545, Acc_f: 0.0087, Acc_r: 0.7047\n"
     ]
    }
   ],
   "source": [
    "Acc_r, Acc_f = np.zeros(3), np.zeros(3)\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        Acc_r[i], Acc_f[i] = test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label Acc_f: 1.75 \\pm 0.70\n",
      "Random label Acc_r: 76.95 \\pm 5.98\n"
     ]
    }
   ],
   "source": [
    "Acc_f = 100*np.array([0.0180, 0.0257, 0.0087])\n",
    "Acc_r = 100*np.array([0.7548, 0.8490, 0.7047])\n",
    "print(f'Random label Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "0.9719, 0.9712, 0.9624, [0.9167], 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9167, Acc_r: 0.9567\n",
      "[train] epoch 0, batch 14, loss 2.40285062789917\n",
      "0.9553, 0.9922, 0.8265, [0.1513], 0.9291, 0.8775, 0.8270, 0.6568, 0.7596, 0.7592, Acc_f: 0.1513, Acc_r: 0.8426\n",
      "[train] epoch 1, batch 14, loss 2.193714141845703\n",
      "0.9404, 0.9941, 0.7619, [0.0545], 0.9005, 0.8658, 0.7618, 0.5597, 0.7054, 0.5774, Acc_f: 0.0545, Acc_r: 0.7852\n",
      "[train] epoch 2, batch 14, loss 2.172280788421631\n",
      "0.9278, 0.9945, 0.7293, [0.0295], 0.8720, 0.8842, 0.7294, 0.5017, 0.6831, 0.4803, Acc_f: 0.0295, Acc_r: 0.7558\n",
      "[train] epoch 3, batch 14, loss 2.1772871017456055\n",
      "0.9163, 0.9949, 0.6802, [0.0142], 0.8450, 0.8893, 0.6970, 0.4522, 0.6645, 0.3737, Acc_f: 0.0142, Acc_r: 0.7237\n",
      "[train] epoch 4, batch 14, loss 2.1005611419677734\n",
      "0.9123, 0.9953, 0.6385, [0.0097], 0.8161, 0.8859, 0.6763, 0.4185, 0.6542, 0.3122, Acc_f: 0.0097, Acc_r: 0.7010\n",
      "[train] epoch 5, batch 14, loss 2.0606777667999268\n",
      "0.9083, 0.9953, 0.6247, [0.0073], 0.8002, 0.8872, 0.6611, 0.4002, 0.6500, 0.2746, Acc_f: 0.0073, Acc_r: 0.6891\n",
      "[train] epoch 6, batch 14, loss 2.14465069770813\n",
      "0.9100, 0.9959, 0.6148, [0.0073], 0.7887, 0.8851, 0.6571, 0.3962, 0.6566, 0.2608, Acc_f: 0.0073, Acc_r: 0.6850\n",
      "[train] epoch 7, batch 14, loss 2.049461841583252\n",
      "0.9111, 0.9959, 0.6201, [0.0076], 0.7769, 0.8867, 0.6550, 0.3933, 0.6596, 0.2564, Acc_f: 0.0076, Acc_r: 0.6839\n",
      "[train] epoch 8, batch 14, loss 2.0727012157440186\n",
      "0.9094, 0.9961, 0.6136, [0.0066], 0.7697, 0.8838, 0.6515, 0.3903, 0.6590, 0.2439, Acc_f: 0.0066, Acc_r: 0.6797\n",
      "[train] epoch 9, batch 14, loss 2.0710456371307373\n",
      "0.9146, 0.9961, 0.6163, [0.0076], 0.7721, 0.8846, 0.6530, 0.3972, 0.6711, 0.2527, Acc_f: 0.0076, Acc_r: 0.6842\n",
      "[train] epoch 10, batch 14, loss 2.0920517444610596\n",
      "0.9157, 0.9959, 0.6242, [0.0080], 0.7713, 0.8897, 0.6525, 0.4012, 0.6747, 0.2539, Acc_f: 0.0080, Acc_r: 0.6866\n",
      "[train] epoch 11, batch 14, loss 2.0738162994384766\n",
      "0.9169, 0.9959, 0.6204, [0.0080], 0.7634, 0.8893, 0.6449, 0.3957, 0.6723, 0.2464, Acc_f: 0.0080, Acc_r: 0.6828\n",
      "[train] epoch 12, batch 14, loss 2.026437997817993\n",
      "0.9169, 0.9961, 0.6209, [0.0080], 0.7598, 0.8813, 0.6409, 0.3913, 0.6717, 0.2439, Acc_f: 0.0080, Acc_r: 0.6803\n",
      "[train] epoch 13, batch 14, loss 2.037412643432617\n",
      "0.9174, 0.9961, 0.6317, [0.0087], 0.7578, 0.8893, 0.6424, 0.3962, 0.6777, 0.2464, Acc_f: 0.0087, Acc_r: 0.6839\n",
      "[train] epoch 14, batch 14, loss 2.084766149520874\n",
      "0.9174, 0.9961, 0.6233, [0.0087], 0.7471, 0.8792, 0.6323, 0.3928, 0.6729, 0.2345, Acc_f: 0.0087, Acc_r: 0.6773\n",
      "[train] epoch 15, batch 14, loss 2.0280444622039795\n",
      "0.9169, 0.9963, 0.6180, [0.0087], 0.7408, 0.8725, 0.6222, 0.3868, 0.6723, 0.2295, Acc_f: 0.0087, Acc_r: 0.6728\n",
      "[train] epoch 16, batch 14, loss 2.071474075317383\n",
      "0.9174, 0.9965, 0.6267, [0.0101], 0.7412, 0.8771, 0.6237, 0.3918, 0.6753, 0.2395, Acc_f: 0.0101, Acc_r: 0.6766\n",
      "[train] epoch 17, batch 14, loss 2.0229275226593018\n",
      "0.9174, 0.9965, 0.6211, [0.0101], 0.7388, 0.8838, 0.6191, 0.3908, 0.6741, 0.2332, Acc_f: 0.0101, Acc_r: 0.6750\n",
      "[train] epoch 18, batch 14, loss 2.0826351642608643\n",
      "0.9197, 0.9965, 0.6235, [0.0108], 0.7337, 0.8846, 0.6181, 0.3913, 0.6747, 0.2370, Acc_f: 0.0108, Acc_r: 0.6755\n",
      "[train] epoch 19, batch 14, loss 1.9805574417114258\n",
      "0.9203, 0.9963, 0.6291, [0.0121], 0.7337, 0.8859, 0.6156, 0.3933, 0.6759, 0.2389, Acc_f: 0.0121, Acc_r: 0.6765\n",
      "============================================================\n",
      "0.9639, 0.9722, 0.9658, [0.9129], 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9129, Acc_r: 0.9539\n",
      "[train] epoch 0, batch 14, loss 2.5118629932403564\n",
      "0.9599, 0.9863, 0.9518, [0.3376], 0.9655, 0.9564, 0.9413, 0.8732, 0.8512, 0.9473, Acc_f: 0.3376, Acc_r: 0.9370\n",
      "[train] epoch 1, batch 14, loss 2.2977185249328613\n",
      "0.9518, 0.9855, 0.9390, [0.1672], 0.9675, 0.9627, 0.9186, 0.8385, 0.7934, 0.9016, Acc_f: 0.1672, Acc_r: 0.9176\n",
      "[train] epoch 2, batch 14, loss 2.165015697479248\n",
      "0.9455, 0.9849, 0.9337, [0.1273], 0.9671, 0.9685, 0.9105, 0.8182, 0.7669, 0.8564, Acc_f: 0.1273, Acc_r: 0.9058\n",
      "[train] epoch 3, batch 14, loss 2.1356074810028076\n",
      "0.9335, 0.9849, 0.9284, [0.1128], 0.9659, 0.9732, 0.9044, 0.7994, 0.7554, 0.8075, Acc_f: 0.1128, Acc_r: 0.8947\n",
      "[train] epoch 4, batch 14, loss 2.163886785507202\n",
      "0.9220, 0.9857, 0.9243, [0.0978], 0.9627, 0.9761, 0.8993, 0.7766, 0.7506, 0.7530, Acc_f: 0.0978, Acc_r: 0.8834\n",
      "[train] epoch 5, batch 14, loss 2.097500801086426\n",
      "0.9203, 0.9855, 0.9219, [0.0982], 0.9604, 0.9782, 0.8993, 0.7632, 0.7518, 0.7166, Acc_f: 0.0982, Acc_r: 0.8775\n",
      "[train] epoch 6, batch 14, loss 1.9920799732208252\n",
      "0.9180, 0.9865, 0.9159, [0.0968], 0.9580, 0.9786, 0.8993, 0.7489, 0.7500, 0.6834, Acc_f: 0.0968, Acc_r: 0.8710\n",
      "[train] epoch 7, batch 14, loss 2.035724401473999\n",
      "0.9151, 0.9876, 0.9115, [0.0958], 0.9552, 0.9786, 0.9024, 0.7390, 0.7524, 0.6589, Acc_f: 0.0958, Acc_r: 0.8668\n",
      "[train] epoch 8, batch 14, loss 2.0598738193511963\n",
      "0.9117, 0.9882, 0.9070, [0.0965], 0.9540, 0.9786, 0.9029, 0.7301, 0.7524, 0.6251, Acc_f: 0.0965, Acc_r: 0.8611\n",
      "[train] epoch 9, batch 14, loss 2.0467536449432373\n",
      "0.9117, 0.9888, 0.9084, [0.1093], 0.9536, 0.9773, 0.9064, 0.7306, 0.7669, 0.6263, Acc_f: 0.1093, Acc_r: 0.8633\n",
      "[train] epoch 10, batch 14, loss 2.0506751537323\n",
      "0.9111, 0.9890, 0.9079, [0.1173], 0.9532, 0.9773, 0.9100, 0.7266, 0.7675, 0.6188, Acc_f: 0.1173, Acc_r: 0.8624\n",
      "[train] epoch 11, batch 14, loss 2.0477664470672607\n",
      "0.9094, 0.9894, 0.9074, [0.1155], 0.9509, 0.9765, 0.9130, 0.7167, 0.7687, 0.6119, Acc_f: 0.1155, Acc_r: 0.8604\n",
      "[train] epoch 12, batch 14, loss 2.070802927017212\n",
      "0.9083, 0.9896, 0.9060, [0.1117], 0.9505, 0.9757, 0.9165, 0.7098, 0.7645, 0.5937, Acc_f: 0.1117, Acc_r: 0.8572\n",
      "[train] epoch 13, batch 14, loss 2.05439829826355\n",
      "0.9077, 0.9902, 0.9077, [0.1249], 0.9501, 0.9732, 0.9231, 0.7088, 0.7669, 0.6013, Acc_f: 0.1249, Acc_r: 0.8588\n",
      "[train] epoch 14, batch 14, loss 2.024358034133911\n",
      "0.9060, 0.9908, 0.9053, [0.1176], 0.9505, 0.9719, 0.9277, 0.6994, 0.7633, 0.5831, Acc_f: 0.1176, Acc_r: 0.8553\n",
      "[train] epoch 15, batch 14, loss 2.0362038612365723\n",
      "0.9037, 0.9912, 0.9050, [0.1211], 0.9509, 0.9715, 0.9307, 0.6919, 0.7608, 0.5781, Acc_f: 0.1211, Acc_r: 0.8537\n",
      "[train] epoch 16, batch 14, loss 1.9534976482391357\n",
      "0.9014, 0.9912, 0.9058, [0.1253], 0.9512, 0.9711, 0.9347, 0.6885, 0.7608, 0.5806, Acc_f: 0.1253, Acc_r: 0.8539\n",
      "[train] epoch 17, batch 14, loss 1.9740937948226929\n",
      "0.9014, 0.9912, 0.9060, [0.1332], 0.9512, 0.9698, 0.9378, 0.6845, 0.7639, 0.5856, Acc_f: 0.1332, Acc_r: 0.8546\n",
      "[train] epoch 18, batch 14, loss 2.017293930053711\n",
      "0.9002, 0.9916, 0.9062, [0.1395], 0.9509, 0.9681, 0.9378, 0.6800, 0.7651, 0.5887, Acc_f: 0.1395, Acc_r: 0.8543\n",
      "[train] epoch 19, batch 14, loss 1.9880894422531128\n",
      "0.8997, 0.9916, 0.9046, [0.1360], 0.9509, 0.9660, 0.9393, 0.6756, 0.7633, 0.5900, Acc_f: 0.1360, Acc_r: 0.8534\n",
      "============================================================\n",
      "0.9616, 0.9645, 0.9655, [0.9094], 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9094, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 14, loss 2.531710624694824\n",
      "0.9461, 0.9780, 0.7831, [0.2130], 0.9695, 0.9195, 0.8412, 0.8257, 0.7380, 0.7931, Acc_f: 0.2130, Acc_r: 0.8660\n",
      "[train] epoch 1, batch 14, loss 2.3153369426727295\n",
      "0.9358, 0.9733, 0.7544, [0.1464], 0.9782, 0.9459, 0.8164, 0.8049, 0.6705, 0.7078, Acc_f: 0.1464, Acc_r: 0.8430\n",
      "[train] epoch 2, batch 14, loss 2.2794137001037598\n",
      "0.9266, 0.9698, 0.7503, [0.1260], 0.9790, 0.9639, 0.8078, 0.7920, 0.6434, 0.6502, Acc_f: 0.1260, Acc_r: 0.8314\n",
      "[train] epoch 3, batch 14, loss 2.2217352390289307\n",
      "0.9237, 0.9655, 0.7508, [0.1273], 0.9790, 0.9753, 0.8053, 0.7870, 0.6410, 0.5912, Acc_f: 0.1273, Acc_r: 0.8243\n",
      "[train] epoch 4, batch 14, loss 2.2158424854278564\n",
      "0.9220, 0.9621, 0.7310, [0.1183], 0.9770, 0.9778, 0.7992, 0.7717, 0.6283, 0.5097, Acc_f: 0.1183, Acc_r: 0.8088\n",
      "[train] epoch 5, batch 14, loss 2.2064383029937744\n",
      "0.9232, 0.9586, 0.7361, [0.1266], 0.9806, 0.9778, 0.8053, 0.7647, 0.6223, 0.4614, Acc_f: 0.1266, Acc_r: 0.8033\n",
      "[train] epoch 6, batch 14, loss 2.177579164505005\n",
      "0.9237, 0.9572, 0.7358, [0.1391], 0.9818, 0.9773, 0.8103, 0.7623, 0.6337, 0.4307, Acc_f: 0.1391, Acc_r: 0.8014\n",
      "[train] epoch 7, batch 14, loss 2.168816328048706\n",
      "0.9260, 0.9539, 0.7455, [0.1554], 0.9826, 0.9773, 0.8144, 0.7632, 0.6416, 0.4019, Acc_f: 0.1554, Acc_r: 0.8007\n",
      "[train] epoch 8, batch 14, loss 2.1531014442443848\n",
      "0.9283, 0.9527, 0.7534, [0.1919], 0.9834, 0.9765, 0.8250, 0.7623, 0.6512, 0.3762, Acc_f: 0.1919, Acc_r: 0.8010\n",
      "[train] epoch 9, batch 14, loss 2.20566725730896\n",
      "0.9283, 0.9504, 0.7428, [0.1874], 0.9845, 0.9753, 0.8295, 0.7524, 0.6482, 0.3342, Acc_f: 0.1874, Acc_r: 0.7940\n",
      "[train] epoch 10, batch 14, loss 2.1466541290283203\n",
      "0.9278, 0.9494, 0.7325, [0.1763], 0.9845, 0.9757, 0.8381, 0.7464, 0.6464, 0.2991, Acc_f: 0.1763, Acc_r: 0.7889\n",
      "[train] epoch 11, batch 14, loss 2.1699507236480713\n",
      "0.9295, 0.9488, 0.7320, [0.1895], 0.9845, 0.9748, 0.8462, 0.7449, 0.6530, 0.2790, Acc_f: 0.1895, Acc_r: 0.7881\n",
      "[train] epoch 12, batch 14, loss 2.140695810317993\n",
      "0.9306, 0.9474, 0.7235, [0.1856], 0.9861, 0.9740, 0.8558, 0.7380, 0.6536, 0.2577, Acc_f: 0.1856, Acc_r: 0.7852\n",
      "[train] epoch 13, batch 14, loss 2.1274266242980957\n",
      "0.9318, 0.9461, 0.7211, [0.1947], 0.9869, 0.9723, 0.8624, 0.7340, 0.6554, 0.2401, Acc_f: 0.1947, Acc_r: 0.7834\n",
      "[train] epoch 14, batch 14, loss 2.2212159633636475\n",
      "0.9329, 0.9476, 0.7166, [0.2012], 0.9877, 0.9694, 0.8730, 0.7291, 0.6645, 0.2276, Acc_f: 0.2012, Acc_r: 0.7832\n",
      "[train] epoch 15, batch 14, loss 2.1522345542907715\n",
      "0.9329, 0.9476, 0.7197, [0.2058], 0.9877, 0.9685, 0.8781, 0.7335, 0.6735, 0.2207, Acc_f: 0.2058, Acc_r: 0.7847\n",
      "[train] epoch 16, batch 14, loss 2.1138572692871094\n",
      "0.9346, 0.9488, 0.7245, [0.2085], 0.9877, 0.9677, 0.8837, 0.7365, 0.6861, 0.2163, Acc_f: 0.2085, Acc_r: 0.7873\n",
      "[train] epoch 17, batch 14, loss 2.1176605224609375\n",
      "0.9346, 0.9486, 0.7243, [0.2120], 0.9877, 0.9669, 0.8872, 0.7355, 0.6892, 0.2056, Acc_f: 0.2120, Acc_r: 0.7866\n",
      "[train] epoch 18, batch 14, loss 2.1522436141967773\n",
      "0.9346, 0.9463, 0.7248, [0.2106], 0.9889, 0.9635, 0.8897, 0.7355, 0.6910, 0.1975, Acc_f: 0.2106, Acc_r: 0.7858\n",
      "[train] epoch 19, batch 14, loss 2.112426280975342\n",
      "0.9381, 0.9453, 0.7334, [0.2266], 0.9889, 0.9631, 0.8953, 0.7449, 0.6976, 0.2006, Acc_f: 0.2266, Acc_r: 0.7897\n"
     ]
    }
   ],
   "source": [
    "from agents.adv import FGSM\n",
    "\n",
    "def find_adjacent_cls(adv_agent, x, y):\n",
    "    x_adv = adv_agent.perturb(x, y)\n",
    "    adv_logits = model(x_adv)\n",
    "    adv_pred = torch.argmax(adv_logits.data, 1)\n",
    "    return adv_pred, x_adv\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    adv_agent = FGSM(deepcopy(model), bound=0.5, norm=False, random_start=True, device='cuda')\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    print('==='*20)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0005)\n",
    "\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    model.eval()\n",
    "    for ep in range(20):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            adv_pred, x_adv = find_adjacent_cls(adv_agent, x, y)\n",
    "            adv_y = torch.argmax(model(x_adv), dim=1).detach().cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, adv_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, [0.9167], 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9439, Acc_r: 0.9549\n",
      "0.9639, [0.9722], 0.9658, [0.9129], 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9425, Acc_r: 0.9516\n",
      "0.9616, [0.9645], 0.9655, [0.9094], 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9370, Acc_r: 0.9537\n",
      "------------ Retrained model ------------\n",
      "0.9622, [0.0000], 0.9730, [0.0000], 0.9766, 0.9606, 0.9560, 0.9797, 0.9428, 0.9561, Acc_f: 0.0000, Acc_r: 0.9634\n",
      "0.9702, [0.0000], 0.9718, [0.0000], 0.9723, 0.9543, 0.9600, 0.9777, 0.9476, 0.9592, Acc_f: 0.0000, Acc_r: 0.9641\n",
      "0.9644, [0.0000], 0.9747, [0.0000], 0.9786, 0.9551, 0.9504, 0.9792, 0.9416, 0.9542, Acc_f: 0.0000, Acc_r: 0.9623\n",
      "Original model Acc_f: 94.11 \\pm 0.30\n",
      "Original model Acc_r: 95.34 \\pm 0.14\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 96.33 \\pm 0.08\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 2\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n",
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 31/576\n",
      "Layer 3 : 409/1152\n",
      "Layer 4 : 1087/2304\n",
      "Layer 5 : 364/2304\n",
      "Layer 6 : 362/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 40/576\n",
      "Layer 3 : 586/1152\n",
      "Layer 4 : 1630/2304\n",
      "Layer 5 : 734/2304\n",
      "Layer 6 : 754/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 10/512\n",
      "Layer 10 : 4/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 46/576\n",
      "Layer 3 : 694/1152\n",
      "Layer 4 : 1875/2304\n",
      "Layer 5 : 1090/2304\n",
      "Layer 6 : 1146/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 16/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 55/576\n",
      "Layer 3 : 773/1152\n",
      "Layer 4 : 1986/2304\n",
      "Layer 5 : 1416/2304\n",
      "Layer 6 : 1543/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 21/512\n",
      "Layer 10 : 8/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 67/576\n",
      "Layer 3 : 825/1152\n",
      "Layer 4 : 2046/2304\n",
      "Layer 5 : 1697/2304\n",
      "Layer 6 : 1929/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 26/512\n",
      "Layer 10 : 10/4096\n",
      "Layer 11 : 7/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 73/576\n",
      "Layer 3 : 869/1152\n",
      "Layer 4 : 2096/2304\n",
      "Layer 5 : 1931/2304\n",
      "Layer 6 : 2321/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 31/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 96/576\n",
      "Layer 3 : 926/1152\n",
      "Layer 4 : 2162/2304\n",
      "Layer 5 : 2105/2304\n",
      "Layer 6 : 2728/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 39/512\n",
      "Layer 10 : 14/4096\n",
      "Layer 11 : 10/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 151/576\n",
      "Layer 3 : 1011/1152\n",
      "Layer 4 : 2222/2304\n",
      "Layer 5 : 2221/2304\n",
      "Layer 6 : 3152/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 62/512\n",
      "Layer 10 : 19/4096\n",
      "Layer 11 : 13/4096\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 30/576\n",
      "Layer 3 : 403/1152\n",
      "Layer 4 : 1056/2304\n",
      "Layer 5 : 360/2304\n",
      "Layer 6 : 361/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 2/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 38/576\n",
      "Layer 3 : 566/1152\n",
      "Layer 4 : 1596/2304\n",
      "Layer 5 : 722/2304\n",
      "Layer 6 : 744/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 8/512\n",
      "Layer 10 : 3/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 44/576\n",
      "Layer 3 : 675/1152\n",
      "Layer 4 : 1849/2304\n",
      "Layer 5 : 1077/2304\n",
      "Layer 6 : 1134/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 13/512\n",
      "Layer 10 : 5/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 52/576\n",
      "Layer 3 : 750/1152\n",
      "Layer 4 : 1965/2304\n",
      "Layer 5 : 1403/2304\n",
      "Layer 6 : 1524/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 17/512\n",
      "Layer 10 : 8/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 62/576\n",
      "Layer 3 : 802/1152\n",
      "Layer 4 : 2029/2304\n",
      "Layer 5 : 1684/2304\n",
      "Layer 6 : 1914/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 21/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 6/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 69/576\n",
      "Layer 3 : 842/1152\n",
      "Layer 4 : 2079/2304\n",
      "Layer 5 : 1920/2304\n",
      "Layer 6 : 2308/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 27/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 99/576\n",
      "Layer 3 : 914/1152\n",
      "Layer 4 : 2150/2304\n",
      "Layer 5 : 2094/2304\n",
      "Layer 6 : 2710/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 35/512\n",
      "Layer 10 : 14/4096\n",
      "Layer 11 : 10/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 145/576\n",
      "Layer 3 : 998/1152\n",
      "Layer 4 : 2214/2304\n",
      "Layer 5 : 2215/2304\n",
      "Layer 6 : 3137/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 58/512\n",
      "Layer 10 : 19/4096\n",
      "Layer 11 : 13/4096\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 32/576\n",
      "Layer 3 : 409/1152\n",
      "Layer 4 : 1076/2304\n",
      "Layer 5 : 367/2304\n",
      "Layer 6 : 377/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 1/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 39/576\n",
      "Layer 3 : 573/1152\n",
      "Layer 4 : 1611/2304\n",
      "Layer 5 : 733/2304\n",
      "Layer 6 : 766/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 10/512\n",
      "Layer 10 : 5/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 48/576\n",
      "Layer 3 : 691/1152\n",
      "Layer 4 : 1863/2304\n",
      "Layer 5 : 1089/2304\n",
      "Layer 6 : 1160/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 16/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 51/576\n",
      "Layer 3 : 750/1152\n",
      "Layer 4 : 1972/2304\n",
      "Layer 5 : 1413/2304\n",
      "Layer 6 : 1557/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 20/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 60/576\n",
      "Layer 3 : 798/1152\n",
      "Layer 4 : 2035/2304\n",
      "Layer 5 : 1692/2304\n",
      "Layer 6 : 1948/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 26/512\n",
      "Layer 10 : 10/4096\n",
      "Layer 11 : 7/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 68/576\n",
      "Layer 3 : 847/1152\n",
      "Layer 4 : 2084/2304\n",
      "Layer 5 : 1928/2304\n",
      "Layer 6 : 2346/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 32/512\n",
      "Layer 10 : 12/4096\n",
      "Layer 11 : 9/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 102/576\n",
      "Layer 3 : 924/1152\n",
      "Layer 4 : 2156/2304\n",
      "Layer 5 : 2100/2304\n",
      "Layer 6 : 2753/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 40/512\n",
      "Layer 10 : 15/4096\n",
      "Layer 11 : 11/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 150/576\n",
      "Layer 3 : 1009/1152\n",
      "Layer 4 : 2219/2304\n",
      "Layer 5 : 2217/2304\n",
      "Layer 6 : 3181/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 63/512\n",
      "Layer 10 : 19/4096\n",
      "Layer 11 : 14/4096\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, [0.9167], 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9439, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 39, loss 0.5830514430999756\n",
      "0.9794, [0.0002], 0.9764, [0.0056], 0.9790, 0.9643, 0.9342, 0.9871, 0.8928, 0.9549, Acc_f: 0.0029, Acc_r: 0.9585\n",
      "[train] epoch 1, batch 39, loss 0.3588450849056244\n",
      "0.9799, [0.0000], 0.9769, [0.0003], 0.9806, 0.9660, 0.9418, 0.9856, 0.9223, 0.9530, Acc_f: 0.0002, Acc_r: 0.9633\n",
      "[train] epoch 2, batch 39, loss 0.3573075234889984\n",
      "0.9794, [0.0000], 0.9771, [0.0003], 0.9806, 0.9664, 0.9413, 0.9856, 0.9337, 0.9524, Acc_f: 0.0002, Acc_r: 0.9646\n",
      "[train] epoch 3, batch 39, loss 0.34689757227897644\n",
      "0.9794, [0.0000], 0.9759, [0.0000], 0.9814, 0.9669, 0.9393, 0.9842, 0.9367, 0.9542, Acc_f: 0.0000, Acc_r: 0.9647\n",
      "[train] epoch 4, batch 39, loss 0.2947760820388794\n",
      "0.9805, [0.0000], 0.9732, [0.0000], 0.9814, 0.9664, 0.9383, 0.9861, 0.9373, 0.9549, Acc_f: 0.0000, Acc_r: 0.9648\n",
      "[train] epoch 5, batch 39, loss 0.2616407573223114\n",
      "0.9799, [0.0000], 0.9757, [0.0000], 0.9802, 0.9660, 0.9388, 0.9861, 0.9367, 0.9524, Acc_f: 0.0000, Acc_r: 0.9645\n",
      "[train] epoch 6, batch 39, loss 0.2412530928850174\n",
      "0.9805, [0.0000], 0.9720, [0.0000], 0.9802, 0.9660, 0.9378, 0.9842, 0.9398, 0.9542, Acc_f: 0.0000, Acc_r: 0.9643\n",
      "[train] epoch 7, batch 39, loss 0.2566989064216614\n",
      "0.9805, [0.0000], 0.9742, [0.0000], 0.9810, 0.9652, 0.9398, 0.9842, 0.9386, 0.9536, Acc_f: 0.0000, Acc_r: 0.9646\n",
      "[train] epoch 8, batch 39, loss 0.24529750645160675\n",
      "0.9799, [0.0000], 0.9759, [0.0000], 0.9806, 0.9656, 0.9383, 0.9846, 0.9380, 0.9536, Acc_f: 0.0000, Acc_r: 0.9646\n",
      "[train] epoch 9, batch 39, loss 0.2127971053123474\n",
      "0.9805, [0.0000], 0.9752, [0.0000], 0.9802, 0.9656, 0.9398, 0.9842, 0.9367, 0.9542, Acc_f: 0.0000, Acc_r: 0.9646\n",
      "[train] epoch 10, batch 39, loss 0.1929926872253418\n",
      "0.9811, [0.0000], 0.9749, [0.0000], 0.9802, 0.9635, 0.9408, 0.9846, 0.9361, 0.9536, Acc_f: 0.0000, Acc_r: 0.9644\n",
      "[train] epoch 11, batch 39, loss 0.24368587136268616\n",
      "0.9805, [0.0000], 0.9699, [0.0000], 0.9806, 0.9660, 0.9423, 0.9861, 0.9380, 0.9542, Acc_f: 0.0000, Acc_r: 0.9647\n",
      "[train] epoch 12, batch 39, loss 0.1878683716058731\n",
      "0.9811, [0.0000], 0.9718, [0.0000], 0.9802, 0.9643, 0.9408, 0.9861, 0.9373, 0.9536, Acc_f: 0.0000, Acc_r: 0.9644\n",
      "[train] epoch 13, batch 39, loss 0.18792101740837097\n",
      "0.9805, [0.0000], 0.9713, [0.0000], 0.9802, 0.9643, 0.9413, 0.9871, 0.9380, 0.9530, Acc_f: 0.0000, Acc_r: 0.9645\n",
      "[train] epoch 14, batch 39, loss 0.2165951281785965\n",
      "0.9805, [0.0000], 0.9740, [0.0000], 0.9802, 0.9639, 0.9383, 0.9842, 0.9398, 0.9536, Acc_f: 0.0000, Acc_r: 0.9643\n",
      "0.9639, [0.9722], 0.9658, [0.9129], 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9425, Acc_r: 0.9516\n",
      "[train] epoch 0, batch 39, loss 0.5782381892204285\n",
      "0.9656, [0.0012], 0.9752, [0.0243], 0.9786, 0.9698, 0.9600, 0.9767, 0.8982, 0.9423, Acc_f: 0.0127, Acc_r: 0.9583\n",
      "[train] epoch 1, batch 39, loss 0.5376148223876953\n",
      "0.9685, [0.0000], 0.9737, [0.0028], 0.9798, 0.9660, 0.9641, 0.9782, 0.9175, 0.9367, Acc_f: 0.0014, Acc_r: 0.9606\n",
      "[train] epoch 2, batch 39, loss 0.29273298382759094\n",
      "0.9702, [0.0000], 0.9757, [0.0010], 0.9810, 0.9639, 0.9651, 0.9757, 0.9289, 0.9342, Acc_f: 0.0005, Acc_r: 0.9618\n",
      "[train] epoch 3, batch 39, loss 0.3187949061393738\n",
      "0.9725, [0.0000], 0.9740, [0.0000], 0.9822, 0.9610, 0.9646, 0.9767, 0.9319, 0.9354, Acc_f: 0.0000, Acc_r: 0.9623\n",
      "[train] epoch 4, batch 39, loss 0.3033277690410614\n",
      "0.9725, [0.0000], 0.9745, [0.0000], 0.9810, 0.9576, 0.9656, 0.9767, 0.9349, 0.9329, Acc_f: 0.0000, Acc_r: 0.9620\n",
      "[train] epoch 5, batch 39, loss 0.2645035982131958\n",
      "0.9725, [0.0000], 0.9730, [0.0000], 0.9810, 0.9602, 0.9646, 0.9777, 0.9349, 0.9310, Acc_f: 0.0000, Acc_r: 0.9619\n",
      "[train] epoch 6, batch 39, loss 0.24643279612064362\n",
      "0.9725, [0.0000], 0.9730, [0.0000], 0.9810, 0.9597, 0.9661, 0.9767, 0.9349, 0.9304, Acc_f: 0.0000, Acc_r: 0.9618\n",
      "[train] epoch 7, batch 39, loss 0.27955079078674316\n",
      "0.9725, [0.0000], 0.9732, [0.0000], 0.9806, 0.9597, 0.9671, 0.9767, 0.9343, 0.9304, Acc_f: 0.0000, Acc_r: 0.9618\n",
      "[train] epoch 8, batch 39, loss 0.2321675419807434\n",
      "0.9731, [0.0000], 0.9737, [0.0000], 0.9786, 0.9581, 0.9671, 0.9782, 0.9349, 0.9310, Acc_f: 0.0000, Acc_r: 0.9618\n",
      "[train] epoch 9, batch 39, loss 0.24965091049671173\n",
      "0.9725, [0.0000], 0.9728, [0.0000], 0.9814, 0.9589, 0.9656, 0.9782, 0.9355, 0.9335, Acc_f: 0.0000, Acc_r: 0.9623\n",
      "[train] epoch 10, batch 39, loss 0.21181125938892365\n",
      "0.9719, [0.0000], 0.9718, [0.0000], 0.9802, 0.9572, 0.9666, 0.9787, 0.9380, 0.9323, Acc_f: 0.0000, Acc_r: 0.9621\n",
      "[train] epoch 11, batch 39, loss 0.19519945979118347\n",
      "0.9725, [0.0000], 0.9718, [0.0000], 0.9794, 0.9593, 0.9666, 0.9757, 0.9386, 0.9348, Acc_f: 0.0000, Acc_r: 0.9623\n",
      "[train] epoch 12, batch 39, loss 0.18886610865592957\n",
      "0.9731, [0.0000], 0.9713, [0.0000], 0.9810, 0.9576, 0.9661, 0.9757, 0.9398, 0.9342, Acc_f: 0.0000, Acc_r: 0.9623\n",
      "[train] epoch 13, batch 39, loss 0.2802318036556244\n",
      "0.9719, [0.0000], 0.9720, [0.0000], 0.9790, 0.9576, 0.9671, 0.9772, 0.9392, 0.9348, Acc_f: 0.0000, Acc_r: 0.9624\n",
      "[train] epoch 14, batch 39, loss 0.18959008157253265\n",
      "0.9736, [0.0000], 0.9723, [0.0000], 0.9814, 0.9560, 0.9656, 0.9772, 0.9367, 0.9329, Acc_f: 0.0000, Acc_r: 0.9620\n",
      "0.9616, [0.9645], 0.9655, [0.9094], 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9370, Acc_r: 0.9537\n",
      "[train] epoch 0, batch 39, loss 0.5712406039237976\n",
      "0.9673, [0.0027], 0.9771, [0.0118], 0.9830, 0.9727, 0.9514, 0.9787, 0.9355, 0.9429, Acc_f: 0.0073, Acc_r: 0.9636\n",
      "[train] epoch 1, batch 39, loss 0.39852941036224365\n",
      "0.9702, [0.0002], 0.9757, [0.0014], 0.9830, 0.9719, 0.9565, 0.9797, 0.9416, 0.9429, Acc_f: 0.0008, Acc_r: 0.9652\n",
      "[train] epoch 2, batch 39, loss 0.35880663990974426\n",
      "0.9719, [0.0002], 0.9735, [0.0007], 0.9845, 0.9719, 0.9560, 0.9792, 0.9464, 0.9398, Acc_f: 0.0004, Acc_r: 0.9654\n",
      "[train] epoch 3, batch 39, loss 0.31582117080688477\n",
      "0.9725, [0.0002], 0.9732, [0.0003], 0.9834, 0.9723, 0.9550, 0.9797, 0.9464, 0.9423, Acc_f: 0.0003, Acc_r: 0.9656\n",
      "[train] epoch 4, batch 39, loss 0.26896408200263977\n",
      "0.9731, [0.0000], 0.9728, [0.0003], 0.9830, 0.9727, 0.9560, 0.9797, 0.9476, 0.9373, Acc_f: 0.0002, Acc_r: 0.9653\n",
      "[train] epoch 5, batch 39, loss 0.2744326889514923\n",
      "0.9731, [0.0000], 0.9701, [0.0003], 0.9861, 0.9719, 0.9580, 0.9777, 0.9494, 0.9367, Acc_f: 0.0002, Acc_r: 0.9654\n",
      "[train] epoch 6, batch 39, loss 0.2905219793319702\n",
      "0.9742, [0.0000], 0.9706, [0.0000], 0.9841, 0.9723, 0.9575, 0.9797, 0.9488, 0.9398, Acc_f: 0.0000, Acc_r: 0.9659\n",
      "[train] epoch 7, batch 39, loss 0.2326933741569519\n",
      "0.9725, [0.0000], 0.9730, [0.0000], 0.9826, 0.9723, 0.9580, 0.9777, 0.9488, 0.9354, Acc_f: 0.0000, Acc_r: 0.9650\n",
      "[train] epoch 8, batch 39, loss 0.2438759207725525\n",
      "0.9736, [0.0000], 0.9713, [0.0000], 0.9849, 0.9715, 0.9585, 0.9782, 0.9482, 0.9392, Acc_f: 0.0000, Acc_r: 0.9657\n",
      "[train] epoch 9, batch 39, loss 0.22335287928581238\n",
      "0.9725, [0.0000], 0.9720, [0.0000], 0.9841, 0.9702, 0.9580, 0.9792, 0.9482, 0.9335, Acc_f: 0.0000, Acc_r: 0.9647\n",
      "[train] epoch 10, batch 39, loss 0.20472092926502228\n",
      "0.9725, [0.0000], 0.9713, [0.0000], 0.9837, 0.9719, 0.9605, 0.9802, 0.9470, 0.9361, Acc_f: 0.0000, Acc_r: 0.9654\n",
      "[train] epoch 11, batch 39, loss 0.2558436691761017\n",
      "0.9731, [0.0000], 0.9672, [0.0000], 0.9845, 0.9723, 0.9570, 0.9817, 0.9506, 0.9373, Acc_f: 0.0000, Acc_r: 0.9655\n",
      "[train] epoch 12, batch 39, loss 0.2350558489561081\n",
      "0.9742, [0.0000], 0.9706, [0.0000], 0.9841, 0.9711, 0.9575, 0.9807, 0.9476, 0.9386, Acc_f: 0.0000, Acc_r: 0.9655\n",
      "[train] epoch 13, batch 39, loss 0.2140229046344757\n",
      "0.9736, [0.0000], 0.9708, [0.0000], 0.9837, 0.9715, 0.9590, 0.9802, 0.9464, 0.9386, Acc_f: 0.0000, Acc_r: 0.9655\n",
      "[train] epoch 14, batch 39, loss 0.21268577873706818\n",
      "0.9731, [0.0000], 0.9701, [0.0000], 0.9853, 0.9702, 0.9590, 0.9787, 0.9482, 0.9379, Acc_f: 0.0000, Acc_r: 0.9653\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.00 \\pm 0.00\n",
      "UNSC Acc_r: 96.44 \\pm 0.15\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9648, 0.9624, 0.9659])\n",
    "Acc_f = 100*np.array([0.0000, 0.0000, 0.0000])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0966666666666736, 0.09463379711051795)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (100*np.array([0.9648, 0.9624, 0.9659])-100*np.array([0.9549, 0.9516, 0.9537]))\n",
    "a.mean(), a.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, [0.9167], 0.9687, [0.9488], 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9456, Acc_r: 0.9558\n",
      "0.9639, [0.9722], 0.9658, [0.9129], 0.9691, [0.9480], 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9443, Acc_r: 0.9521\n",
      "0.9616, [0.9645], 0.9655, [0.9094], 0.9734, [0.9534], 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9425, Acc_r: 0.9538\n",
      "------------ Retrained model ------------\n",
      "0.9748, [0.0000], 0.9723, [0.0000], 0.9754, [0.0000], 0.9661, 0.9767, 0.9488, 0.9517, Acc_f: 0.0000, Acc_r: 0.9665\n",
      "0.9335, [0.0000], 0.9689, [0.0000], 0.9643, [0.0000], 0.9555, 0.9470, 0.9301, 0.9003, Acc_f: 0.0000, Acc_r: 0.9428\n",
      "0.9656, [0.0000], 0.9785, [0.0000], 0.9746, [0.0000], 0.9727, 0.9757, 0.9452, 0.9611, Acc_f: 0.0000, Acc_r: 0.9676\n",
      "Original model Acc_f: 94.41 \\pm 0.13\n",
      "Original model Acc_r: 95.39 \\pm 0.15\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 95.90 \\pm 1.15\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 3\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n",
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 31/576\n",
      "Layer 3 : 425/1152\n",
      "Layer 4 : 1085/2304\n",
      "Layer 5 : 361/2304\n",
      "Layer 6 : 360/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 2/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 38/576\n",
      "Layer 3 : 576/1152\n",
      "Layer 4 : 1609/2304\n",
      "Layer 5 : 719/2304\n",
      "Layer 6 : 735/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 9/512\n",
      "Layer 10 : 4/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 45/576\n",
      "Layer 3 : 692/1152\n",
      "Layer 4 : 1872/2304\n",
      "Layer 5 : 1074/2304\n",
      "Layer 6 : 1122/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 15/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 61/576\n",
      "Layer 3 : 792/1152\n",
      "Layer 4 : 2018/2304\n",
      "Layer 5 : 1420/2304\n",
      "Layer 6 : 1530/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 21/512\n",
      "Layer 10 : 8/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 68/576\n",
      "Layer 3 : 848/1152\n",
      "Layer 4 : 2085/2304\n",
      "Layer 5 : 1731/2304\n",
      "Layer 6 : 1941/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 26/512\n",
      "Layer 10 : 10/4096\n",
      "Layer 11 : 7/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 99/576\n",
      "Layer 3 : 925/1152\n",
      "Layer 4 : 2159/2304\n",
      "Layer 5 : 1988/2304\n",
      "Layer 6 : 2362/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 35/512\n",
      "Layer 10 : 14/4096\n",
      "Layer 11 : 10/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 148/576\n",
      "Layer 3 : 1006/1152\n",
      "Layer 4 : 2221/2304\n",
      "Layer 5 : 2178/2304\n",
      "Layer 6 : 2804/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 59/512\n",
      "Layer 10 : 19/4096\n",
      "Layer 11 : 14/4096\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 30/576\n",
      "Layer 3 : 394/1152\n",
      "Layer 4 : 1064/2304\n",
      "Layer 5 : 364/2304\n",
      "Layer 6 : 366/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 1/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 37/576\n",
      "Layer 3 : 554/1152\n",
      "Layer 4 : 1599/2304\n",
      "Layer 5 : 728/2304\n",
      "Layer 6 : 748/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 9/512\n",
      "Layer 10 : 4/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 46/576\n",
      "Layer 3 : 675/1152\n",
      "Layer 4 : 1851/2304\n",
      "Layer 5 : 1084/2304\n",
      "Layer 6 : 1140/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 14/512\n",
      "Layer 10 : 5/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 63/576\n",
      "Layer 3 : 795/1152\n",
      "Layer 4 : 2001/2304\n",
      "Layer 5 : 1424/2304\n",
      "Layer 6 : 1543/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 19/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 73/576\n",
      "Layer 3 : 855/1152\n",
      "Layer 4 : 2075/2304\n",
      "Layer 5 : 1731/2304\n",
      "Layer 6 : 1954/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 25/512\n",
      "Layer 10 : 10/4096\n",
      "Layer 11 : 7/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 92/576\n",
      "Layer 3 : 917/1152\n",
      "Layer 4 : 2148/2304\n",
      "Layer 5 : 1981/2304\n",
      "Layer 6 : 2370/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 34/512\n",
      "Layer 10 : 13/4096\n",
      "Layer 11 : 10/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 144/576\n",
      "Layer 3 : 997/1152\n",
      "Layer 4 : 2213/2304\n",
      "Layer 5 : 2172/2304\n",
      "Layer 6 : 2811/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 59/512\n",
      "Layer 10 : 18/4096\n",
      "Layer 11 : 14/4096\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 31/576\n",
      "Layer 3 : 413/1152\n",
      "Layer 4 : 1077/2304\n",
      "Layer 5 : 365/2304\n",
      "Layer 6 : 374/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 1/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 40/576\n",
      "Layer 3 : 574/1152\n",
      "Layer 4 : 1613/2304\n",
      "Layer 5 : 732/2304\n",
      "Layer 6 : 764/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 11/512\n",
      "Layer 10 : 5/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 46/576\n",
      "Layer 3 : 688/1152\n",
      "Layer 4 : 1864/2304\n",
      "Layer 5 : 1086/2304\n",
      "Layer 6 : 1154/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 17/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 4/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 59/576\n",
      "Layer 3 : 787/1152\n",
      "Layer 4 : 2007/2304\n",
      "Layer 5 : 1428/2304\n",
      "Layer 6 : 1566/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 23/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 6/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 73/576\n",
      "Layer 3 : 851/1152\n",
      "Layer 4 : 2080/2304\n",
      "Layer 5 : 1741/2304\n",
      "Layer 6 : 1982/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 31/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 96/576\n",
      "Layer 3 : 922/1152\n",
      "Layer 4 : 2155/2304\n",
      "Layer 5 : 1991/2304\n",
      "Layer 6 : 2405/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 39/512\n",
      "Layer 10 : 14/4096\n",
      "Layer 11 : 10/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 151/576\n",
      "Layer 3 : 1007/1152\n",
      "Layer 4 : 2219/2304\n",
      "Layer 5 : 2178/2304\n",
      "Layer 6 : 2850/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 63/512\n",
      "Layer 10 : 19/4096\n",
      "Layer 11 : 13/4096\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, [0.9167], 0.9687, [0.9488], 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9456, Acc_r: 0.9558\n",
      "[train] epoch 0, batch 51, loss 0.5003509521484375\n",
      "0.9799, [0.0000], 0.9802, [0.0000], 0.9794, [0.0055], 0.9565, 0.9891, 0.8849, 0.9555, Acc_f: 0.0018, Acc_r: 0.9608\n",
      "[train] epoch 1, batch 51, loss 0.3797537684440613\n",
      "0.9805, [0.0000], 0.9798, [0.0000], 0.9822, [0.0013], 0.9575, 0.9881, 0.9307, 0.9555, Acc_f: 0.0004, Acc_r: 0.9678\n",
      "[train] epoch 2, batch 51, loss 0.35247498750686646\n",
      "0.9811, [0.0000], 0.9812, [0.0000], 0.9826, [0.0004], 0.9575, 0.9856, 0.9337, 0.9561, Acc_f: 0.0001, Acc_r: 0.9683\n",
      "[train] epoch 3, batch 51, loss 0.30832216143608093\n",
      "0.9782, [0.0000], 0.9800, [0.0000], 0.9822, [0.0004], 0.9600, 0.9866, 0.9349, 0.9561, Acc_f: 0.0001, Acc_r: 0.9683\n",
      "[train] epoch 4, batch 51, loss 0.3369174897670746\n",
      "0.9805, [0.0000], 0.9812, [0.0000], 0.9822, [0.0004], 0.9590, 0.9866, 0.9325, 0.9555, Acc_f: 0.0001, Acc_r: 0.9682\n",
      "[train] epoch 5, batch 51, loss 0.2328011691570282\n",
      "0.9765, [0.0000], 0.9824, [0.0000], 0.9822, [0.0004], 0.9611, 0.9866, 0.9319, 0.9555, Acc_f: 0.0001, Acc_r: 0.9680\n",
      "[train] epoch 6, batch 51, loss 0.2743590176105499\n",
      "0.9776, [0.0000], 0.9773, [0.0000], 0.9826, [0.0004], 0.9600, 0.9866, 0.9325, 0.9567, Acc_f: 0.0001, Acc_r: 0.9676\n",
      "[train] epoch 7, batch 51, loss 0.2445259541273117\n",
      "0.9782, [0.0000], 0.9793, [0.0000], 0.9822, [0.0004], 0.9605, 0.9866, 0.9313, 0.9549, Acc_f: 0.0001, Acc_r: 0.9676\n",
      "[train] epoch 8, batch 51, loss 0.22971180081367493\n",
      "0.9771, [0.0000], 0.9814, [0.0000], 0.9826, [0.0004], 0.9585, 0.9851, 0.9319, 0.9561, Acc_f: 0.0001, Acc_r: 0.9675\n",
      "[train] epoch 9, batch 51, loss 0.2004420906305313\n",
      "0.9805, [0.0000], 0.9783, [0.0000], 0.9814, [0.0000], 0.9600, 0.9856, 0.9325, 0.9561, Acc_f: 0.0000, Acc_r: 0.9678\n",
      "[train] epoch 10, batch 51, loss 0.18668025732040405\n",
      "0.9776, [0.0000], 0.9810, [0.0000], 0.9814, [0.0000], 0.9611, 0.9861, 0.9307, 0.9549, Acc_f: 0.0000, Acc_r: 0.9675\n",
      "[train] epoch 11, batch 51, loss 0.19033315777778625\n",
      "0.9794, [0.0000], 0.9807, [0.0000], 0.9810, [0.0000], 0.9600, 0.9856, 0.9289, 0.9561, Acc_f: 0.0000, Acc_r: 0.9674\n",
      "[train] epoch 12, batch 51, loss 0.22601276636123657\n",
      "0.9788, [0.0000], 0.9790, [0.0000], 0.9818, [0.0000], 0.9616, 0.9866, 0.9307, 0.9561, Acc_f: 0.0000, Acc_r: 0.9678\n",
      "[train] epoch 13, batch 51, loss 0.18740978837013245\n",
      "0.9788, [0.0000], 0.9776, [0.0000], 0.9818, [0.0004], 0.9595, 0.9871, 0.9337, 0.9567, Acc_f: 0.0001, Acc_r: 0.9679\n",
      "[train] epoch 14, batch 51, loss 0.20460759103298187\n",
      "0.9799, [0.0000], 0.9793, [0.0000], 0.9818, [0.0000], 0.9611, 0.9846, 0.9289, 0.9561, Acc_f: 0.0000, Acc_r: 0.9674\n",
      "0.9639, [0.9722], 0.9658, [0.9129], 0.9691, [0.9480], 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9443, Acc_r: 0.9521\n",
      "[train] epoch 0, batch 51, loss 0.5717000961303711\n",
      "0.9644, [0.0018], 0.9841, [0.0052], 0.9794, [0.0034], 0.9828, 0.9772, 0.8807, 0.9310, Acc_f: 0.0034, Acc_r: 0.9571\n",
      "[train] epoch 1, batch 51, loss 0.45020782947540283\n",
      "0.9627, [0.0000], 0.9826, [0.0007], 0.9814, [0.0008], 0.9828, 0.9747, 0.9114, 0.9335, Acc_f: 0.0005, Acc_r: 0.9613\n",
      "[train] epoch 2, batch 51, loss 0.40657511353492737\n",
      "0.9650, [0.0000], 0.9834, [0.0000], 0.9818, [0.0004], 0.9813, 0.9747, 0.9181, 0.9310, Acc_f: 0.0001, Acc_r: 0.9622\n",
      "[train] epoch 3, batch 51, loss 0.31347227096557617\n",
      "0.9656, [0.0000], 0.9824, [0.0000], 0.9822, [0.0004], 0.9808, 0.9772, 0.9157, 0.9348, Acc_f: 0.0001, Acc_r: 0.9627\n",
      "[train] epoch 4, batch 51, loss 0.22945758700370789\n",
      "0.9667, [0.0000], 0.9822, [0.0000], 0.9822, [0.0000], 0.9808, 0.9777, 0.9181, 0.9367, Acc_f: 0.0000, Acc_r: 0.9635\n",
      "[train] epoch 5, batch 51, loss 0.26361703872680664\n",
      "0.9679, [0.0000], 0.9812, [0.0000], 0.9830, [0.0000], 0.9788, 0.9787, 0.9169, 0.9361, Acc_f: 0.0000, Acc_r: 0.9632\n",
      "[train] epoch 6, batch 51, loss 0.2671225965023041\n",
      "0.9685, [0.0000], 0.9810, [0.0000], 0.9830, [0.0000], 0.9798, 0.9807, 0.9169, 0.9367, Acc_f: 0.0000, Acc_r: 0.9638\n",
      "[train] epoch 7, batch 51, loss 0.21555621922016144\n",
      "0.9702, [0.0000], 0.9810, [0.0000], 0.9830, [0.0000], 0.9777, 0.9777, 0.9187, 0.9373, Acc_f: 0.0000, Acc_r: 0.9636\n",
      "[train] epoch 8, batch 51, loss 0.19718626141548157\n",
      "0.9690, [0.0000], 0.9807, [0.0000], 0.9837, [0.0000], 0.9772, 0.9782, 0.9199, 0.9386, Acc_f: 0.0000, Acc_r: 0.9639\n",
      "[train] epoch 9, batch 51, loss 0.191416397690773\n",
      "0.9719, [0.0000], 0.9819, [0.0000], 0.9830, [0.0000], 0.9747, 0.9817, 0.9235, 0.9373, Acc_f: 0.0000, Acc_r: 0.9649\n",
      "[train] epoch 10, batch 51, loss 0.17479094862937927\n",
      "0.9696, [0.0000], 0.9814, [0.0000], 0.9822, [0.0000], 0.9782, 0.9797, 0.9223, 0.9392, Acc_f: 0.0000, Acc_r: 0.9647\n",
      "[train] epoch 11, batch 51, loss 0.16487398743629456\n",
      "0.9713, [0.0000], 0.9810, [0.0000], 0.9822, [0.0000], 0.9762, 0.9787, 0.9205, 0.9404, Acc_f: 0.0000, Acc_r: 0.9643\n",
      "[train] epoch 12, batch 51, loss 0.15720272064208984\n",
      "0.9713, [0.0000], 0.9824, [0.0000], 0.9826, [0.0000], 0.9762, 0.9797, 0.9229, 0.9361, Acc_f: 0.0000, Acc_r: 0.9645\n",
      "[train] epoch 13, batch 51, loss 0.20605744421482086\n",
      "0.9708, [0.0000], 0.9831, [0.0000], 0.9826, [0.0000], 0.9752, 0.9777, 0.9289, 0.9373, Acc_f: 0.0000, Acc_r: 0.9651\n",
      "[train] epoch 14, batch 51, loss 0.2391831874847412\n",
      "0.9708, [0.0000], 0.9807, [0.0000], 0.9822, [0.0000], 0.9757, 0.9792, 0.9229, 0.9373, Acc_f: 0.0000, Acc_r: 0.9641\n",
      "0.9616, [0.9645], 0.9655, [0.9094], 0.9734, [0.9534], 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9425, Acc_r: 0.9538\n",
      "[train] epoch 0, batch 51, loss 0.5293956995010376\n",
      "0.9639, [0.0016], 0.9846, [0.0024], 0.9853, [0.0042], 0.9732, 0.9767, 0.9361, 0.9423, Acc_f: 0.0027, Acc_r: 0.9660\n",
      "[train] epoch 1, batch 51, loss 0.4270256757736206\n",
      "0.9667, [0.0002], 0.9819, [0.0003], 0.9861, [0.0008], 0.9747, 0.9802, 0.9398, 0.9436, Acc_f: 0.0005, Acc_r: 0.9676\n",
      "[train] epoch 2, batch 51, loss 0.322146475315094\n",
      "0.9662, [0.0002], 0.9812, [0.0000], 0.9857, [0.0004], 0.9742, 0.9782, 0.9416, 0.9455, Acc_f: 0.0002, Acc_r: 0.9675\n",
      "[train] epoch 3, batch 51, loss 0.31459733843803406\n",
      "0.9685, [0.0000], 0.9798, [0.0000], 0.9849, [0.0000], 0.9747, 0.9807, 0.9392, 0.9524, Acc_f: 0.0000, Acc_r: 0.9686\n",
      "[train] epoch 4, batch 51, loss 0.23753966391086578\n",
      "0.9685, [0.0000], 0.9814, [0.0000], 0.9857, [0.0000], 0.9742, 0.9802, 0.9398, 0.9480, Acc_f: 0.0000, Acc_r: 0.9682\n",
      "[train] epoch 5, batch 51, loss 0.223215714097023\n",
      "0.9667, [0.0000], 0.9812, [0.0000], 0.9869, [0.0000], 0.9732, 0.9792, 0.9416, 0.9498, Acc_f: 0.0000, Acc_r: 0.9684\n",
      "[train] epoch 6, batch 51, loss 0.19022032618522644\n",
      "0.9696, [0.0000], 0.9822, [0.0000], 0.9857, [0.0000], 0.9737, 0.9762, 0.9404, 0.9436, Acc_f: 0.0000, Acc_r: 0.9673\n",
      "[train] epoch 7, batch 51, loss 0.20972786843776703\n",
      "0.9685, [0.0000], 0.9817, [0.0000], 0.9853, [0.0000], 0.9722, 0.9792, 0.9410, 0.9480, Acc_f: 0.0000, Acc_r: 0.9680\n",
      "[train] epoch 8, batch 51, loss 0.1935087889432907\n",
      "0.9685, [0.0000], 0.9807, [0.0000], 0.9857, [0.0000], 0.9737, 0.9792, 0.9373, 0.9498, Acc_f: 0.0000, Acc_r: 0.9679\n",
      "[train] epoch 9, batch 51, loss 0.21259705722332\n",
      "0.9685, [0.0000], 0.9778, [0.0000], 0.9861, [0.0000], 0.9732, 0.9807, 0.9380, 0.9524, Acc_f: 0.0000, Acc_r: 0.9681\n",
      "[train] epoch 10, batch 51, loss 0.2260872721672058\n",
      "0.9685, [0.0000], 0.9785, [0.0000], 0.9857, [0.0000], 0.9742, 0.9802, 0.9386, 0.9467, Acc_f: 0.0000, Acc_r: 0.9675\n",
      "[train] epoch 11, batch 51, loss 0.18341591954231262\n",
      "0.9696, [0.0000], 0.9790, [0.0000], 0.9857, [0.0000], 0.9717, 0.9802, 0.9404, 0.9492, Acc_f: 0.0000, Acc_r: 0.9680\n",
      "[train] epoch 12, batch 51, loss 0.1724863499403\n",
      "0.9708, [0.0000], 0.9790, [0.0000], 0.9853, [0.0000], 0.9722, 0.9797, 0.9398, 0.9549, Acc_f: 0.0000, Acc_r: 0.9688\n",
      "[train] epoch 13, batch 51, loss 0.2223123461008072\n",
      "0.9719, [0.0000], 0.9795, [0.0000], 0.9865, [0.0000], 0.9717, 0.9772, 0.9416, 0.9473, Acc_f: 0.0000, Acc_r: 0.9680\n",
      "[train] epoch 14, batch 51, loss 0.1875145584344864\n",
      "0.9690, [0.0000], 0.9771, [0.0000], 0.9865, [0.0000], 0.9737, 0.9802, 0.9398, 0.9511, Acc_f: 0.0000, Acc_r: 0.9682\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.01 \\pm 0.02\n",
      "UNSC Acc_r: 96.74 \\pm 0.16\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9683, 0.9651, 0.9688])\n",
    "Acc_f = 100*np.array([0.0004, 0.0000, 0.0000])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, [0.9167], 0.9687, [0.9488], 0.9474, 0.9534, [0.9253], 0.9611, Acc_f: 0.9405, Acc_r: 0.9608\n",
      "0.9639, [0.9722], 0.9658, [0.9129], 0.9691, [0.9480], 0.9646, 0.9287, [0.9253], 0.9473, Acc_f: 0.9396, Acc_r: 0.9566\n",
      "0.9616, [0.9645], 0.9655, [0.9094], 0.9734, [0.9534], 0.9560, 0.9376, [0.9361], 0.9461, Acc_f: 0.9409, Acc_r: 0.9567\n",
      "------------ Retrained model ------------\n",
      "0.9581, [0.0000], 0.9771, [0.0000], 0.9746, [0.0000], 0.9722, 0.9807, [0.0000], 0.9592, Acc_f: 0.0000, Acc_r: 0.9703\n",
      "0.9639, [0.0000], 0.9761, [0.0000], 0.9810, [0.0000], 0.9712, 0.9782, [0.0000], 0.9567, Acc_f: 0.0000, Acc_r: 0.9712\n",
      "0.9610, [0.0000], 0.9742, [0.0000], 0.9818, [0.0000], 0.9757, 0.9822, [0.0000], 0.9342, Acc_f: 0.0000, Acc_r: 0.9682\n",
      "Original model Acc_f: 94.03 \\pm 0.05\n",
      "Original model Acc_r: 95.80 \\pm 0.20\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 96.99 \\pm 0.13\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 4\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n",
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 30/576\n",
      "Layer 3 : 407/1152\n",
      "Layer 4 : 1083/2304\n",
      "Layer 5 : 365/2304\n",
      "Layer 6 : 371/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 39/576\n",
      "Layer 3 : 583/1152\n",
      "Layer 4 : 1624/2304\n",
      "Layer 5 : 732/2304\n",
      "Layer 6 : 755/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 10/512\n",
      "Layer 10 : 4/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 46/576\n",
      "Layer 3 : 688/1152\n",
      "Layer 4 : 1873/2304\n",
      "Layer 5 : 1090/2304\n",
      "Layer 6 : 1151/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 17/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 4/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 61/576\n",
      "Layer 3 : 786/1152\n",
      "Layer 4 : 2017/2304\n",
      "Layer 5 : 1437/2304\n",
      "Layer 6 : 1563/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 23/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 6/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 69/576\n",
      "Layer 3 : 845/1152\n",
      "Layer 4 : 2081/2304\n",
      "Layer 5 : 1745/2304\n",
      "Layer 6 : 1973/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 28/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 146/576\n",
      "Layer 3 : 992/1152\n",
      "Layer 4 : 2212/2304\n",
      "Layer 5 : 2052/2304\n",
      "Layer 6 : 2429/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 55/512\n",
      "Layer 10 : 17/4096\n",
      "Layer 11 : 13/4096\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 31/576\n",
      "Layer 3 : 407/1152\n",
      "Layer 4 : 1073/2304\n",
      "Layer 5 : 365/2304\n",
      "Layer 6 : 369/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 1/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 39/576\n",
      "Layer 3 : 575/1152\n",
      "Layer 4 : 1607/2304\n",
      "Layer 5 : 728/2304\n",
      "Layer 6 : 749/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 10/512\n",
      "Layer 10 : 5/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 47/576\n",
      "Layer 3 : 687/1152\n",
      "Layer 4 : 1855/2304\n",
      "Layer 5 : 1082/2304\n",
      "Layer 6 : 1140/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 15/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 4/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 61/576\n",
      "Layer 3 : 786/1152\n",
      "Layer 4 : 1998/2304\n",
      "Layer 5 : 1423/2304\n",
      "Layer 6 : 1544/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 20/512\n",
      "Layer 10 : 8/4096\n",
      "Layer 11 : 6/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 73/576\n",
      "Layer 3 : 852/1152\n",
      "Layer 4 : 2075/2304\n",
      "Layer 5 : 1732/2304\n",
      "Layer 6 : 1954/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 26/512\n",
      "Layer 10 : 10/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 148/576\n",
      "Layer 3 : 991/1152\n",
      "Layer 4 : 2205/2304\n",
      "Layer 5 : 2043/2304\n",
      "Layer 6 : 2411/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 52/512\n",
      "Layer 10 : 15/4096\n",
      "Layer 11 : 12/4096\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 30/576\n",
      "Layer 3 : 383/1152\n",
      "Layer 4 : 1043/2304\n",
      "Layer 5 : 361/2304\n",
      "Layer 6 : 370/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 4/512\n",
      "Layer 10 : 1/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 41/576\n",
      "Layer 3 : 565/1152\n",
      "Layer 4 : 1596/2304\n",
      "Layer 5 : 725/2304\n",
      "Layer 6 : 758/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 11/512\n",
      "Layer 10 : 4/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 46/576\n",
      "Layer 3 : 673/1152\n",
      "Layer 4 : 1855/2304\n",
      "Layer 5 : 1079/2304\n",
      "Layer 6 : 1149/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 17/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 3/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 63/576\n",
      "Layer 3 : 794/1152\n",
      "Layer 4 : 2007/2304\n",
      "Layer 5 : 1420/2304\n",
      "Layer 6 : 1557/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 23/512\n",
      "Layer 10 : 8/4096\n",
      "Layer 11 : 6/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 73/576\n",
      "Layer 3 : 856/1152\n",
      "Layer 4 : 2084/2304\n",
      "Layer 5 : 1730/2304\n",
      "Layer 6 : 1968/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 29/512\n",
      "Layer 10 : 10/4096\n",
      "Layer 11 : 7/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 151/576\n",
      "Layer 3 : 998/1152\n",
      "Layer 4 : 2211/2304\n",
      "Layer 5 : 2039/2304\n",
      "Layer 6 : 2425/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 55/512\n",
      "Layer 10 : 15/4096\n",
      "Layer 11 : 11/4096\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, [0.9167], 0.9687, [0.9488], 0.9474, 0.9534, [0.9253], 0.9611, Acc_f: 0.9405, Acc_r: 0.9608\n",
      "[train] epoch 0, batch 60, loss 0.6527426242828369\n",
      "0.9771, [0.0008], 0.9798, [0.0000], 0.9806, [0.0042], 0.9626, 0.9881, [0.0006], 0.9599, Acc_f: 0.0014, Acc_r: 0.9747\n",
      "[train] epoch 1, batch 60, loss 0.3771827816963196\n",
      "0.9725, [0.0000], 0.9817, [0.0000], 0.9834, [0.0013], 0.9626, 0.9866, [0.0000], 0.9643, Acc_f: 0.0003, Acc_r: 0.9752\n",
      "[train] epoch 2, batch 60, loss 0.26301056146621704\n",
      "0.9765, [0.0000], 0.9824, [0.0000], 0.9814, [0.0000], 0.9666, 0.9866, [0.0000], 0.9636, Acc_f: 0.0000, Acc_r: 0.9762\n",
      "[train] epoch 3, batch 60, loss 0.32000425457954407\n",
      "0.9759, [0.0000], 0.9798, [0.0000], 0.9830, [0.0004], 0.9646, 0.9861, [0.0000], 0.9649, Acc_f: 0.0001, Acc_r: 0.9757\n",
      "[train] epoch 4, batch 60, loss 0.29089492559432983\n",
      "0.9748, [0.0000], 0.9822, [0.0000], 0.9830, [0.0004], 0.9681, 0.9876, [0.0000], 0.9624, Acc_f: 0.0001, Acc_r: 0.9763\n",
      "[train] epoch 5, batch 60, loss 0.1863582879304886\n",
      "0.9765, [0.0000], 0.9836, [0.0000], 0.9818, [0.0004], 0.9676, 0.9856, [0.0000], 0.9624, Acc_f: 0.0001, Acc_r: 0.9763\n",
      "[train] epoch 6, batch 60, loss 0.31720617413520813\n",
      "0.9771, [0.0000], 0.9793, [0.0000], 0.9834, [0.0004], 0.9636, 0.9866, [0.0000], 0.9618, Acc_f: 0.0001, Acc_r: 0.9753\n",
      "[train] epoch 7, batch 60, loss 0.3080890476703644\n",
      "0.9759, [0.0000], 0.9829, [0.0000], 0.9822, [0.0004], 0.9641, 0.9861, [0.0000], 0.9618, Acc_f: 0.0001, Acc_r: 0.9755\n",
      "[train] epoch 8, batch 60, loss 0.2519335448741913\n",
      "0.9759, [0.0000], 0.9836, [0.0000], 0.9822, [0.0004], 0.9671, 0.9861, [0.0000], 0.9630, Acc_f: 0.0001, Acc_r: 0.9763\n",
      "[train] epoch 9, batch 60, loss 0.21448224782943726\n",
      "0.9765, [0.0000], 0.9785, [0.0000], 0.9826, [0.0000], 0.9676, 0.9871, [0.0000], 0.9649, Acc_f: 0.0000, Acc_r: 0.9762\n",
      "[train] epoch 10, batch 60, loss 0.10790961235761642\n",
      "0.9771, [0.0000], 0.9795, [0.0000], 0.9826, [0.0000], 0.9661, 0.9866, [0.0000], 0.9636, Acc_f: 0.0000, Acc_r: 0.9759\n",
      "[train] epoch 11, batch 60, loss 0.27597200870513916\n",
      "0.9776, [0.0000], 0.9785, [0.0000], 0.9822, [0.0000], 0.9656, 0.9856, [0.0000], 0.9630, Acc_f: 0.0000, Acc_r: 0.9754\n",
      "[train] epoch 12, batch 60, loss 0.21009966731071472\n",
      "0.9771, [0.0000], 0.9790, [0.0000], 0.9834, [0.0000], 0.9661, 0.9866, [0.0000], 0.9624, Acc_f: 0.0000, Acc_r: 0.9758\n",
      "[train] epoch 13, batch 60, loss 0.19936083257198334\n",
      "0.9765, [0.0000], 0.9814, [0.0000], 0.9826, [0.0000], 0.9646, 0.9866, [0.0000], 0.9605, Acc_f: 0.0000, Acc_r: 0.9754\n",
      "[train] epoch 14, batch 60, loss 0.1517488658428192\n",
      "0.9771, [0.0000], 0.9800, [0.0000], 0.9810, [0.0000], 0.9656, 0.9861, [0.0000], 0.9618, Acc_f: 0.0000, Acc_r: 0.9753\n",
      "0.9639, [0.9722], 0.9658, [0.9129], 0.9691, [0.9480], 0.9646, 0.9287, [0.9253], 0.9473, Acc_f: 0.9396, Acc_r: 0.9566\n",
      "[train] epoch 0, batch 60, loss 0.518962025642395\n",
      "0.9558, [0.0004], 0.9860, [0.0007], 0.9802, [0.0013], 0.9853, 0.9737, [0.0030], 0.9348, Acc_f: 0.0013, Acc_r: 0.9693\n",
      "[train] epoch 1, batch 60, loss 0.39311814308166504\n",
      "0.9570, [0.0000], 0.9836, [0.0000], 0.9834, [0.0004], 0.9853, 0.9767, [0.0006], 0.9442, Acc_f: 0.0003, Acc_r: 0.9717\n",
      "[train] epoch 2, batch 60, loss 0.40455272793769836\n",
      "0.9570, [0.0000], 0.9829, [0.0000], 0.9837, [0.0004], 0.9858, 0.9742, [0.0006], 0.9455, Acc_f: 0.0003, Acc_r: 0.9715\n",
      "[train] epoch 3, batch 60, loss 0.19132761657238007\n",
      "0.9547, [0.0000], 0.9851, [0.0000], 0.9834, [0.0004], 0.9853, 0.9757, [0.0000], 0.9386, Acc_f: 0.0001, Acc_r: 0.9705\n",
      "[train] epoch 4, batch 60, loss 0.21991871297359467\n",
      "0.9581, [0.0000], 0.9822, [0.0000], 0.9802, [0.0000], 0.9879, 0.9762, [0.0006], 0.9429, Acc_f: 0.0002, Acc_r: 0.9713\n",
      "[train] epoch 5, batch 60, loss 0.1608608067035675\n",
      "0.9587, [0.0000], 0.9829, [0.0000], 0.9837, [0.0000], 0.9853, 0.9767, [0.0000], 0.9429, Acc_f: 0.0000, Acc_r: 0.9717\n",
      "[train] epoch 6, batch 60, loss 0.20335407555103302\n",
      "0.9644, [0.0000], 0.9793, [0.0000], 0.9869, [0.0000], 0.9833, 0.9767, [0.0000], 0.9461, Acc_f: 0.0000, Acc_r: 0.9728\n",
      "[train] epoch 7, batch 60, loss 0.23874889314174652\n",
      "0.9633, [0.0000], 0.9800, [0.0000], 0.9834, [0.0000], 0.9848, 0.9797, [0.0000], 0.9442, Acc_f: 0.0000, Acc_r: 0.9726\n",
      "[train] epoch 8, batch 60, loss 0.22122623026371002\n",
      "0.9662, [0.0000], 0.9810, [0.0000], 0.9841, [0.0000], 0.9833, 0.9787, [0.0000], 0.9423, Acc_f: 0.0000, Acc_r: 0.9726\n",
      "[train] epoch 9, batch 60, loss 0.1756817251443863\n",
      "0.9679, [0.0000], 0.9788, [0.0000], 0.9849, [0.0000], 0.9828, 0.9782, [0.0000], 0.9480, Acc_f: 0.0000, Acc_r: 0.9734\n",
      "[train] epoch 10, batch 60, loss 0.15423807501792908\n",
      "0.9679, [0.0000], 0.9819, [0.0000], 0.9830, [0.0000], 0.9813, 0.9792, [0.0000], 0.9423, Acc_f: 0.0000, Acc_r: 0.9726\n",
      "[train] epoch 11, batch 60, loss 0.15552918612957\n",
      "0.9673, [0.0000], 0.9800, [0.0000], 0.9830, [0.0000], 0.9823, 0.9807, [0.0000], 0.9429, Acc_f: 0.0000, Acc_r: 0.9727\n",
      "[train] epoch 12, batch 60, loss 0.26695871353149414\n",
      "0.9696, [0.0000], 0.9793, [0.0000], 0.9826, [0.0000], 0.9808, 0.9822, [0.0000], 0.9492, Acc_f: 0.0000, Acc_r: 0.9739\n",
      "[train] epoch 13, batch 60, loss 0.15621130168437958\n",
      "0.9679, [0.0000], 0.9822, [0.0000], 0.9822, [0.0000], 0.9818, 0.9797, [0.0000], 0.9461, Acc_f: 0.0000, Acc_r: 0.9733\n",
      "[train] epoch 14, batch 60, loss 0.15589265525341034\n",
      "0.9690, [0.0000], 0.9790, [0.0000], 0.9830, [0.0000], 0.9818, 0.9792, [0.0000], 0.9455, Acc_f: 0.0000, Acc_r: 0.9729\n",
      "0.9616, [0.9645], 0.9655, [0.9094], 0.9734, [0.9534], 0.9560, 0.9376, [0.9361], 0.9461, Acc_f: 0.9409, Acc_r: 0.9567\n",
      "[train] epoch 0, batch 60, loss 0.45185014605522156\n",
      "0.9536, [0.0004], 0.9834, [0.0007], 0.9837, [0.0025], 0.9828, 0.9797, [0.0066], 0.9517, Acc_f: 0.0026, Acc_r: 0.9725\n",
      "[train] epoch 1, batch 60, loss 0.1941903829574585\n",
      "0.9576, [0.0002], 0.9812, [0.0000], 0.9873, [0.0008], 0.9813, 0.9807, [0.0012], 0.9517, Acc_f: 0.0006, Acc_r: 0.9733\n",
      "[train] epoch 2, batch 60, loss 0.31179413199424744\n",
      "0.9581, [0.0002], 0.9824, [0.0000], 0.9861, [0.0004], 0.9818, 0.9802, [0.0006], 0.9524, Acc_f: 0.0003, Acc_r: 0.9735\n",
      "[train] epoch 3, batch 60, loss 0.2522376477718353\n",
      "0.9604, [0.0000], 0.9853, [0.0000], 0.9849, [0.0000], 0.9813, 0.9762, [0.0006], 0.9492, Acc_f: 0.0002, Acc_r: 0.9729\n",
      "[train] epoch 4, batch 60, loss 0.25273287296295166\n",
      "0.9547, [0.0000], 0.9834, [0.0000], 0.9869, [0.0000], 0.9833, 0.9762, [0.0006], 0.9423, Acc_f: 0.0002, Acc_r: 0.9711\n",
      "[train] epoch 5, batch 60, loss 0.2470194697380066\n",
      "0.9610, [0.0000], 0.9829, [0.0000], 0.9857, [0.0000], 0.9818, 0.9797, [0.0006], 0.9536, Acc_f: 0.0002, Acc_r: 0.9741\n",
      "[train] epoch 6, batch 60, loss 0.2750439941883087\n",
      "0.9633, [0.0000], 0.9805, [0.0000], 0.9869, [0.0000], 0.9803, 0.9812, [0.0006], 0.9580, Acc_f: 0.0002, Acc_r: 0.9750\n",
      "[train] epoch 7, batch 60, loss 0.17695201933383942\n",
      "0.9644, [0.0000], 0.9778, [0.0000], 0.9877, [0.0000], 0.9808, 0.9802, [0.0006], 0.9567, Acc_f: 0.0002, Acc_r: 0.9746\n",
      "[train] epoch 8, batch 60, loss 0.2091486155986786\n",
      "0.9650, [0.0000], 0.9812, [0.0000], 0.9869, [0.0000], 0.9818, 0.9777, [0.0006], 0.9561, Acc_f: 0.0002, Acc_r: 0.9748\n",
      "[train] epoch 9, batch 60, loss 0.23849470913410187\n",
      "0.9667, [0.0000], 0.9824, [0.0000], 0.9857, [0.0000], 0.9808, 0.9772, [0.0000], 0.9536, Acc_f: 0.0000, Acc_r: 0.9744\n",
      "[train] epoch 10, batch 60, loss 0.20191572606563568\n",
      "0.9650, [0.0000], 0.9798, [0.0000], 0.9865, [0.0000], 0.9803, 0.9802, [0.0000], 0.9618, Acc_f: 0.0000, Acc_r: 0.9756\n",
      "[train] epoch 11, batch 60, loss 0.15370510518550873\n",
      "0.9685, [0.0000], 0.9814, [0.0000], 0.9861, [0.0000], 0.9777, 0.9797, [0.0000], 0.9549, Acc_f: 0.0000, Acc_r: 0.9747\n",
      "[train] epoch 12, batch 60, loss 0.17408724129199982\n",
      "0.9627, [0.0000], 0.9783, [0.0000], 0.9885, [0.0000], 0.9818, 0.9767, [0.0006], 0.9530, Acc_f: 0.0002, Acc_r: 0.9735\n",
      "[train] epoch 13, batch 60, loss 0.2611731290817261\n",
      "0.9650, [0.0000], 0.9810, [0.0000], 0.9873, [0.0000], 0.9818, 0.9708, [0.0000], 0.9536, Acc_f: 0.0000, Acc_r: 0.9732\n",
      "[train] epoch 14, batch 60, loss 0.1968483179807663\n",
      "0.9633, [0.0000], 0.9805, [0.0000], 0.9873, [0.0000], 0.9808, 0.9807, [0.0000], 0.9542, Acc_f: 0.0000, Acc_r: 0.9745\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_r = 100*np.array([0.9763, 0.9739, 0.9750])\n",
    "Acc_f = 100*np.array([0.0001, 0.0000, 0.0002])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')\n",
    "\n",
    "Acc_r = 100*np.array([0.9608, 0.9566, 0.9567])\n",
    "Acc_f = 100*np.array([0.9405, 0.9396, 0.9409])\n",
    "print(f'Origin Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Origin Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7029999999999983, 0.11577852420318199)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (100*np.array([0.9763, 0.9739, 0.9750])-100*np.array([0.9608, 0.95661, 0.9567]))\n",
    "a.mean(), a.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, 0.9167, 0.9687, [0.9488], [0.9474], [0.9534], 0.9253, [0.9611], Acc_f: 0.9564, Acc_r: 0.9490\n",
      "0.9639, [0.9722], 0.9658, 0.9129, 0.9691, [0.9480], [0.9646], [0.9287], 0.9253, [0.9473], Acc_f: 0.9521, Acc_r: 0.9474\n",
      "0.9616, [0.9645], 0.9655, 0.9094, 0.9734, [0.9534], [0.9560], [0.9376], 0.9361, [0.9461], Acc_f: 0.9515, Acc_r: 0.9492\n",
      "------------ Retrained model ------------\n",
      "0.9811, [0.0000], 0.9764, 0.9473, 0.9711, [0.0000], [0.0000], [0.0000], 0.9663, [0.0000], Acc_f: 0.0000, Acc_r: 0.9684\n",
      "0.9748, [0.0000], 0.9728, 0.9480, 0.9754, [0.0000], [0.0000], [0.0000], 0.9560, [0.0000], Acc_f: 0.0000, Acc_r: 0.9654\n",
      "0.9719, [0.0000], 0.9706, 0.9410, 0.9727, [0.0000], [0.0000], [0.0000], 0.9663, [0.0000], Acc_f: 0.0000, Acc_r: 0.9645\n",
      "Original model Acc_f: 95.34 \\pm 0.22\n",
      "Original model Acc_r: 94.85 \\pm 0.08\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 96.61 \\pm 0.17\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 5\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n",
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 31/576\n",
      "Layer 3 : 415/1152\n",
      "Layer 4 : 1066/2304\n",
      "Layer 5 : 355/2304\n",
      "Layer 6 : 354/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 2/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 39/576\n",
      "Layer 3 : 582/1152\n",
      "Layer 4 : 1618/2304\n",
      "Layer 5 : 719/2304\n",
      "Layer 6 : 735/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 8/512\n",
      "Layer 10 : 3/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 45/576\n",
      "Layer 3 : 677/1152\n",
      "Layer 4 : 1848/2304\n",
      "Layer 5 : 1065/2304\n",
      "Layer 6 : 1126/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 14/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 4/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 48/576\n",
      "Layer 3 : 729/1152\n",
      "Layer 4 : 1944/2304\n",
      "Layer 5 : 1381/2304\n",
      "Layer 6 : 1510/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 20/512\n",
      "Layer 10 : 8/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 95/576\n",
      "Layer 3 : 896/1152\n",
      "Layer 4 : 2135/2304\n",
      "Layer 5 : 1732/2304\n",
      "Layer 6 : 1947/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 30/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 9/4096\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 29/576\n",
      "Layer 3 : 393/1152\n",
      "Layer 4 : 1052/2304\n",
      "Layer 5 : 361/2304\n",
      "Layer 6 : 360/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 2/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 39/576\n",
      "Layer 3 : 560/1152\n",
      "Layer 4 : 1590/2304\n",
      "Layer 5 : 726/2304\n",
      "Layer 6 : 746/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 8/512\n",
      "Layer 10 : 3/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 44/576\n",
      "Layer 3 : 664/1152\n",
      "Layer 4 : 1825/2304\n",
      "Layer 5 : 1068/2304\n",
      "Layer 6 : 1126/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 15/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 4/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 45/576\n",
      "Layer 3 : 710/1152\n",
      "Layer 4 : 1921/2304\n",
      "Layer 5 : 1378/2304\n",
      "Layer 6 : 1499/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 19/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 95/576\n",
      "Layer 3 : 889/1152\n",
      "Layer 4 : 2121/2304\n",
      "Layer 5 : 1726/2304\n",
      "Layer 6 : 1933/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 29/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/27\n",
      "Layer 2 : 31/576\n",
      "Layer 3 : 407/1152\n",
      "Layer 4 : 1048/2304\n",
      "Layer 5 : 361/2304\n",
      "Layer 6 : 370/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 3/512\n",
      "Layer 10 : 0/4096\n",
      "Layer 11 : 0/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 38/576\n",
      "Layer 3 : 570/1152\n",
      "Layer 4 : 1597/2304\n",
      "Layer 5 : 725/2304\n",
      "Layer 6 : 759/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 10/512\n",
      "Layer 10 : 4/4096\n",
      "Layer 11 : 2/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 47/576\n",
      "Layer 3 : 673/1152\n",
      "Layer 4 : 1835/2304\n",
      "Layer 5 : 1069/2304\n",
      "Layer 6 : 1146/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 17/512\n",
      "Layer 10 : 6/4096\n",
      "Layer 11 : 4/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 48/576\n",
      "Layer 3 : 718/1152\n",
      "Layer 4 : 1933/2304\n",
      "Layer 5 : 1381/2304\n",
      "Layer 6 : 1526/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 21/512\n",
      "Layer 10 : 7/4096\n",
      "Layer 11 : 5/4096\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 7\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 98/576\n",
      "Layer 3 : 902/1152\n",
      "Layer 4 : 2133/2304\n",
      "Layer 5 : 1729/2304\n",
      "Layer 6 : 1963/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 30/512\n",
      "Layer 10 : 11/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719, [0.9712], 0.9624, 0.9167, 0.9687, [0.9488], [0.9474], [0.9534], 0.9253, [0.9611], Acc_f: 0.9564, Acc_r: 0.9490\n",
      "[train] epoch 0, batch 64, loss 0.38560330867767334\n",
      "0.9868, [0.0002], 0.9870, 0.9587, 0.9734, [0.0025], [0.0020], [0.0000], 0.9554, [0.0063], Acc_f: 0.0022, Acc_r: 0.9723\n",
      "[train] epoch 1, batch 64, loss 0.30492883920669556\n",
      "0.9862, [0.0000], 0.9846, 0.9639, 0.9798, [0.0008], [0.0000], [0.0000], 0.9566, [0.0006], Acc_f: 0.0003, Acc_r: 0.9742\n",
      "[train] epoch 2, batch 64, loss 0.2505352795124054\n",
      "0.9851, [0.0000], 0.9858, 0.9611, 0.9802, [0.0004], [0.0000], [0.0000], 0.9584, [0.0000], Acc_f: 0.0001, Acc_r: 0.9741\n",
      "[train] epoch 3, batch 64, loss 0.18862971663475037\n",
      "0.9857, [0.0000], 0.9851, 0.9615, 0.9794, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9739\n",
      "[train] epoch 4, batch 64, loss 0.223812535405159\n",
      "0.9857, [0.0000], 0.9834, 0.9646, 0.9802, [0.0000], [0.0000], [0.0000], 0.9584, [0.0000], Acc_f: 0.0000, Acc_r: 0.9745\n",
      "[train] epoch 5, batch 64, loss 0.19124621152877808\n",
      "0.9868, [0.0000], 0.9834, 0.9639, 0.9806, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9745\n",
      "[train] epoch 6, batch 64, loss 0.2116096019744873\n",
      "0.9851, [0.0000], 0.9841, 0.9639, 0.9806, [0.0000], [0.0000], [0.0000], 0.9572, [0.0000], Acc_f: 0.0000, Acc_r: 0.9742\n",
      "[train] epoch 7, batch 64, loss 0.16443069279193878\n",
      "0.9868, [0.0000], 0.9822, 0.9660, 0.9802, [0.0000], [0.0000], [0.0000], 0.9566, [0.0000], Acc_f: 0.0000, Acc_r: 0.9744\n",
      "[train] epoch 8, batch 64, loss 0.12497734278440475\n",
      "0.9845, [0.0000], 0.9843, 0.9636, 0.9806, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9742\n",
      "[train] epoch 9, batch 64, loss 0.12660576403141022\n",
      "0.9845, [0.0000], 0.9848, 0.9632, 0.9802, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9741\n",
      "[train] epoch 10, batch 64, loss 0.1946069747209549\n",
      "0.9851, [0.0000], 0.9865, 0.9591, 0.9806, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9738\n",
      "[train] epoch 11, batch 64, loss 0.1265871524810791\n",
      "0.9851, [0.0000], 0.9848, 0.9611, 0.9802, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9738\n",
      "[train] epoch 12, batch 64, loss 0.1279120147228241\n",
      "0.9851, [0.0000], 0.9860, 0.9618, 0.9794, [0.0000], [0.0000], [0.0000], 0.9578, [0.0000], Acc_f: 0.0000, Acc_r: 0.9740\n",
      "[train] epoch 13, batch 64, loss 0.15208372473716736\n",
      "0.9851, [0.0000], 0.9841, 0.9643, 0.9802, [0.0000], [0.0000], [0.0000], 0.9566, [0.0000], Acc_f: 0.0000, Acc_r: 0.9741\n",
      "[train] epoch 14, batch 64, loss 0.15523383021354675\n",
      "0.9851, [0.0000], 0.9858, 0.9618, 0.9786, [0.0000], [0.0000], [0.0000], 0.9572, [0.0000], Acc_f: 0.0000, Acc_r: 0.9737\n",
      "0.9639, [0.9722], 0.9658, 0.9129, 0.9691, [0.9480], [0.9646], [0.9287], 0.9253, [0.9473], Acc_f: 0.9521, Acc_r: 0.9474\n",
      "[train] epoch 0, batch 64, loss 0.44933727383613586\n",
      "0.9811, [0.0002], 0.9814, 0.9604, 0.9762, [0.0004], [0.0005], [0.0000], 0.9602, [0.0056], Acc_f: 0.0014, Acc_r: 0.9719\n",
      "[train] epoch 1, batch 64, loss 0.3010295331478119\n",
      "0.9811, [0.0000], 0.9805, 0.9632, 0.9802, [0.0004], [0.0000], [0.0000], 0.9633, [0.0019], Acc_f: 0.0005, Acc_r: 0.9736\n",
      "[train] epoch 2, batch 64, loss 0.2552940249443054\n",
      "0.9817, [0.0000], 0.9812, 0.9625, 0.9806, [0.0000], [0.0000], [0.0000], 0.9657, [0.0019], Acc_f: 0.0004, Acc_r: 0.9743\n",
      "[train] epoch 3, batch 64, loss 0.2134028673171997\n",
      "0.9839, [0.0000], 0.9819, 0.9587, 0.9810, [0.0000], [0.0000], [0.0000], 0.9663, [0.0000], Acc_f: 0.0000, Acc_r: 0.9744\n",
      "[train] epoch 4, batch 64, loss 0.20381677150726318\n",
      "0.9828, [0.0000], 0.9795, 0.9653, 0.9814, [0.0000], [0.0000], [0.0000], 0.9645, [0.0000], Acc_f: 0.0000, Acc_r: 0.9747\n",
      "[train] epoch 5, batch 64, loss 0.2158508598804474\n",
      "0.9822, [0.0000], 0.9788, 0.9646, 0.9818, [0.0000], [0.0000], [0.0000], 0.9657, [0.0000], Acc_f: 0.0000, Acc_r: 0.9746\n",
      "[train] epoch 6, batch 64, loss 0.2106843739748001\n",
      "0.9851, [0.0000], 0.9812, 0.9632, 0.9814, [0.0000], [0.0000], [0.0000], 0.9651, [0.0000], Acc_f: 0.0000, Acc_r: 0.9752\n",
      "[train] epoch 7, batch 64, loss 0.19408287107944489\n",
      "0.9845, [0.0000], 0.9817, 0.9625, 0.9810, [0.0000], [0.0000], [0.0000], 0.9657, [0.0000], Acc_f: 0.0000, Acc_r: 0.9751\n",
      "[train] epoch 8, batch 64, loss 0.15267010033130646\n",
      "0.9845, [0.0000], 0.9810, 0.9625, 0.9814, [0.0000], [0.0000], [0.0000], 0.9675, [0.0000], Acc_f: 0.0000, Acc_r: 0.9754\n",
      "[train] epoch 9, batch 64, loss 0.17269346117973328\n",
      "0.9845, [0.0000], 0.9814, 0.9591, 0.9806, [0.0000], [0.0000], [0.0000], 0.9687, [0.0000], Acc_f: 0.0000, Acc_r: 0.9749\n",
      "[train] epoch 10, batch 64, loss 0.13732978701591492\n",
      "0.9851, [0.0000], 0.9807, 0.9632, 0.9798, [0.0000], [0.0000], [0.0000], 0.9663, [0.0000], Acc_f: 0.0000, Acc_r: 0.9750\n",
      "[train] epoch 11, batch 64, loss 0.1589503437280655\n",
      "0.9839, [0.0000], 0.9817, 0.9632, 0.9794, [0.0000], [0.0000], [0.0000], 0.9669, [0.0000], Acc_f: 0.0000, Acc_r: 0.9750\n",
      "[train] epoch 12, batch 64, loss 0.175327867269516\n",
      "0.9839, [0.0000], 0.9812, 0.9608, 0.9806, [0.0000], [0.0000], [0.0000], 0.9669, [0.0000], Acc_f: 0.0000, Acc_r: 0.9747\n",
      "[train] epoch 13, batch 64, loss 0.17799152433872223\n",
      "0.9839, [0.0000], 0.9814, 0.9639, 0.9798, [0.0000], [0.0000], [0.0000], 0.9675, [0.0000], Acc_f: 0.0000, Acc_r: 0.9753\n",
      "[train] epoch 14, batch 64, loss 0.1385444849729538\n",
      "0.9845, [0.0000], 0.9795, 0.9650, 0.9802, [0.0000], [0.0000], [0.0000], 0.9681, [0.0000], Acc_f: 0.0000, Acc_r: 0.9754\n",
      "0.9616, [0.9645], 0.9655, 0.9094, 0.9734, [0.9534], [0.9560], [0.9376], 0.9361, [0.9461], Acc_f: 0.9515, Acc_r: 0.9492\n",
      "[train] epoch 0, batch 64, loss 0.490403413772583\n",
      "0.9799, [0.0004], 0.9812, 0.9545, 0.9814, [0.0000], [0.0035], [0.0000], 0.9675, [0.0038], Acc_f: 0.0015, Acc_r: 0.9729\n",
      "[train] epoch 1, batch 64, loss 0.3206537365913391\n",
      "0.9828, [0.0002], 0.9790, 0.9535, 0.9830, [0.0000], [0.0005], [0.0000], 0.9681, [0.0013], Acc_f: 0.0004, Acc_r: 0.9733\n",
      "[train] epoch 2, batch 64, loss 0.2637232840061188\n",
      "0.9834, [0.0000], 0.9781, 0.9552, 0.9834, [0.0000], [0.0000], [0.0000], 0.9669, [0.0006], Acc_f: 0.0001, Acc_r: 0.9734\n",
      "[train] epoch 3, batch 64, loss 0.2751750648021698\n",
      "0.9834, [0.0000], 0.9778, 0.9594, 0.9837, [0.0000], [0.0000], [0.0000], 0.9669, [0.0006], Acc_f: 0.0001, Acc_r: 0.9742\n",
      "[train] epoch 4, batch 64, loss 0.23319122195243835\n",
      "0.9828, [0.0000], 0.9776, 0.9591, 0.9834, [0.0000], [0.0000], [0.0000], 0.9681, [0.0006], Acc_f: 0.0001, Acc_r: 0.9742\n",
      "[train] epoch 5, batch 64, loss 0.20938313007354736\n",
      "0.9817, [0.0000], 0.9788, 0.9559, 0.9853, [0.0000], [0.0000], [0.0000], 0.9681, [0.0000], Acc_f: 0.0000, Acc_r: 0.9740\n",
      "[train] epoch 6, batch 64, loss 0.21600142121315002\n",
      "0.9834, [0.0000], 0.9817, 0.9532, 0.9830, [0.0000], [0.0000], [0.0000], 0.9651, [0.0000], Acc_f: 0.0000, Acc_r: 0.9732\n",
      "[train] epoch 7, batch 64, loss 0.17078155279159546\n",
      "0.9828, [0.0000], 0.9793, 0.9577, 0.9849, [0.0000], [0.0000], [0.0000], 0.9675, [0.0006], Acc_f: 0.0001, Acc_r: 0.9744\n",
      "[train] epoch 8, batch 64, loss 0.20380806922912598\n",
      "0.9834, [0.0000], 0.9822, 0.9518, 0.9834, [0.0000], [0.0000], [0.0000], 0.9687, [0.0000], Acc_f: 0.0000, Acc_r: 0.9739\n",
      "[train] epoch 9, batch 64, loss 0.15786640346050262\n",
      "0.9822, [0.0000], 0.9785, 0.9570, 0.9849, [0.0000], [0.0000], [0.0000], 0.9681, [0.0000], Acc_f: 0.0000, Acc_r: 0.9742\n",
      "[train] epoch 10, batch 64, loss 0.17847396433353424\n",
      "0.9845, [0.0000], 0.9812, 0.9539, 0.9837, [0.0000], [0.0000], [0.0000], 0.9663, [0.0000], Acc_f: 0.0000, Acc_r: 0.9739\n",
      "[train] epoch 11, batch 64, loss 0.1696784347295761\n",
      "0.9839, [0.0000], 0.9810, 0.9539, 0.9834, [0.0000], [0.0000], [0.0000], 0.9675, [0.0000], Acc_f: 0.0000, Acc_r: 0.9739\n",
      "[train] epoch 12, batch 64, loss 0.19056233763694763\n",
      "0.9834, [0.0000], 0.9812, 0.9552, 0.9822, [0.0000], [0.0000], [0.0000], 0.9663, [0.0000], Acc_f: 0.0000, Acc_r: 0.9736\n",
      "[train] epoch 13, batch 64, loss 0.1556173712015152\n",
      "0.9828, [0.0000], 0.9810, 0.9525, 0.9834, [0.0000], [0.0000], [0.0000], 0.9693, [0.0000], Acc_f: 0.0000, Acc_r: 0.9738\n",
      "[train] epoch 14, batch 64, loss 0.15670084953308105\n",
      "0.9822, [0.0000], 0.9812, 0.9542, 0.9826, [0.0000], [0.0000], [0.0000], 0.9687, [0.0000], Acc_f: 0.0000, Acc_r: 0.9738\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_r = 100*np.array([0.9745, 0.9754, 0.9744])\n",
    "Acc_f = 100*np.array([0.0000, 0.0000, 0.0001])\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')\n",
    "\n",
    "Acc_r = 100*np.array([0.9490, 0.9474, 0.9492])\n",
    "Acc_f = 100*np.array([0.9564, 0.9521, 0.9515])\n",
    "print(f'Origin Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Origin Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
