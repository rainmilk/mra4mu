{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils\n",
    "from agents import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seeds',          type=int,       default=[2023, 2024, 2025])\n",
    "parser.add_argument('--dataset',        type=str,       default='svhn')\n",
    "parser.add_argument('--batch_size',     type=int,       default=256)\n",
    "parser.add_argument('--model_name',     type=str,       default='vgg11')\n",
    "parser.add_argument('--retrain',        type=bool,      default=False)\n",
    "parser.add_argument('--unlearn_class',  type=list,      default=3)\n",
    "args = parser.parse_args(\"\")\n",
    "args.time_str = time.strftime(\"%m-%d-%H-%M\", time.localtime())\n",
    "if args.dataset.lower() == 'fmnist':\n",
    "    args.n_channels = 1\n",
    "else:\n",
    "    args.n_channels = 3\n",
    "\n",
    "if args.dataset.lower() == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "else:\n",
    "    args.num_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlearn_dataloader(data_loader):\n",
    "    dataset = data_loader.dataset\n",
    "    _indices = data_loader.sampler.indices\n",
    "\n",
    "    if args.dataset.lower() == 'svhn':\n",
    "        train_targets = np.array(dataset.labels)[_indices]\n",
    "    else:\n",
    "        train_targets = np.array(dataset.labels)[_indices]\n",
    "    unlearn_indices, remain_indices = [], []\n",
    "    for i, target in enumerate(train_targets):\n",
    "        if target in args.unlearn_class:\n",
    "            unlearn_indices.append(i)\n",
    "        else:\n",
    "            remain_indices.append(i)\n",
    "\n",
    "    unlearn_indices = np.array(_indices)[unlearn_indices]\n",
    "    remain_indices = np.array(_indices)[remain_indices]\n",
    "\n",
    "    unlearn_sampler = torch.utils.data.SubsetRandomSampler(unlearn_indices)\n",
    "    unlearn_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler = unlearn_sampler,)\n",
    "\n",
    "    remain_sampler = torch.utils.data.SubsetRandomSampler(remain_indices)\n",
    "    remain_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler = remain_sampler)\n",
    "    return remain_loader, unlearn_loader\n",
    "\n",
    "def get_dataloader(args):\n",
    "    train_loader, test_loader = utils._get_dataloader(args)\n",
    "\n",
    "    indices = np.arange(len(train_loader.dataset))\n",
    "    a = np.split(indices,[int(len(indices)*0.9), int(len(indices))])\n",
    "    idx_train = a[0]\n",
    "    idx_val = a[1]\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(idx_train)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(idx_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader.dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler=train_sampler)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(train_loader.dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler=val_sampler)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_loader.dataset,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False)\n",
    "    \n",
    "    if args.retrain:\n",
    "        train_loader, _ = get_unlearn_dataloader(train_loader)\n",
    "        val_loader, _ = get_unlearn_dataloader(val_loader)\n",
    "        \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = 'original_model_12-15-02-49'\n",
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "unlearn_indices = np.where(np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "\n",
    "# conver to the original indices\n",
    "unlearn_indices = train_loader.sampler.indices[unlearn_indices]\n",
    "\n",
    "unlearn_sampler = torch.utils.data.SubsetRandomSampler(unlearn_indices)\n",
    "unlearn_subset_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                    batch_size=args.batch_size, \n",
    "                                                    sampler=unlearn_sampler)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "remain_indices = np.where(~np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "remain_indices = train_loader.sampler.indices[remain_indices]\n",
    "\n",
    "remain_sampler = torch.utils.data.SubsetRandomSampler(remain_indices)\n",
    "remain_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            sampler=remain_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "------------ Retrained model ------------\n",
      "0.8739, 0.0000, 0.7756, 0.6971, 0.9738, 0.5168, 0.3804, 0.7855, 0.8873, 0.7386, Acc_f: 0.6971, Acc_r: 0.6591\n",
      "0.8882, 0.0000, 0.8855, 0.4441, 0.8736, 0.7286, 0.7678, 0.8465, 0.6892, 0.8251, Acc_f: 0.4441, Acc_r: 0.7227\n",
      "0.8331, 0.0000, 0.7308, 0.7366, 0.9164, 0.6976, 0.4148, 0.9584, 0.3898, 0.4069, Acc_f: 0.7366, Acc_r: 0.5942\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{args.seeds[i]}.pth'))\n",
    "        test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model Acc_f: 93.23 $\\pm$ 0.91\n",
      "Original model Acc_r: 95.39 $\\pm$ 0.34\n",
      "Retrained model: 93.54 $\\pm$ 1.72\n"
     ]
    }
   ],
   "source": [
    "Acc_f = 100*np.array([0.9362, 0.9198, 0.9410])\n",
    "Acc_r = 100*np.array([0.9549, 0.9574, 0.9493])\n",
    "print(f'Original model Acc_f: {Acc_f.mean():.2f} $\\pm$ {Acc_f.std():.2f}')\n",
    "print(f'Original model Acc_r: {Acc_r.mean():.2f} $\\pm$ {Acc_r.std():.2f}')\n",
    "\n",
    "Acc_r = 100*np.array([0.9174, 0.9585, 0.9302])\n",
    "print(f'Retrained model: {Acc_r.mean():.2f} $\\pm$ {Acc_r.std():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "[train] epoch 0, batch 29, loss 4.340445041656494\n",
      "0.9753, 0.8763, 0.9250, 0.4476, 0.8906, 0.7118, 0.7142, 0.8975, 0.5958, 0.7273, Acc_f: 0.4476, Acc_r: 0.8126\n",
      "[train] epoch 1, batch 29, loss 3.316979169845581\n",
      "0.9868, 0.8313, 0.9147, 0.3102, 0.8748, 0.6023, 0.6009, 0.8737, 0.4325, 0.5875, Acc_f: 0.3102, Acc_r: 0.7449\n",
      "[train] epoch 2, batch 29, loss 2.9311957359313965\n",
      "0.9903, 0.7988, 0.9019, 0.2425, 0.8637, 0.4987, 0.4729, 0.8470, 0.3235, 0.4865, Acc_f: 0.2425, Acc_r: 0.6870\n",
      "[train] epoch 3, batch 29, loss 2.763293504714966\n",
      "0.9908, 0.7803, 0.8915, 0.1957, 0.8510, 0.4362, 0.3728, 0.8257, 0.2614, 0.4176, Acc_f: 0.1957, Acc_r: 0.6475\n",
      "[train] epoch 4, batch 29, loss 2.7665843963623047\n",
      "0.9914, 0.7762, 0.8812, 0.1728, 0.8470, 0.3863, 0.3197, 0.8123, 0.2151, 0.3561, Acc_f: 0.1728, Acc_r: 0.6206\n",
      "[train] epoch 5, batch 29, loss 2.5609397888183594\n",
      "0.9920, 0.7737, 0.8752, 0.1554, 0.8438, 0.3612, 0.2782, 0.7979, 0.1892, 0.3172, Acc_f: 0.1554, Acc_r: 0.6031\n",
      "[train] epoch 6, batch 29, loss 2.6550779342651367\n",
      "0.9920, 0.7815, 0.8730, 0.1502, 0.8426, 0.3503, 0.2580, 0.7801, 0.1747, 0.2915, Acc_f: 0.1502, Acc_r: 0.5937\n",
      "[train] epoch 7, batch 29, loss 2.5380537509918213\n",
      "0.9920, 0.7900, 0.8711, 0.1461, 0.8419, 0.3414, 0.2453, 0.7632, 0.1675, 0.2665, Acc_f: 0.1461, Acc_r: 0.5865\n",
      "[train] epoch 8, batch 29, loss 2.6234993934631348\n",
      "0.9920, 0.7984, 0.8684, 0.1416, 0.8395, 0.3301, 0.2307, 0.7514, 0.1584, 0.2464, Acc_f: 0.1416, Acc_r: 0.5795\n",
      "[train] epoch 9, batch 29, loss 2.498239755630493\n",
      "0.9920, 0.8009, 0.8686, 0.1360, 0.8351, 0.3196, 0.2180, 0.7449, 0.1518, 0.2270, Acc_f: 0.1360, Acc_r: 0.5731\n",
      "[train] epoch 10, batch 29, loss 2.514183759689331\n",
      "0.9920, 0.8029, 0.8672, 0.1332, 0.8300, 0.3091, 0.2074, 0.7320, 0.1452, 0.2169, Acc_f: 0.1332, Acc_r: 0.5670\n",
      "[train] epoch 11, batch 29, loss 2.49863600730896\n",
      "0.9925, 0.8070, 0.8660, 0.1325, 0.8284, 0.3024, 0.2018, 0.7246, 0.1434, 0.2050, Acc_f: 0.1325, Acc_r: 0.5635\n",
      "[train] epoch 12, batch 29, loss 2.4189212322235107\n",
      "0.9925, 0.8111, 0.8665, 0.1319, 0.8276, 0.2999, 0.1973, 0.7152, 0.1386, 0.1950, Acc_f: 0.1319, Acc_r: 0.5604\n",
      "[train] epoch 13, batch 29, loss 2.4946799278259277\n",
      "0.9925, 0.8151, 0.8650, 0.1305, 0.8264, 0.2915, 0.1932, 0.7063, 0.1355, 0.1856, Acc_f: 0.1305, Acc_r: 0.5568\n",
      "[train] epoch 14, batch 29, loss 2.4641733169555664\n",
      "0.9914, 0.8184, 0.8624, 0.1266, 0.8220, 0.2823, 0.1866, 0.6964, 0.1295, 0.1768, Acc_f: 0.1266, Acc_r: 0.5518\n",
      "[train] epoch 15, batch 29, loss 2.464600086212158\n",
      "0.9914, 0.8206, 0.8631, 0.1246, 0.8185, 0.2764, 0.1821, 0.6855, 0.1253, 0.1687, Acc_f: 0.1246, Acc_r: 0.5479\n",
      "[train] epoch 16, batch 29, loss 2.447936773300171\n",
      "0.9914, 0.8225, 0.8626, 0.1225, 0.8141, 0.2718, 0.1745, 0.6790, 0.1199, 0.1611, Acc_f: 0.1225, Acc_r: 0.5441\n",
      "[train] epoch 17, batch 29, loss 2.4790337085723877\n",
      "0.9920, 0.8235, 0.8626, 0.1204, 0.8117, 0.2643, 0.1700, 0.6686, 0.1163, 0.1542, Acc_f: 0.1204, Acc_r: 0.5404\n",
      "[train] epoch 18, batch 29, loss 2.445183515548706\n",
      "0.9920, 0.8249, 0.8602, 0.1173, 0.8086, 0.2546, 0.1664, 0.6578, 0.1145, 0.1461, Acc_f: 0.1173, Acc_r: 0.5361\n",
      "[train] epoch 19, batch 29, loss 2.4704670906066895\n",
      "0.9920, 0.8276, 0.8590, 0.1166, 0.8066, 0.2513, 0.1644, 0.6513, 0.1114, 0.1423, Acc_f: 0.1166, Acc_r: 0.5340\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "[train] epoch 0, batch 29, loss 3.997234344482422\n",
      "0.9558, 0.8576, 0.7927, 0.3425, 0.9287, 0.6971, 0.8316, 0.9158, 0.4084, 0.8326, Acc_f: 0.3425, Acc_r: 0.8023\n",
      "[train] epoch 1, batch 29, loss 3.2444047927856445\n",
      "0.9673, 0.8555, 0.7698, 0.2516, 0.9298, 0.6216, 0.7623, 0.9168, 0.3030, 0.8000, Acc_f: 0.2516, Acc_r: 0.7696\n",
      "[train] epoch 2, batch 29, loss 2.924258232116699\n",
      "0.9731, 0.8576, 0.7515, 0.1936, 0.9310, 0.5541, 0.6813, 0.9084, 0.2241, 0.7718, Acc_f: 0.1936, Acc_r: 0.7392\n",
      "[train] epoch 3, batch 29, loss 2.831376552581787\n",
      "0.9742, 0.8619, 0.7383, 0.1541, 0.9338, 0.5013, 0.6100, 0.9000, 0.1892, 0.7386, Acc_f: 0.1541, Acc_r: 0.7163\n",
      "[train] epoch 4, batch 29, loss 2.857900857925415\n",
      "0.9748, 0.8653, 0.7269, 0.1298, 0.9342, 0.4652, 0.5518, 0.8940, 0.1596, 0.7179, Acc_f: 0.1298, Acc_r: 0.6989\n",
      "[train] epoch 5, batch 29, loss 2.7921175956726074\n",
      "0.9742, 0.8680, 0.7221, 0.1121, 0.9330, 0.4476, 0.5073, 0.8886, 0.1410, 0.6922, Acc_f: 0.1121, Acc_r: 0.6860\n",
      "[train] epoch 6, batch 29, loss 2.6582281589508057\n",
      "0.9736, 0.8710, 0.7180, 0.1013, 0.9318, 0.4337, 0.4800, 0.8821, 0.1277, 0.6821, Acc_f: 0.1013, Acc_r: 0.6778\n",
      "[train] epoch 7, batch 29, loss 2.682582139968872\n",
      "0.9736, 0.8729, 0.7127, 0.0930, 0.9322, 0.4195, 0.4608, 0.8762, 0.1181, 0.6721, Acc_f: 0.0930, Acc_r: 0.6709\n",
      "[train] epoch 8, batch 29, loss 2.7279422283172607\n",
      "0.9736, 0.8763, 0.7125, 0.0867, 0.9298, 0.4153, 0.4482, 0.8697, 0.1090, 0.6690, Acc_f: 0.0867, Acc_r: 0.6670\n",
      "[train] epoch 9, batch 29, loss 2.6360082626342773\n",
      "0.9742, 0.8782, 0.7105, 0.0805, 0.9298, 0.4102, 0.4325, 0.8608, 0.0982, 0.6646, Acc_f: 0.0805, Acc_r: 0.6621\n",
      "[train] epoch 10, batch 29, loss 2.612330198287964\n",
      "0.9742, 0.8806, 0.7105, 0.0767, 0.9267, 0.4090, 0.4219, 0.8549, 0.0946, 0.6621, Acc_f: 0.0767, Acc_r: 0.6594\n",
      "[train] epoch 11, batch 29, loss 2.523325204849243\n",
      "0.9742, 0.8817, 0.7096, 0.0753, 0.9259, 0.4081, 0.4158, 0.8494, 0.0928, 0.6596, Acc_f: 0.0753, Acc_r: 0.6575\n",
      "[train] epoch 12, batch 29, loss 2.5742993354797363\n",
      "0.9742, 0.8849, 0.7069, 0.0736, 0.9227, 0.4065, 0.4072, 0.8400, 0.0904, 0.6608, Acc_f: 0.0736, Acc_r: 0.6548\n",
      "[train] epoch 13, batch 29, loss 2.581148147583008\n",
      "0.9736, 0.8886, 0.7052, 0.0725, 0.9219, 0.4039, 0.4001, 0.8331, 0.0837, 0.6602, Acc_f: 0.0725, Acc_r: 0.6523\n",
      "[train] epoch 14, batch 29, loss 2.599879503250122\n",
      "0.9731, 0.8908, 0.7038, 0.0697, 0.9203, 0.4031, 0.3910, 0.8291, 0.0795, 0.6589, Acc_f: 0.0697, Acc_r: 0.6500\n",
      "[train] epoch 15, batch 29, loss 2.572800636291504\n",
      "0.9725, 0.8933, 0.7016, 0.0701, 0.9191, 0.4002, 0.3829, 0.8227, 0.0747, 0.6596, Acc_f: 0.0701, Acc_r: 0.6474\n",
      "[train] epoch 16, batch 29, loss 2.4925265312194824\n",
      "0.9713, 0.8949, 0.6999, 0.0677, 0.9184, 0.3977, 0.3758, 0.8162, 0.0705, 0.6571, Acc_f: 0.0677, Acc_r: 0.6446\n",
      "[train] epoch 17, batch 29, loss 2.4879446029663086\n",
      "0.9713, 0.8965, 0.7009, 0.0670, 0.9176, 0.4006, 0.3763, 0.8118, 0.0681, 0.6608, Acc_f: 0.0670, Acc_r: 0.6449\n",
      "[train] epoch 18, batch 29, loss 2.4898931980133057\n",
      "0.9719, 0.8970, 0.6990, 0.0649, 0.9164, 0.4027, 0.3733, 0.8078, 0.0651, 0.6621, Acc_f: 0.0649, Acc_r: 0.6439\n",
      "[train] epoch 19, batch 29, loss 2.4737038612365723\n",
      "0.9719, 0.8992, 0.6970, 0.0625, 0.9160, 0.4002, 0.3667, 0.8009, 0.0620, 0.6614, Acc_f: 0.0625, Acc_r: 0.6417\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "[train] epoch 0, batch 29, loss 3.8238630294799805\n",
      "0.9318, 0.8684, 0.9431, 0.5031, 0.9160, 0.7630, 0.8053, 0.8613, 0.6139, 0.6771, Acc_f: 0.5031, Acc_r: 0.8200\n",
      "[train] epoch 1, batch 29, loss 3.1826367378234863\n",
      "0.9656, 0.8404, 0.9349, 0.3980, 0.9160, 0.6921, 0.7582, 0.8529, 0.4952, 0.6082, Acc_f: 0.3980, Acc_r: 0.7848\n",
      "[train] epoch 2, batch 29, loss 2.8820295333862305\n",
      "0.9765, 0.8086, 0.9181, 0.3234, 0.9069, 0.6170, 0.6970, 0.8366, 0.4205, 0.5436, Acc_f: 0.3234, Acc_r: 0.7472\n",
      "[train] epoch 3, batch 29, loss 2.7074050903320312\n",
      "0.9794, 0.7966, 0.9034, 0.2571, 0.9001, 0.5529, 0.6378, 0.8207, 0.3554, 0.4759, Acc_f: 0.2571, Acc_r: 0.7136\n",
      "[train] epoch 4, batch 29, loss 2.652470350265503\n",
      "0.9834, 0.7935, 0.8886, 0.2255, 0.8914, 0.5042, 0.5948, 0.7984, 0.3199, 0.4320, Acc_f: 0.2255, Acc_r: 0.6896\n",
      "[train] epoch 5, batch 29, loss 2.6144278049468994\n",
      "0.9845, 0.8009, 0.8771, 0.1978, 0.8882, 0.4614, 0.5680, 0.7816, 0.2922, 0.3987, Acc_f: 0.1978, Acc_r: 0.6725\n",
      "[train] epoch 6, batch 29, loss 2.540740728378296\n",
      "0.9857, 0.8070, 0.8660, 0.1728, 0.8831, 0.4283, 0.5468, 0.7697, 0.2687, 0.3649, Acc_f: 0.1728, Acc_r: 0.6578\n",
      "[train] epoch 7, batch 29, loss 2.5488133430480957\n",
      "0.9857, 0.8164, 0.8551, 0.1530, 0.8807, 0.3981, 0.5306, 0.7548, 0.2482, 0.3467, Acc_f: 0.1530, Acc_r: 0.6463\n",
      "[train] epoch 8, batch 29, loss 2.4654688835144043\n",
      "0.9851, 0.8262, 0.8443, 0.1381, 0.8783, 0.3779, 0.5175, 0.7365, 0.2337, 0.3204, Acc_f: 0.1381, Acc_r: 0.6356\n",
      "[train] epoch 9, batch 29, loss 2.4233179092407227\n",
      "0.9845, 0.8315, 0.8351, 0.1284, 0.8744, 0.3507, 0.4997, 0.7246, 0.2193, 0.3034, Acc_f: 0.1284, Acc_r: 0.6248\n",
      "[train] epoch 10, batch 29, loss 2.3854589462280273\n",
      "0.9834, 0.8400, 0.8286, 0.1201, 0.8732, 0.3364, 0.4896, 0.7117, 0.2108, 0.2966, Acc_f: 0.1201, Acc_r: 0.6189\n",
      "[train] epoch 11, batch 29, loss 2.471411943435669\n",
      "0.9828, 0.8490, 0.8202, 0.1128, 0.8708, 0.3201, 0.4780, 0.6984, 0.2006, 0.2871, Acc_f: 0.1128, Acc_r: 0.6119\n",
      "[train] epoch 12, batch 29, loss 2.415628671646118\n",
      "0.9817, 0.8561, 0.8103, 0.1013, 0.8664, 0.3075, 0.4674, 0.6825, 0.1886, 0.2765, Acc_f: 0.1013, Acc_r: 0.6041\n",
      "[train] epoch 13, batch 29, loss 2.484238386154175\n",
      "0.9811, 0.8619, 0.8031, 0.0951, 0.8601, 0.2924, 0.4598, 0.6706, 0.1825, 0.2677, Acc_f: 0.0951, Acc_r: 0.5977\n",
      "[train] epoch 14, batch 29, loss 2.44628643989563\n",
      "0.9811, 0.8657, 0.7978, 0.0906, 0.8585, 0.2798, 0.4527, 0.6592, 0.1759, 0.2621, Acc_f: 0.0906, Acc_r: 0.5925\n",
      "[train] epoch 15, batch 29, loss 2.3887715339660645\n",
      "0.9794, 0.8694, 0.7946, 0.0885, 0.8561, 0.2706, 0.4461, 0.6474, 0.1711, 0.2571, Acc_f: 0.0885, Acc_r: 0.5880\n",
      "[train] epoch 16, batch 29, loss 2.4083900451660156\n",
      "0.9788, 0.8731, 0.7893, 0.0836, 0.8518, 0.2613, 0.4380, 0.6384, 0.1651, 0.2527, Acc_f: 0.0836, Acc_r: 0.5832\n",
      "[train] epoch 17, batch 29, loss 2.4064979553222656\n",
      "0.9788, 0.8764, 0.7816, 0.0815, 0.8494, 0.2513, 0.4350, 0.6251, 0.1614, 0.2495, Acc_f: 0.0815, Acc_r: 0.5787\n",
      "[train] epoch 18, batch 29, loss 2.427004814147949\n",
      "0.9776, 0.8782, 0.7751, 0.0791, 0.8458, 0.2433, 0.4299, 0.6191, 0.1584, 0.2439, Acc_f: 0.0791, Acc_r: 0.5746\n",
      "[train] epoch 19, batch 29, loss 2.4136877059936523\n",
      "0.9782, 0.8808, 0.7691, 0.0763, 0.8442, 0.2378, 0.4264, 0.6082, 0.1536, 0.2382, Acc_f: 0.0763, Acc_r: 0.5707\n"
     ]
    }
   ],
   "source": [
    "Acc_r, Acc_f = np.zeros(3), np.zeros(3)\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(20):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        Acc_r[i], Acc_f[i] = test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label Acc_f: 1.17 \\pm 0.16\n",
      "Random label Acc_r: 75.87 \\pm 8.67\n"
     ]
    }
   ],
   "source": [
    "Acc_f = 100*np.array([0.0139, 0.0111, 0.0101])\n",
    "Acc_r = 100*np.array([0.8372, 0.8010, 0.6379])\n",
    "print(f'Random label Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 67/576\n",
      "Layer 3 : 372/1152\n",
      "Layer 4 : 1318/2304\n",
      "Layer 5 : 575/2304\n",
      "Layer 6 : 710/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 14/512\n",
      "Layer 10 : 34/4096\n",
      "Layer 11 : 20/4096\n",
      "----------------------------------------\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 65/576\n",
      "Layer 3 : 378/1152\n",
      "Layer 4 : 1293/2304\n",
      "Layer 5 : 572/2304\n",
      "Layer 6 : 703/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 15/512\n",
      "Layer 10 : 35/4096\n",
      "Layer 11 : 22/4096\n",
      "----------------------------------------\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 71/576\n",
      "Layer 3 : 374/1152\n",
      "Layer 4 : 1314/2304\n",
      "Layer 5 : 575/2304\n",
      "Layer 6 : 704/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 15/512\n",
      "Layer 10 : 34/4096\n",
      "Layer 11 : 21/4096\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Proj_mat_lst =[]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    \n",
    "    feature_list = []\n",
    "    merged_feat_mat = []\n",
    "    for batch, (x, y) in enumerate(remain_loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        mat_list = get_representation_matrix(model, x, batch_list=[256]*30)\n",
    "        break\n",
    "    threshold = 0.99\n",
    "    merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "    proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "    Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "[train] epoch 0, batch 29, loss 3.4053196907043457\n",
      "0.9604, 0.9298, 0.9241, 0.4011, 0.8676, 0.7022, 0.7334, 0.8940, 0.5361, 0.7887, Acc_f: 0.4011, Acc_r: 0.8152\n",
      "[train] epoch 1, batch 29, loss 2.8472275733947754\n",
      "0.9708, 0.9300, 0.9113, 0.2991, 0.8514, 0.5680, 0.6323, 0.8861, 0.3536, 0.7335, Acc_f: 0.2991, Acc_r: 0.7597\n",
      "[train] epoch 2, batch 29, loss 2.791172742843628\n",
      "0.9759, 0.9294, 0.9012, 0.2398, 0.8426, 0.4958, 0.5210, 0.8752, 0.2452, 0.6978, Acc_f: 0.2398, Acc_r: 0.7205\n",
      "[train] epoch 3, batch 29, loss 2.5219521522521973\n",
      "0.9771, 0.9302, 0.8927, 0.2065, 0.8419, 0.4518, 0.4734, 0.8692, 0.1880, 0.6796, Acc_f: 0.2065, Acc_r: 0.7004\n",
      "[train] epoch 4, batch 29, loss 2.573842763900757\n",
      "0.9788, 0.9276, 0.8872, 0.1811, 0.8419, 0.4253, 0.4380, 0.8653, 0.1536, 0.6639, Acc_f: 0.1811, Acc_r: 0.6869\n",
      "[train] epoch 5, batch 29, loss 2.4584286212921143\n",
      "0.9788, 0.9272, 0.8850, 0.1582, 0.8450, 0.4165, 0.4183, 0.8603, 0.1199, 0.6658, Acc_f: 0.1582, Acc_r: 0.6797\n",
      "[train] epoch 6, batch 29, loss 2.439899444580078\n",
      "0.9788, 0.9276, 0.8800, 0.1468, 0.8474, 0.4144, 0.4315, 0.8559, 0.1060, 0.6671, Acc_f: 0.1468, Acc_r: 0.6787\n",
      "[train] epoch 7, batch 29, loss 2.474731206893921\n",
      "0.9788, 0.9296, 0.8771, 0.1256, 0.8482, 0.4077, 0.4249, 0.8524, 0.0873, 0.6702, Acc_f: 0.1256, Acc_r: 0.6751\n",
      "[train] epoch 8, batch 29, loss 2.424762010574341\n",
      "0.9788, 0.9294, 0.8732, 0.1145, 0.8490, 0.4081, 0.4274, 0.8499, 0.0705, 0.6708, Acc_f: 0.1145, Acc_r: 0.6730\n",
      "[train] epoch 9, batch 29, loss 2.433086395263672\n",
      "0.9782, 0.9296, 0.8715, 0.1076, 0.8522, 0.4178, 0.4502, 0.8479, 0.0627, 0.6828, Acc_f: 0.1076, Acc_r: 0.6770\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "[train] epoch 0, batch 29, loss 3.301210880279541\n",
      "0.9484, 0.8513, 0.8233, 0.3064, 0.9386, 0.6548, 0.7572, 0.9000, 0.3880, 0.8596, Acc_f: 0.3064, Acc_r: 0.7912\n",
      "[train] epoch 1, batch 29, loss 2.874919891357422\n",
      "0.9599, 0.8486, 0.7975, 0.2026, 0.9441, 0.5696, 0.6211, 0.8772, 0.2530, 0.8596, Acc_f: 0.2026, Acc_r: 0.7478\n",
      "[train] epoch 2, batch 29, loss 2.6659328937530518\n",
      "0.9644, 0.8506, 0.7879, 0.1659, 0.9461, 0.5315, 0.5372, 0.8683, 0.1904, 0.8508, Acc_f: 0.1659, Acc_r: 0.7252\n",
      "[train] epoch 3, batch 29, loss 2.5978920459747314\n",
      "0.9650, 0.8549, 0.7766, 0.1357, 0.9453, 0.5126, 0.4967, 0.8623, 0.1518, 0.8433, Acc_f: 0.1357, Acc_r: 0.7120\n",
      "[train] epoch 4, batch 29, loss 2.573817014694214\n",
      "0.9662, 0.8602, 0.7686, 0.1239, 0.9437, 0.5113, 0.4841, 0.8583, 0.1271, 0.8433, Acc_f: 0.1239, Acc_r: 0.7070\n",
      "[train] epoch 5, batch 29, loss 2.5473673343658447\n",
      "0.9679, 0.8668, 0.7599, 0.1114, 0.9402, 0.5147, 0.4765, 0.8529, 0.1078, 0.8395, Acc_f: 0.1114, Acc_r: 0.7029\n",
      "[train] epoch 6, batch 29, loss 2.4942145347595215\n",
      "0.9685, 0.8700, 0.7515, 0.1079, 0.9362, 0.5214, 0.4800, 0.8489, 0.0946, 0.8332, Acc_f: 0.1079, Acc_r: 0.7005\n",
      "[train] epoch 7, batch 29, loss 2.4740712642669678\n",
      "0.9685, 0.8751, 0.7460, 0.1027, 0.9342, 0.5331, 0.4831, 0.8430, 0.0886, 0.8345, Acc_f: 0.1027, Acc_r: 0.7007\n",
      "[train] epoch 8, batch 29, loss 2.4859657287597656\n",
      "0.9685, 0.8796, 0.7457, 0.0968, 0.9310, 0.5394, 0.4861, 0.8420, 0.0777, 0.8339, Acc_f: 0.0968, Acc_r: 0.7004\n",
      "[train] epoch 9, batch 29, loss 2.403878927230835\n",
      "0.9685, 0.8839, 0.7402, 0.0892, 0.9298, 0.5453, 0.4871, 0.8380, 0.0723, 0.8345, Acc_f: 0.0892, Acc_r: 0.7000\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "[train] epoch 0, batch 29, loss 3.14919376373291\n",
      "0.8979, 0.8937, 0.9540, 0.3681, 0.9076, 0.7651, 0.8376, 0.8460, 0.6030, 0.7492, Acc_f: 0.3681, Acc_r: 0.8282\n",
      "[train] epoch 1, batch 29, loss 2.7209525108337402\n",
      "0.9255, 0.8961, 0.9535, 0.2450, 0.9140, 0.6921, 0.8058, 0.8484, 0.4807, 0.7448, Acc_f: 0.2450, Acc_r: 0.8068\n",
      "[train] epoch 2, batch 29, loss 2.581815481185913\n",
      "0.9352, 0.8990, 0.9499, 0.1738, 0.9148, 0.6518, 0.7911, 0.8455, 0.4060, 0.7549, Acc_f: 0.1738, Acc_r: 0.7942\n",
      "[train] epoch 3, batch 29, loss 2.51658034324646\n",
      "0.9398, 0.9015, 0.9453, 0.1305, 0.9168, 0.6187, 0.7800, 0.8455, 0.3355, 0.7668, Acc_f: 0.1305, Acc_r: 0.7833\n",
      "[train] epoch 4, batch 29, loss 2.4265329837799072\n",
      "0.9427, 0.9053, 0.9417, 0.0836, 0.9180, 0.5986, 0.7688, 0.8420, 0.2952, 0.7655, Acc_f: 0.0836, Acc_r: 0.7753\n",
      "[train] epoch 5, batch 29, loss 2.4517102241516113\n",
      "0.9461, 0.9086, 0.9373, 0.0645, 0.9184, 0.5814, 0.7739, 0.8346, 0.2614, 0.7705, Acc_f: 0.0645, Acc_r: 0.7702\n",
      "[train] epoch 6, batch 29, loss 2.522763252258301\n",
      "0.9461, 0.9102, 0.9340, 0.0507, 0.9180, 0.5671, 0.7734, 0.8281, 0.2307, 0.7806, Acc_f: 0.0507, Acc_r: 0.7653\n",
      "[train] epoch 7, batch 29, loss 2.487062692642212\n",
      "0.9461, 0.9127, 0.9320, 0.0385, 0.9187, 0.5587, 0.7724, 0.8276, 0.2060, 0.7799, Acc_f: 0.0385, Acc_r: 0.7616\n",
      "[train] epoch 8, batch 29, loss 2.4069955348968506\n",
      "0.9455, 0.9139, 0.9306, 0.0285, 0.9184, 0.5575, 0.7769, 0.8242, 0.1910, 0.7824, Acc_f: 0.0285, Acc_r: 0.7600\n",
      "[train] epoch 9, batch 29, loss 2.452665328979492\n",
      "0.9461, 0.9159, 0.9291, 0.0198, 0.9176, 0.5419, 0.7754, 0.8247, 0.1789, 0.7875, Acc_f: 0.0198, Acc_r: 0.7575\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label + Subspace Acc_f: 2.88 \\pm 1.97\n",
      "Random label + Subspace Acc_r: 94.61 \\pm 0.70\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9521, 0.9499, 0.9363])\n",
    "Acc_f = 100*np.array([0.0160, 0.0139, 0.0566])\n",
    "\n",
    "print(f'Random label + Subspace Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label + Subspace Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "[train] epoch 0, batch 29, loss 0.5064550042152405\n",
      "0.8234, 0.9339, 0.9332, 0.0014, 0.8573, 0.7626, 0.4057, 0.8182, 0.9295, 0.8420, Acc_f: 0.0014, Acc_r: 0.8118\n",
      "[train] epoch 1, batch 29, loss 0.4426722228527069\n",
      "0.8698, 0.9139, 0.9460, 0.0000, 0.8672, 0.9065, 0.5286, 0.8697, 0.8530, 0.7655, Acc_f: 0.0000, Acc_r: 0.8356\n",
      "[train] epoch 2, batch 29, loss 0.21191827952861786\n",
      "0.8790, 0.9208, 0.9487, 0.0000, 0.8648, 0.8796, 0.5468, 0.8643, 0.8717, 0.7455, Acc_f: 0.0000, Acc_r: 0.8357\n",
      "[train] epoch 3, batch 29, loss 0.19525010883808136\n",
      "0.8888, 0.9361, 0.9405, 0.0000, 0.8688, 0.8586, 0.5362, 0.8529, 0.8892, 0.7624, Acc_f: 0.0000, Acc_r: 0.8370\n",
      "[train] epoch 4, batch 29, loss 0.38105636835098267\n",
      "0.8905, 0.9082, 0.9409, 0.0000, 0.8664, 0.9207, 0.5938, 0.8985, 0.8187, 0.7241, Acc_f: 0.0000, Acc_r: 0.8402\n",
      "[train] epoch 5, batch 29, loss 0.24008722603321075\n",
      "0.9008, 0.9390, 0.9361, 0.0000, 0.8736, 0.8377, 0.5862, 0.8554, 0.8867, 0.7379, Acc_f: 0.0000, Acc_r: 0.8393\n",
      "[train] epoch 6, batch 29, loss 0.20352794229984283\n",
      "0.8956, 0.9308, 0.9436, 0.0000, 0.8613, 0.8624, 0.5701, 0.8648, 0.8783, 0.7304, Acc_f: 0.0000, Acc_r: 0.8375\n",
      "[train] epoch 7, batch 29, loss 0.17551587522029877\n",
      "0.9071, 0.9341, 0.9450, 0.0000, 0.8736, 0.8582, 0.6414, 0.8618, 0.8572, 0.7442, Acc_f: 0.0000, Acc_r: 0.8470\n",
      "[train] epoch 8, batch 29, loss 0.12137206643819809\n",
      "0.8847, 0.9365, 0.9395, 0.0000, 0.8700, 0.8654, 0.5781, 0.8529, 0.8831, 0.7335, Acc_f: 0.0000, Acc_r: 0.8382\n",
      "[train] epoch 9, batch 29, loss 0.18084724247455597\n",
      "0.8893, 0.9282, 0.9475, 0.0000, 0.8704, 0.8733, 0.6211, 0.8663, 0.8554, 0.7329, Acc_f: 0.0000, Acc_r: 0.8427\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "[train] epoch 0, batch 29, loss 0.6425192356109619\n",
      "0.8245, 0.8058, 0.8014, 0.0007, 0.8819, 0.8918, 0.8761, 0.9222, 0.5253, 0.8119, Acc_f: 0.0007, Acc_r: 0.8157\n",
      "[train] epoch 1, batch 29, loss 0.6324235200881958\n",
      "0.8670, 0.8282, 0.8462, 0.0000, 0.9025, 0.8591, 0.8867, 0.9212, 0.4765, 0.7580, Acc_f: 0.0000, Acc_r: 0.8162\n",
      "[train] epoch 2, batch 29, loss 0.27964717149734497\n",
      "0.8968, 0.8247, 0.8636, 0.0000, 0.9069, 0.8091, 0.8857, 0.9183, 0.5940, 0.7580, Acc_f: 0.0000, Acc_r: 0.8286\n",
      "[train] epoch 3, batch 29, loss 0.2595446705818176\n",
      "0.8899, 0.8268, 0.8677, 0.0000, 0.8997, 0.8041, 0.8781, 0.9208, 0.6283, 0.7793, Acc_f: 0.0000, Acc_r: 0.8327\n",
      "[train] epoch 4, batch 29, loss 0.25783443450927734\n",
      "0.8733, 0.8160, 0.8604, 0.0000, 0.9037, 0.8201, 0.8938, 0.9178, 0.5235, 0.7223, Acc_f: 0.0000, Acc_r: 0.8145\n",
      "[train] epoch 5, batch 29, loss 0.15602751076221466\n",
      "0.9083, 0.8288, 0.8684, 0.0000, 0.9108, 0.7525, 0.8897, 0.9128, 0.5873, 0.7536, Acc_f: 0.0000, Acc_r: 0.8236\n",
      "[train] epoch 6, batch 29, loss 0.7395595908164978\n",
      "0.8716, 0.7860, 0.8443, 0.0000, 0.8755, 0.8813, 0.8705, 0.9336, 0.4560, 0.7072, Acc_f: 0.0000, Acc_r: 0.8029\n",
      "[train] epoch 7, batch 29, loss 0.23240023851394653\n",
      "0.9295, 0.8413, 0.8711, 0.0000, 0.8985, 0.7122, 0.8816, 0.9208, 0.6506, 0.7448, Acc_f: 0.0000, Acc_r: 0.8278\n",
      "[train] epoch 8, batch 29, loss 0.2058006227016449\n",
      "0.9203, 0.8478, 0.8732, 0.0000, 0.9092, 0.7345, 0.8842, 0.9074, 0.6331, 0.7455, Acc_f: 0.0000, Acc_r: 0.8284\n",
      "[train] epoch 9, batch 29, loss 0.20770418643951416\n",
      "0.9088, 0.8309, 0.8477, 0.0000, 0.9061, 0.7659, 0.8953, 0.9094, 0.6235, 0.7643, Acc_f: 0.0000, Acc_r: 0.8280\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "[train] epoch 0, batch 29, loss 0.4471432864665985\n",
      "0.7764, 0.8774, 0.9612, 0.0003, 0.8553, 0.8880, 0.7466, 0.7390, 0.8404, 0.6464, Acc_f: 0.0003, Acc_r: 0.8145\n",
      "[train] epoch 1, batch 29, loss 0.2539179027080536\n",
      "0.7815, 0.8898, 0.9617, 0.0000, 0.8605, 0.8985, 0.7112, 0.7573, 0.8392, 0.5680, Acc_f: 0.0000, Acc_r: 0.8075\n",
      "[train] epoch 2, batch 29, loss 0.3278522491455078\n",
      "0.8096, 0.8876, 0.9561, 0.0000, 0.8716, 0.8939, 0.7380, 0.7519, 0.8416, 0.6013, Acc_f: 0.0000, Acc_r: 0.8168\n",
      "[train] epoch 3, batch 29, loss 0.26995667815208435\n",
      "0.7729, 0.8649, 0.9585, 0.0000, 0.8644, 0.9283, 0.5893, 0.7737, 0.7235, 0.4295, Acc_f: 0.0000, Acc_r: 0.7672\n",
      "[train] epoch 4, batch 29, loss 0.16652090847492218\n",
      "0.8251, 0.8761, 0.9675, 0.0000, 0.8843, 0.9010, 0.7112, 0.7489, 0.7807, 0.5135, Acc_f: 0.0000, Acc_r: 0.8009\n",
      "[train] epoch 5, batch 29, loss 0.15790487825870514\n",
      "0.8222, 0.8770, 0.9634, 0.0000, 0.8787, 0.9069, 0.7420, 0.7538, 0.7711, 0.5053, Acc_f: 0.0000, Acc_r: 0.8023\n",
      "[train] epoch 6, batch 29, loss 0.17158664762973785\n",
      "0.8217, 0.8808, 0.9501, 0.0000, 0.8704, 0.9069, 0.7547, 0.7687, 0.8139, 0.6132, Acc_f: 0.0000, Acc_r: 0.8200\n",
      "[train] epoch 7, batch 29, loss 0.16417834162712097\n",
      "0.8291, 0.8788, 0.9588, 0.0000, 0.8732, 0.9048, 0.7582, 0.7930, 0.7717, 0.5636, Acc_f: 0.0000, Acc_r: 0.8146\n",
      "[train] epoch 8, batch 29, loss 0.18246765434741974\n",
      "0.8131, 0.8710, 0.9458, 0.0000, 0.8859, 0.9316, 0.7324, 0.7737, 0.7578, 0.6006, Acc_f: 0.0000, Acc_r: 0.8124\n",
      "[train] epoch 9, batch 29, loss 0.17919620871543884\n",
      "0.8446, 0.8833, 0.9503, 0.0000, 0.8827, 0.9014, 0.7547, 0.7831, 0.8193, 0.6320, Acc_f: 0.0000, Acc_r: 0.8279\n"
     ]
    }
   ],
   "source": [
    "def get_2nd_score(model, x, y):\n",
    "    indices = torch.topk(model(x), k=2, dim=1).indices\n",
    "    top1_matches = indices[:, 0] == y\n",
    "    selected_labels = torch.where(top1_matches, indices[:, 1], indices[:, 0])\n",
    "    return selected_labels\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.03)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_2nd_score(model, x, y.cuda())\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.00 \\pm 0.00\n",
      "UNSC Acc_r: 95.77 \\pm 0.41\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9597, 0.9613, 0.9520])\n",
    "Acc_f = 100*np.array([0.0000, 0.0000, 0.0000])\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================================================================================================\n",
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "[train] epoch 0, batch 29, loss 1.9348245859146118\n",
      "0.9719, 0.9396, 0.8479, 0.0965, 0.7753, 0.2433, 0.2944, 0.8103, 0.3446, 0.5762, Acc_f: 0.0965, Acc_r: 0.6448\n",
      "[train] epoch 1, batch 29, loss 1.7836272716522217\n",
      "0.9237, 0.9718, 0.7756, 0.0468, 0.6492, 0.1418, 0.2777, 0.7578, 0.3922, 0.6690, Acc_f: 0.0468, Acc_r: 0.6176\n",
      "[train] epoch 2, batch 29, loss 1.8350627422332764\n",
      "0.8979, 0.9733, 0.7510, 0.0291, 0.5549, 0.1007, 0.2949, 0.7395, 0.4012, 0.7279, Acc_f: 0.0291, Acc_r: 0.6046\n",
      "[train] epoch 3, batch 29, loss 1.7259252071380615\n",
      "0.8658, 0.9723, 0.7332, 0.0184, 0.5069, 0.0881, 0.3237, 0.7360, 0.4102, 0.7674, Acc_f: 0.0184, Acc_r: 0.6004\n",
      "[train] epoch 4, batch 29, loss 1.634499430656433\n",
      "0.8440, 0.9686, 0.7110, 0.0139, 0.4665, 0.0784, 0.3101, 0.7286, 0.3994, 0.7843, Acc_f: 0.0139, Acc_r: 0.5879\n",
      "[train] epoch 5, batch 29, loss 1.5350629091262817\n",
      "0.8280, 0.9671, 0.7004, 0.0118, 0.4522, 0.0784, 0.3414, 0.7330, 0.4325, 0.7762, Acc_f: 0.0118, Acc_r: 0.5899\n",
      "[train] epoch 6, batch 29, loss 1.672627568244934\n",
      "0.8200, 0.9618, 0.6956, 0.0083, 0.4487, 0.0805, 0.3500, 0.7524, 0.4319, 0.7824, Acc_f: 0.0083, Acc_r: 0.5915\n",
      "[train] epoch 7, batch 29, loss 1.6591473817825317\n",
      "0.8039, 0.9620, 0.6886, 0.0062, 0.4269, 0.0801, 0.3379, 0.7558, 0.4247, 0.7724, Acc_f: 0.0062, Acc_r: 0.5836\n",
      "[train] epoch 8, batch 29, loss 1.479451060295105\n",
      "0.8056, 0.9602, 0.6847, 0.0049, 0.4372, 0.0793, 0.3480, 0.7489, 0.4223, 0.7705, Acc_f: 0.0049, Acc_r: 0.5841\n",
      "[train] epoch 9, batch 29, loss 1.3666154146194458\n",
      "0.7964, 0.9582, 0.6802, 0.0038, 0.4376, 0.0789, 0.3536, 0.7519, 0.4452, 0.7737, Acc_f: 0.0038, Acc_r: 0.5862\n",
      "[train] epoch 10, batch 29, loss 1.4037411212921143\n",
      "0.7867, 0.9578, 0.6898, 0.0038, 0.4411, 0.0952, 0.3591, 0.7623, 0.4518, 0.7680, Acc_f: 0.0038, Acc_r: 0.5902\n",
      "[train] epoch 11, batch 29, loss 1.6359763145446777\n",
      "0.7833, 0.9563, 0.6884, 0.0031, 0.4522, 0.0969, 0.3399, 0.7543, 0.4361, 0.7561, Acc_f: 0.0031, Acc_r: 0.5848\n",
      "[train] epoch 12, batch 29, loss 1.4239991903305054\n",
      "0.7913, 0.9518, 0.6867, 0.0031, 0.4697, 0.0956, 0.3465, 0.7613, 0.4608, 0.7649, Acc_f: 0.0031, Acc_r: 0.5921\n",
      "[train] epoch 13, batch 29, loss 1.4315725564956665\n",
      "0.7804, 0.9506, 0.6886, 0.0031, 0.4669, 0.1036, 0.3450, 0.7632, 0.4494, 0.7567, Acc_f: 0.0031, Acc_r: 0.5894\n",
      "[train] epoch 14, batch 29, loss 1.2590159177780151\n",
      "0.7815, 0.9482, 0.6915, 0.0024, 0.4792, 0.1120, 0.3460, 0.7667, 0.4440, 0.7411, Acc_f: 0.0024, Acc_r: 0.5900\n",
      "====================================================================================================================================================================================\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "[train] epoch 0, batch 29, loss 2.004275321960449\n",
      "0.9255, 0.8776, 0.5842, 0.0229, 0.9635, 0.5357, 0.5797, 0.8281, 0.0590, 0.8439, Acc_f: 0.0229, Acc_r: 0.6886\n",
      "[train] epoch 1, batch 29, loss 1.988398551940918\n",
      "0.8928, 0.9088, 0.5360, 0.0035, 0.9612, 0.6112, 0.5943, 0.7940, 0.0235, 0.8790, Acc_f: 0.0035, Acc_r: 0.6890\n",
      "[train] epoch 2, batch 29, loss 1.7834904193878174\n",
      "0.8813, 0.9327, 0.5047, 0.0010, 0.9560, 0.6321, 0.6166, 0.7801, 0.0096, 0.8765, Acc_f: 0.0010, Acc_r: 0.6877\n",
      "[train] epoch 3, batch 29, loss 1.8143138885498047\n",
      "0.8681, 0.9392, 0.4753, 0.0000, 0.9509, 0.6313, 0.6176, 0.7667, 0.0036, 0.8853, Acc_f: 0.0000, Acc_r: 0.6820\n",
      "[train] epoch 4, batch 29, loss 1.7349861860275269\n",
      "0.8578, 0.9380, 0.4823, 0.0000, 0.9509, 0.6451, 0.6545, 0.7702, 0.0036, 0.8777, Acc_f: 0.0000, Acc_r: 0.6867\n",
      "[train] epoch 5, batch 29, loss 1.7631486654281616\n",
      "0.8503, 0.9388, 0.4888, 0.0000, 0.9469, 0.6342, 0.6333, 0.7737, 0.0012, 0.8727, Acc_f: 0.0000, Acc_r: 0.6822\n",
      "[train] epoch 6, batch 29, loss 1.6227620840072632\n",
      "0.8394, 0.9365, 0.4965, 0.0000, 0.9489, 0.6326, 0.6469, 0.7642, 0.0030, 0.8715, Acc_f: 0.0000, Acc_r: 0.6822\n",
      "[train] epoch 7, batch 29, loss 1.6455943584442139\n",
      "0.8412, 0.9361, 0.5148, 0.0000, 0.9409, 0.6149, 0.6555, 0.7727, 0.0030, 0.8539, Acc_f: 0.0000, Acc_r: 0.6815\n",
      "[train] epoch 8, batch 29, loss 1.5073304176330566\n",
      "0.8182, 0.9270, 0.5201, 0.0000, 0.9441, 0.6107, 0.6535, 0.7697, 0.0036, 0.8577, Acc_f: 0.0000, Acc_r: 0.6783\n",
      "[train] epoch 9, batch 29, loss 1.5420061349868774\n",
      "0.8217, 0.9347, 0.5213, 0.0000, 0.9251, 0.5864, 0.6667, 0.7692, 0.0042, 0.8483, Acc_f: 0.0000, Acc_r: 0.6753\n",
      "[train] epoch 10, batch 29, loss 1.57362699508667\n",
      "0.8091, 0.9314, 0.5401, 0.0000, 0.9199, 0.5910, 0.6464, 0.7628, 0.0042, 0.8370, Acc_f: 0.0000, Acc_r: 0.6713\n",
      "[train] epoch 11, batch 29, loss 1.5928637981414795\n",
      "0.7976, 0.9204, 0.5240, 0.0000, 0.9215, 0.5784, 0.6535, 0.7424, 0.0114, 0.8433, Acc_f: 0.0000, Acc_r: 0.6658\n",
      "[train] epoch 12, batch 29, loss 1.6235883235931396\n",
      "0.8217, 0.9231, 0.5705, 0.0000, 0.8938, 0.5763, 0.6328, 0.7692, 0.0139, 0.8238, Acc_f: 0.0000, Acc_r: 0.6695\n",
      "[train] epoch 13, batch 29, loss 1.4436806440353394\n",
      "0.8039, 0.9178, 0.5630, 0.0000, 0.8997, 0.5768, 0.6206, 0.7524, 0.0169, 0.8088, Acc_f: 0.0000, Acc_r: 0.6622\n",
      "[train] epoch 14, batch 29, loss 1.3849624395370483\n",
      "0.7878, 0.9065, 0.5524, 0.0000, 0.8973, 0.5902, 0.6222, 0.7405, 0.0247, 0.8138, Acc_f: 0.0000, Acc_r: 0.6595\n",
      "====================================================================================================================================================================================\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "[train] epoch 0, batch 29, loss 2.1058874130249023\n",
      "0.9564, 0.9347, 0.8848, 0.1454, 0.8934, 0.6342, 0.7355, 0.7667, 0.2663, 0.5072, Acc_f: 0.1454, Acc_r: 0.7310\n",
      "[train] epoch 1, batch 29, loss 1.7200597524642944\n",
      "0.9151, 0.9747, 0.8267, 0.0770, 0.8098, 0.6158, 0.7486, 0.6043, 0.2380, 0.4740, Acc_f: 0.0770, Acc_r: 0.6897\n",
      "[train] epoch 2, batch 29, loss 1.7818557024002075\n",
      "0.8687, 0.9859, 0.7881, 0.0392, 0.7416, 0.6137, 0.7521, 0.4884, 0.2349, 0.4777, Acc_f: 0.0392, Acc_r: 0.6612\n",
      "[train] epoch 3, batch 29, loss 1.6635340452194214\n",
      "0.8217, 0.9873, 0.7787, 0.0194, 0.7031, 0.6321, 0.7360, 0.4601, 0.2343, 0.4721, Acc_f: 0.0194, Acc_r: 0.6473\n",
      "[train] epoch 4, batch 29, loss 1.6687219142913818\n",
      "0.7907, 0.9863, 0.7705, 0.0076, 0.6813, 0.6284, 0.7248, 0.4329, 0.2373, 0.4809, Acc_f: 0.0076, Acc_r: 0.6370\n",
      "[train] epoch 5, batch 29, loss 1.3983087539672852\n",
      "0.7643, 0.9849, 0.7636, 0.0042, 0.6512, 0.6141, 0.7132, 0.4071, 0.2536, 0.4859, Acc_f: 0.0042, Acc_r: 0.6264\n",
      "[train] epoch 6, batch 29, loss 1.648146629333496\n",
      "0.7414, 0.9814, 0.7701, 0.0017, 0.6441, 0.6263, 0.7066, 0.4002, 0.2464, 0.4665, Acc_f: 0.0017, Acc_r: 0.6203\n",
      "[train] epoch 7, batch 29, loss 1.4403536319732666\n",
      "0.7288, 0.9780, 0.7674, 0.0007, 0.6326, 0.6158, 0.7041, 0.3754, 0.2560, 0.4759, Acc_f: 0.0007, Acc_r: 0.6149\n",
      "[train] epoch 8, batch 29, loss 1.4221981763839722\n",
      "0.7053, 0.9755, 0.7674, 0.0007, 0.6171, 0.6296, 0.6894, 0.3650, 0.2470, 0.4571, Acc_f: 0.0007, Acc_r: 0.6059\n",
      "[train] epoch 9, batch 29, loss 1.4520199298858643\n",
      "0.6909, 0.9731, 0.7648, 0.0007, 0.6092, 0.6258, 0.6889, 0.3457, 0.2524, 0.4665, Acc_f: 0.0007, Acc_r: 0.6019\n",
      "[train] epoch 10, batch 29, loss 1.366113543510437\n",
      "0.6869, 0.9696, 0.7684, 0.0007, 0.6009, 0.6145, 0.6803, 0.3408, 0.2512, 0.4608, Acc_f: 0.0007, Acc_r: 0.5970\n",
      "[train] epoch 11, batch 29, loss 1.3248291015625\n",
      "0.6823, 0.9671, 0.7691, 0.0003, 0.5949, 0.6133, 0.6803, 0.3299, 0.2765, 0.4665, Acc_f: 0.0003, Acc_r: 0.5978\n",
      "[train] epoch 12, batch 29, loss 1.455518126487732\n",
      "0.6766, 0.9663, 0.7633, 0.0003, 0.5858, 0.6099, 0.6773, 0.3165, 0.2651, 0.4621, Acc_f: 0.0003, Acc_r: 0.5914\n",
      "[train] epoch 13, batch 29, loss 1.3627829551696777\n",
      "0.6646, 0.9582, 0.7749, 0.0003, 0.5957, 0.6346, 0.6768, 0.3150, 0.2500, 0.4627, Acc_f: 0.0003, Acc_r: 0.5925\n",
      "[train] epoch 14, batch 29, loss 1.4069746732711792\n",
      "0.6720, 0.9592, 0.7643, 0.0000, 0.5818, 0.6158, 0.6717, 0.3160, 0.2458, 0.4545, Acc_f: 0.0000, Acc_r: 0.5868\n"
     ]
    }
   ],
   "source": [
    "from agents.adv import FGSM\n",
    "\n",
    "def find_adjacent_cls(adv_agent, x, y):\n",
    "    x_adv = adv_agent.perturb(x, y)\n",
    "    adv_logits = model(x_adv)\n",
    "    adv_pred = torch.argmax(adv_logits.data, 1)\n",
    "    return adv_pred, x_adv\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "\n",
    "    adv_agent = FGSM(deepcopy(model), bound=0.5, norm=False, random_start=True, device='cuda')\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    print('==='*60)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            adv_pred, x_adv = find_adjacent_cls(adv_agent, x, y)\n",
    "            adv_y = torch.argmax(model(x_adv), dim=1).detach().cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, adv_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================================================================================================\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "[train] epoch 0, batch 29, loss 2.134913921356201\n",
      "0.6325, 0.9645, 0.5924, 0.2065, 0.7063, 0.9723, 0.7274, 0.3725, 0.6199, 0.1498, Acc_f: 0.2065, Acc_r: 0.6375\n",
      "[train] epoch 1, batch 29, loss 2.041485071182251\n",
      "0.5476, 0.9873, 0.5577, 0.2738, 0.6595, 0.9354, 0.6935, 0.2744, 0.6614, 0.1135, Acc_f: 0.2738, Acc_r: 0.6034\n",
      "[train] epoch 2, batch 29, loss 1.991804599761963\n",
      "0.5126, 0.9890, 0.5421, 0.3123, 0.6377, 0.9299, 0.6748, 0.2382, 0.6849, 0.0903, Acc_f: 0.3123, Acc_r: 0.5888\n",
      "[train] epoch 3, batch 29, loss 1.968560814857483\n",
      "0.5178, 0.9910, 0.5401, 0.2890, 0.6457, 0.9195, 0.6884, 0.2402, 0.7120, 0.1034, Acc_f: 0.2890, Acc_r: 0.5953\n",
      "[train] epoch 4, batch 29, loss 1.9913945198059082\n",
      "0.5487, 0.9914, 0.6035, 0.2856, 0.6829, 0.9123, 0.6970, 0.2759, 0.7337, 0.1417, Acc_f: 0.2856, Acc_r: 0.6208\n",
      "[train] epoch 5, batch 29, loss 1.9811981916427612\n",
      "0.5677, 0.9918, 0.6226, 0.2720, 0.6932, 0.9102, 0.7248, 0.2942, 0.7651, 0.1661, Acc_f: 0.2720, Acc_r: 0.6373\n",
      "[train] epoch 6, batch 29, loss 1.9128860235214233\n",
      "0.5786, 0.9912, 0.6775, 0.2689, 0.7158, 0.9195, 0.7350, 0.3289, 0.7801, 0.1812, Acc_f: 0.2689, Acc_r: 0.6564\n",
      "[train] epoch 7, batch 29, loss 1.8777530193328857\n",
      "0.5958, 0.9924, 0.6763, 0.2644, 0.7582, 0.9119, 0.7516, 0.3556, 0.8006, 0.1925, Acc_f: 0.2644, Acc_r: 0.6705\n",
      "[train] epoch 8, batch 29, loss 1.842572569847107\n",
      "0.6365, 0.9908, 0.7344, 0.3591, 0.7772, 0.9203, 0.7683, 0.4131, 0.8169, 0.2351, Acc_f: 0.3591, Acc_r: 0.6992\n",
      "[train] epoch 9, batch 29, loss 1.9199988842010498\n",
      "0.6330, 0.9922, 0.7390, 0.2974, 0.7685, 0.9048, 0.7719, 0.4289, 0.8355, 0.2677, Acc_f: 0.2974, Acc_r: 0.7046\n",
      "[train] epoch 10, batch 29, loss 1.8599315881729126\n",
      "0.6514, 0.9886, 0.7816, 0.3439, 0.7883, 0.9207, 0.7764, 0.4720, 0.8500, 0.2671, Acc_f: 0.3439, Acc_r: 0.7218\n",
      "[train] epoch 11, batch 29, loss 1.7996186017990112\n",
      "0.6577, 0.9908, 0.7780, 0.3348, 0.7994, 0.9044, 0.7759, 0.4844, 0.8470, 0.2940, Acc_f: 0.3348, Acc_r: 0.7257\n",
      "[train] epoch 12, batch 29, loss 1.9656484127044678\n",
      "0.6760, 0.9896, 0.8062, 0.3168, 0.8070, 0.9052, 0.7931, 0.5171, 0.8536, 0.3530, Acc_f: 0.3168, Acc_r: 0.7445\n",
      "[train] epoch 13, batch 29, loss 1.8703277111053467\n",
      "0.6869, 0.9896, 0.8161, 0.2745, 0.8070, 0.8972, 0.7916, 0.5379, 0.8536, 0.3755, Acc_f: 0.2745, Acc_r: 0.7506\n",
      "[train] epoch 14, batch 29, loss 1.9712308645248413\n",
      "0.7231, 0.9873, 0.8494, 0.2786, 0.8181, 0.9077, 0.8113, 0.5899, 0.8620, 0.4326, Acc_f: 0.2786, Acc_r: 0.7757\n"
     ]
    }
   ],
   "source": [
    "from agents.adv import FGSM\n",
    "\n",
    "def find_adjacent_cls(adv_agent, x, y):\n",
    "    x_adv = adv_agent.perturb(x, y)\n",
    "    adv_logits = model(x_adv)\n",
    "    adv_pred = torch.argmax(adv_logits.data, 1)\n",
    "    return adv_pred, x_adv\n",
    "\n",
    "for i in range(2, 3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "\n",
    "    adv_agent = FGSM(deepcopy(model), bound=0.5, norm=False, random_start=True, device='cuda')\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    print('==='*60)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            adv_pred, x_adv = find_adjacent_cls(adv_agent, x, y)\n",
    "            adv_y = torch.argmax(model(x_adv), dim=1).detach().cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, adv_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary Unlearning Acc_f: 4.03 \\pm 1.87\n",
      "Boundary Unlearning Acc_r: 80.99 \\pm 4.01\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.8015, 0.8627,  0.7656])\n",
    "Acc_f = 100*np.array([0.0219, 0.0659,  0.0330])\n",
    "print(f'Boundary Unlearning Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Boundary Unlearning Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[train] epoch 0, batch 29, loss -0.8680832982063293\n",
      "0.9375, 0.9123, 0.9185, 0.5958, 0.8831, 0.7106, 0.7360, 0.8980, 0.7572, 0.8270, Acc_f: 0.5958, Acc_r: 0.8422\n",
      "[train] epoch 1, batch 29, loss -1.5237183570861816\n",
      "0.9650, 0.8847, 0.8997, 0.3838, 0.8744, 0.6460, 0.7562, 0.8970, 0.7102, 0.8232, Acc_f: 0.3838, Acc_r: 0.8285\n",
      "[train] epoch 2, batch 29, loss -2.780498743057251\n",
      "0.9845, 0.7896, 0.8094, 0.1367, 0.8109, 0.3272, 0.6864, 0.8544, 0.5849, 0.7812, Acc_f: 0.1367, Acc_r: 0.7365\n",
      "[train] epoch 3, batch 29, loss -7.0883965492248535\n",
      "0.9925, 0.2646, 0.3476, 0.0007, 0.4923, 0.0042, 0.3652, 0.6246, 0.3458, 0.5674, Acc_f: 0.0007, Acc_r: 0.4449\n",
      "[train] epoch 4, batch 29, loss -56.50541687011719\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 5, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 6, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 7, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 8, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 9, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 10, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 11, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 12, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 13, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 14, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 15, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 16, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 17, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 18, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "\n",
      "\n",
      "\n",
      "[train] epoch 0, batch 29, loss -1.680394172668457\n",
      "0.9243, 0.8466, 0.7445, 0.3282, 0.9116, 0.7051, 0.8862, 0.8900, 0.5410, 0.8959, Acc_f: 0.3282, Acc_r: 0.8161\n",
      "[train] epoch 1, batch 29, loss -3.338623285293579\n",
      "0.8985, 0.8098, 0.4731, 0.0840, 0.8593, 0.4299, 0.9125, 0.8098, 0.5139, 0.9348, Acc_f: 0.0840, Acc_r: 0.7380\n",
      "[train] epoch 2, batch 29, loss -7.202852249145508\n",
      "0.8182, 0.6148, 0.0593, 0.0017, 0.5248, 0.0566, 0.9413, 0.5513, 0.5157, 0.9536, Acc_f: 0.0017, Acc_r: 0.5595\n",
      "[train] epoch 3, batch 29, loss -18.718095779418945\n",
      "0.2534, 0.0696, 0.0000, 0.0000, 0.0103, 0.0000, 0.9929, 0.0857, 0.3482, 0.7737, Acc_f: 0.0000, Acc_r: 0.2815\n",
      "[train] epoch 4, batch 29, loss -1778.403076171875\n",
      "0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 5, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 6, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 7, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 8, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 9, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 10, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 11, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 12, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 13, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 14, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 15, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 16, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 17, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 18, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "\n",
      "\n",
      "\n",
      "[train] epoch 0, batch 29, loss -1.4298030138015747\n",
      "0.8767, 0.8804, 0.9325, 0.5173, 0.9001, 0.8121, 0.8397, 0.8197, 0.7283, 0.7787, Acc_f: 0.5173, Acc_r: 0.8409\n",
      "[train] epoch 1, batch 29, loss -2.612771511077881\n",
      "0.8899, 0.8374, 0.8807, 0.2238, 0.8823, 0.6648, 0.9059, 0.7464, 0.6922, 0.8803, Acc_f: 0.2238, Acc_r: 0.8200\n",
      "[train] epoch 2, batch 29, loss -7.749074459075928\n",
      "0.8825, 0.5593, 0.4984, 0.0069, 0.6504, 0.0818, 0.9570, 0.3928, 0.5958, 0.8527, Acc_f: 0.0069, Acc_r: 0.6078\n",
      "[train] epoch 3, batch 29, loss -37.47075271606445\n",
      "0.0206, 0.0000, 0.0002, 0.0000, 0.0004, 0.0000, 1.0000, 0.0000, 0.0006, 0.0483, Acc_f: 0.0000, Acc_r: 0.1189\n",
      "[train] epoch 4, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 5, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 6, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 7, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 8, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 9, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 10, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 11, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 12, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 13, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 14, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 15, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 16, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 17, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 18, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('\\n\\n')\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.00008)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(30):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = -criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Acc_f: 3.10 \\pm 3.90\n",
      "GA Acc_r: 75.11 \\pm 6.76\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.7419,  0.6733,  0.8381])\n",
    "Acc_f = 100*np.array([0.0070,  0.0000,  0.0861])\n",
    "\n",
    "print(f'GA Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'GA Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(device), orig_target.to(device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad2_acc += torch.mean(prob[:, y]) * p.grad.data.pow(2) \n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc /= len(train_loader)\n",
    "    \n",
    "def get_mean_var(args, p, alpha=1e-7):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3) \n",
    "    if p.size(0) == args.num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var \n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == args.num_classes:\n",
    "        mu[args.unlearn_class] = 0\n",
    "        var[args.unlearn_class] = 0.0001\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        var *= 10 \n",
    "    return mu, var\n",
    "\n",
    "def fisher_new(dataset, model):\n",
    "    for p in model.parameters():\n",
    "        p.data0 = copy.deepcopy(p.data.clone())\n",
    "    hessian(dataset, model)\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        mu, var = get_mean_var(args, p)\n",
    "        p.data = mu + var.sqrt() * torch.empty_like(p.data).normal_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 114/114 [00:41<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8010, 0.9774, 0.9255, 0.0000, 0.7380, 0.7898, 0.6459, 0.8539, 0.7916, 0.8094, Acc_f: 0.0000, Acc_r: 0.8147\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 114/114 [00:41<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8922, 0.9196, 0.7007, 0.0000, 0.8359, 0.8054, 0.8690, 0.9009, 0.4500, 0.8439, Acc_f: 0.0000, Acc_r: 0.8019\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 114/114 [00:40<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8091, 0.9149, 0.9679, 0.0000, 0.8419, 0.8859, 0.7790, 0.7954, 0.4807, 0.5937, Acc_f: 0.0000, Acc_r: 0.7854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remain_dataset = torch.utils.data.Subset(train_loader.dataset, remain_indices)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    fisher_model = copy.deepcopy(model)\n",
    "    fisher_new(remain_dataset, fisher_model)\n",
    "    test_by_class(fisher_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Acc_f: 2.53 \\pm 0.91\n",
      "Fisher Acc_r: 95.72 \\pm 0.30\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9596,  0.9590, 0.9530])\n",
    "Acc_f = 100*np.array([0.0378,  0.0163, 0.0219])\n",
    "\n",
    "print(f'Fisher Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Fisher Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(device), orig_target.to(device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad2_acc += torch.mean(prob[:, y]) * p.grad.data.pow(2) \n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc /= len(train_loader)\n",
    "    \n",
    "def get_mean_var(args, p, alpha=1.25e-7):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3) \n",
    "    if p.size(0) == args.num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var \n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == args.num_classes:\n",
    "        mu[unlearn_class] = 0\n",
    "        var[unlearn_class] = 0.0001\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        var *= 10 \n",
    "    return mu, var\n",
    "\n",
    "def fisher_new(dataset, model):\n",
    "    for p in model.parameters():\n",
    "        p.data0 = copy.deepcopy(p.data.clone())\n",
    "    hessian(dataset, model)\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        mu, var = get_mean_var(args, p)\n",
    "        p.data = mu + var.sqrt() * torch.empty_like(p.data).normal_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_class = 6\n",
    "remain_class = list(set(list(range(10))) -set([unlearn_class]))\n",
    "train_targets_list = np.array(train_loader.dataset.labels)\n",
    "remain_cls_indices = np.where(~np.isin(train_targets_list, unlearn_class))[0]\n",
    "cls_sampler = torch.utils.data.SubsetRandomSampler(remain_cls_indices)\n",
    "remain_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            sampler=cls_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2111/2111 [03:04<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8245, 0.9335, 0.8952, 0.6325, 0.9041, 0.8192, 0.0000, 0.6196, 0.7084, 0.7937, Acc_f: 0.0000, Acc_r: 0.7923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.7923146250346373)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_model = copy.deepcopy(model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "remain_dataset = torch.utils.data.Subset(train_loader.dataset, remain_cls_indices)\n",
    "fisher_new(remain_dataset, fisher_model)\n",
    "test_by_class(fisher_model, test_loader, i=unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1402/4115894094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mremain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremain_cls_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_dataloader(args)\n",
    "remain_dataset = torch.utils.data.Subset(train_loader.dataset, remain_cls_indices)\n",
    "\n",
    "for i in range(1,4):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}_{i}.pth'))\n",
    "    test_by_class(model, test_loader, i=6)\n",
    "    fisher_model = copy.deepcopy(model)\n",
    "    fisher_new(remain_dataset, fisher_model)\n",
    "    test_by_class(fisher_model, test_loader, i=unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain 78.7433-5.6311\n",
      "Forget 1.0333-1.3225\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.8307, 0.8237, 0.7079])\n",
    "Acc_f = 100*np.array([0.0020, 0.0290, 0.0000])\n",
    "\n",
    "print(f'Remain {np.mean(Acc_r):.4f}-{np.std(Acc_r):.4f}')\n",
    "print(f'Forget {np.mean(Acc_f):.4f}-{np.std(Acc_f):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SalUn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = 'original_model_12-15-02-49'\n",
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# create saliency map\n",
    "def save_gradient_ratio(unlearn_train_loader, model, criterion, args, seed):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        args.unlearn_lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    gradients = {}\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        gradients[name] = 0\n",
    "\n",
    "    for i, (image, target) in enumerate(unlearn_train_loader):\n",
    "        image = image.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output_clean = model(image)\n",
    "        loss = - criterion(output_clean, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    gradients[name] += param.grad.data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name in gradients:\n",
    "            gradients[name] = torch.abs_(gradients[name])\n",
    "\n",
    "    threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    for i in threshold_list:\n",
    "        print(i)\n",
    "        sorted_dict_positions = {}\n",
    "        hard_dict = {}\n",
    "\n",
    "        # Concatenate all tensors into a single tensor\n",
    "        all_elements = - torch.cat([tensor.flatten() for tensor in gradients.values()])\n",
    "\n",
    "        # Calculate the threshold index for the top 10% elements\n",
    "        threshold_index = int(len(all_elements) * i)\n",
    "\n",
    "        # Calculate positions of all elements\n",
    "        positions = torch.argsort(all_elements)\n",
    "        ranks = torch.argsort(positions)\n",
    "\n",
    "        start_index = 0\n",
    "        for key, tensor in gradients.items():\n",
    "            num_elements = tensor.numel()\n",
    "            # tensor_positions = positions[start_index: start_index + num_elements]\n",
    "            tensor_ranks = ranks[start_index : start_index + num_elements]\n",
    "\n",
    "            sorted_positions = tensor_ranks.reshape(tensor.shape)\n",
    "            sorted_dict_positions[key] = sorted_positions\n",
    "\n",
    "            # Set the corresponding elements to 1\n",
    "            threshold_tensor = torch.zeros_like(tensor_ranks)\n",
    "            threshold_tensor[tensor_ranks < threshold_index] = 1\n",
    "            threshold_tensor = threshold_tensor.reshape(tensor.shape)\n",
    "            hard_dict[key] = threshold_tensor\n",
    "            start_index += num_elements\n",
    "\n",
    "        torch.save(hard_dict, f'./save/{args.dataset}/{args.model_name}/mask_threshold_{seed}_{i}.pt')\n",
    "\n",
    "\n",
    "args.unlearn_lr=0.01\n",
    "args.momentum=0.9\n",
    "args.weight_decay=5e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    save_gradient_ratio(unlearn_train_loader, model, criterion, args, args.seeds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.9151, 0.9202, 0.9140, 0.7359, 0.8803, 0.6774, 0.6606, 0.8841, 0.7578, 0.8176, Acc_f: 0.7359, Acc_r: 0.8252\n",
      "[train] epoch 0, batch 29, loss 4.167675971984863\n",
      "0.9753, 0.8763, 0.9253, 0.4511, 0.8910, 0.7110, 0.7137, 0.8945, 0.6048, 0.7298, Acc_f: 0.4511, Acc_r: 0.8135\n",
      "[train] epoch 1, batch 29, loss 3.2399892807006836\n",
      "0.9868, 0.8325, 0.9142, 0.3137, 0.8751, 0.6032, 0.6019, 0.8707, 0.4373, 0.5868, Acc_f: 0.3137, Acc_r: 0.7454\n",
      "[train] epoch 2, batch 29, loss 2.9124772548675537\n",
      "0.9903, 0.8007, 0.9021, 0.2450, 0.8644, 0.4975, 0.4669, 0.8455, 0.3247, 0.4809, Acc_f: 0.2450, Acc_r: 0.6859\n",
      "[train] epoch 3, batch 29, loss 2.7284464836120605\n",
      "0.9908, 0.7847, 0.8908, 0.1978, 0.8533, 0.4308, 0.3698, 0.8222, 0.2633, 0.4119, Acc_f: 0.1978, Acc_r: 0.6464\n",
      "[train] epoch 4, batch 29, loss 2.646198272705078\n",
      "0.9914, 0.7829, 0.8817, 0.1759, 0.8498, 0.3872, 0.3217, 0.8093, 0.2223, 0.3599, Acc_f: 0.1759, Acc_r: 0.6229\n",
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.9203, 0.8537, 0.8166, 0.5749, 0.9076, 0.7718, 0.8655, 0.9029, 0.5687, 0.8458, Acc_f: 0.5749, Acc_r: 0.8281\n",
      "[train] epoch 0, batch 29, loss 3.968151569366455\n",
      "0.9564, 0.8566, 0.7922, 0.3411, 0.9275, 0.6971, 0.8316, 0.9178, 0.4042, 0.8320, Acc_f: 0.3411, Acc_r: 0.8017\n",
      "[train] epoch 1, batch 29, loss 3.3404252529144287\n",
      "0.9690, 0.8539, 0.7686, 0.2488, 0.9298, 0.6141, 0.7618, 0.9163, 0.3012, 0.7969, Acc_f: 0.2488, Acc_r: 0.7680\n",
      "[train] epoch 2, batch 29, loss 3.1113243103027344\n",
      "0.9731, 0.8541, 0.7525, 0.1912, 0.9306, 0.5524, 0.6793, 0.9108, 0.2223, 0.7680, Acc_f: 0.1912, Acc_r: 0.7381\n",
      "[train] epoch 3, batch 29, loss 2.8793368339538574\n",
      "0.9742, 0.8588, 0.7383, 0.1509, 0.9326, 0.4987, 0.6080, 0.9004, 0.1861, 0.7367, Acc_f: 0.1509, Acc_r: 0.7149\n",
      "[train] epoch 4, batch 29, loss 2.7706456184387207\n",
      "0.9748, 0.8625, 0.7267, 0.1298, 0.9334, 0.4685, 0.5539, 0.8955, 0.1584, 0.7185, Acc_f: 0.1298, Acc_r: 0.6991\n",
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.8481, 0.8837, 0.9371, 0.6888, 0.8914, 0.7966, 0.7446, 0.8286, 0.7175, 0.6871, Acc_f: 0.6888, Acc_r: 0.8150\n",
      "[train] epoch 0, batch 29, loss 3.76859188079834\n",
      "0.9318, 0.8676, 0.9429, 0.5038, 0.9160, 0.7647, 0.8073, 0.8618, 0.6151, 0.6784, Acc_f: 0.5038, Acc_r: 0.8206\n",
      "[train] epoch 1, batch 29, loss 2.9845094680786133\n",
      "0.9662, 0.8384, 0.9340, 0.3969, 0.9156, 0.6955, 0.7607, 0.8509, 0.5012, 0.6150, Acc_f: 0.3969, Acc_r: 0.7864\n",
      "[train] epoch 2, batch 29, loss 2.889730930328369\n",
      "0.9765, 0.8068, 0.9183, 0.3230, 0.9065, 0.6162, 0.7006, 0.8346, 0.4265, 0.5480, Acc_f: 0.3230, Acc_r: 0.7482\n",
      "[train] epoch 3, batch 29, loss 2.699429512023926\n",
      "0.9811, 0.7921, 0.9031, 0.2568, 0.8997, 0.5516, 0.6368, 0.8177, 0.3584, 0.4803, Acc_f: 0.2568, Acc_r: 0.7134\n",
      "[train] epoch 4, batch 29, loss 2.5896220207214355\n",
      "0.9834, 0.7907, 0.8884, 0.2252, 0.8910, 0.5067, 0.5959, 0.7979, 0.3223, 0.4357, Acc_f: 0.2252, Acc_r: 0.6902\n",
      "MIA acc: 0.26 \\pm 0.37\n"
     ]
    }
   ],
   "source": [
    "from agents.svc_mia import SVC_MIA\n",
    "indice = remain_train_loader.sampler.indices\n",
    "neg_size = len(test_loader.sampler) + len(val_loader.sampler.indices)\n",
    "balanced_indice = np.random.choice(indice, size=neg_size, replace=False)\n",
    "balanced_sampler = torch.utils.data.SubsetRandomSampler(balanced_indice)\n",
    "balanced_train_loader = torch.utils.data.DataLoader(remain_train_loader.dataset,\n",
    "                                                    batch_size=args.batch_size,\n",
    "                                                    sampler=balanced_sampler)\n",
    "\n",
    "threshold = 0.8\n",
    "MIA_acc = np.zeros(3)\n",
    "for i in range(3):\n",
    "    print(\"=======\"*50)\n",
    "    mask = torch.load(f'./save/{args.dataset}/{args.model_name}/mask_threshold_{args.seeds[i]}_{threshold}.pt')\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(5):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for name, param in sgd_mr_model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= mask[name]\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "    MIA_acc[i] =  SVC_MIA(shadow_train=balanced_train_loader, \n",
    "            target_train=None, \n",
    "            target_test=unlearn_train_loader,\n",
    "            shadow_test=test_loader, \n",
    "            model=sgd_mr_model)\n",
    "print(f'MIA acc: {MIA_acc.mean():.2f} \\pm {MIA_acc.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalUn Acc_f: 3.56 \\pm 0.56\n",
      "SalUn Acc_r: 78.81 \\pm 4.85\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.7825,  0.8501,  0.7317])\n",
    "Acc_f = 100*np.array([0.0416,  0.0281,  0.0371])\n",
    "\n",
    "print(f'SalUn Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'SalUn Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
