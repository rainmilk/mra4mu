{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "from agents import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seeds',          type=int,       default=[2023, 2024, 2025])\n",
    "parser.add_argument('--dataset',        type=str,       default='cifar100')\n",
    "parser.add_argument('--batch_size',     type=int,       default=512)\n",
    "parser.add_argument('--model_name',     type=str,       default='resnet18')\n",
    "parser.add_argument('--retrain',        type=bool,      default=False)\n",
    "parser.add_argument('--unlearn_class',  type=list,      default=3)\n",
    "args = parser.parse_args(\"\")\n",
    "args.time_str = time.strftime(\"%m-%d-%H-%M\", time.localtime())\n",
    "if args.dataset.lower() == 'fmnist':\n",
    "    args.n_channels = 1\n",
    "else:\n",
    "    args.n_channels = 3\n",
    "\n",
    "if args.dataset.lower() == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "else:\n",
    "    args.num_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    10: {'unlearn_class': [65, 80, 51, 28, 85, 97, 67, 86, 19, 32],\n",
    "        'arxiv_name': 'retrain_model_12-20-03-04'},\n",
    "\n",
    "    20 : {'unlearn_class': [65, 80, 51, 28, 85, 97, 67, 86, 19, 32, \n",
    "                  11, 24, 56, 68, 90, 39, 42, 37, 89, 72],\n",
    "        'arxiv_name': 'retrain_model_12-20-11-14'},\n",
    "\n",
    "    30: {'unlearn_class': [65, 80, 51, 28, 85, 97, 67, 86, 19, 32, \n",
    "                  11, 24, 56, 68, 90, 39, 42, 37, 89, 72, \n",
    "                  71, 9,  48, 99, 49, 35, 3,  22, 12, 82],\n",
    "        'arxiv_name': 'retrain_model_12-22-00-15'},\n",
    "\n",
    "    40: {'unlearn_class': [65, 80, 51, 28, 85, 97, 67, 86, 19, 32, \n",
    "                        11, 24, 56, 68, 90, 39, 42, 37, 89, 72, \n",
    "                        71, 9, 48, 99, 49, 35, 3, 22, 12, 82, \n",
    "                        64, 61, 8, 79, 4, 60, 83, 66, 5, 23],\n",
    "        'arxiv_name': 'retrain_model_12-22-01-28'},\n",
    "\n",
    "    50: {'unlearn_class': [65, 80, 51, 28, 85, 97, 67, 86, 19, 32, \n",
    "                            11, 24, 56, 68, 90, 39, 42, 37, 89, 72, \n",
    "                            71, 9, 48, 99, 49, 35, 3, 22, 12, 82, \n",
    "                            64, 61, 8, 79, 4, 60, 83, 66, 5, 23, \n",
    "                            70, 33, 78, 13, 81, 43, 14, 40, 36, 34],\n",
    "        'arxiv_name': 'retrain_model_12-22-02-49'}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "train_targets_list = np.array(train_loader.dataset.targets)[train_loader.sampler.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unlearn 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100, 0.8300, 0.6100, 0.6500, 0.6700, 0.7900, 0.8200, 0.7700, 0.9000, 0.8600, 0.5200, 0.5000, 0.7900, 0.7400, 0.7300, 0.7600, 0.7500, 0.9000, 0.5900, 0.7100, 0.8600, 0.9000, 0.7700, 0.8000, 0.8200, 0.5700, 0.7700, 0.6100, 0.8100, 0.7500, 0.6600, 0.7500, 0.6900, 0.6700, 0.8200, 0.4400, 0.8600, 0.7600, 0.7300, 0.8900, 0.6300, 0.9000, 0.8000, 0.8400, 0.5100, 0.6400, 0.5100, 0.7100, 0.9400, 0.8700, 0.5600, 0.7300, 0.6600, 0.9100, 0.8100, 0.5200, 0.8700, 0.8200, 0.8700, 0.7000, 0.8900, 0.7500, 0.7900, 0.7000, 0.5900, 0.6200, 0.8300, 0.7100, 0.9400, 0.8300, 0.7500, 0.7900, 0.4700, 0.6400, 0.5800, 0.9000, 0.9200, 0.7000, 0.7100, 0.8200, 0.6000, 0.7700, 0.9200, 0.7200, 0.6700, 0.8500, 0.7500, 0.9000, 0.8500, 0.8900, 0.8400, 0.8400, 0.6700, 0.5600, 0.9300, 0.7400, 0.6700, 0.8100, 0.5500, 0.7100, Acc_f: 0.7280, Acc_r: 0.7491\n",
      "0.8800, 0.8700, 0.6700, 0.6100, 0.7100, 0.8000, 0.8400, 0.7900, 0.8600, 0.8600, 0.4800, 0.4300, 0.8300, 0.7000, 0.7300, 0.8100, 0.8000, 0.8400, 0.6800, 0.6600, 0.9000, 0.9200, 0.7500, 0.8700, 0.8200, 0.6000, 0.7000, 0.6200, 0.8100, 0.7100, 0.7200, 0.7800, 0.6700, 0.6100, 0.7900, 0.5800, 0.8300, 0.7600, 0.7300, 0.8700, 0.7100, 0.9000, 0.7900, 0.8300, 0.5200, 0.7000, 0.5400, 0.7300, 0.9500, 0.8800, 0.5900, 0.7200, 0.6800, 0.9400, 0.8800, 0.5700, 0.8800, 0.8200, 0.8000, 0.7500, 0.9100, 0.7100, 0.7600, 0.7200, 0.5100, 0.5800, 0.8100, 0.6200, 0.9400, 0.8500, 0.7700, 0.8200, 0.5500, 0.6100, 0.5400, 0.8900, 0.9100, 0.7100, 0.6700, 0.8200, 0.6100, 0.8100, 0.8900, 0.7000, 0.7300, 0.8500, 0.7100, 0.8600, 0.7600, 0.9100, 0.8800, 0.8400, 0.6500, 0.5500, 0.9100, 0.7400, 0.6300, 0.8100, 0.5900, 0.7300, Acc_f: 0.7040, Acc_r: 0.7543\n",
      "0.9100, 0.8500, 0.6400, 0.6300, 0.6600, 0.7600, 0.8100, 0.7800, 0.8900, 0.8600, 0.4800, 0.4500, 0.8400, 0.7000, 0.6900, 0.8100, 0.7700, 0.8600, 0.6500, 0.7000, 0.8900, 0.9200, 0.7300, 0.8100, 0.8000, 0.5900, 0.7300, 0.6600, 0.7900, 0.7200, 0.6700, 0.7200, 0.7100, 0.6800, 0.8200, 0.6300, 0.8300, 0.8300, 0.6700, 0.8900, 0.6900, 0.8900, 0.8300, 0.8700, 0.4900, 0.7000, 0.5000, 0.7100, 0.9600, 0.9200, 0.6100, 0.7700, 0.6400, 0.9300, 0.8600, 0.5200, 0.9000, 0.7800, 0.9100, 0.6900, 0.9100, 0.7500, 0.8000, 0.7100, 0.5600, 0.6500, 0.8100, 0.6400, 0.9600, 0.8600, 0.7800, 0.8600, 0.5000, 0.5900, 0.5800, 0.9100, 0.9000, 0.7100, 0.6600, 0.8200, 0.6600, 0.8100, 0.9200, 0.7000, 0.6800, 0.8400, 0.7400, 0.8600, 0.8300, 0.9100, 0.8200, 0.8500, 0.7000, 0.6500, 0.9400, 0.7500, 0.6900, 0.8000, 0.6000, 0.7400, Acc_f: 0.7300, Acc_r: 0.7572\n",
      "------------ Retrained model ------------\n",
      "0.9100, 0.8300, 0.6200, 0.6300, 0.6400, 0.7800, 0.8200, 0.7400, 0.9200, 0.8300, 0.5100, 0.5100, 0.8700, 0.6900, 0.7100, 0.7900, 0.7800, 0.8800, 0.6600, 0.0000, 0.8500, 0.9000, 0.7100, 0.7900, 0.8500, 0.6000, 0.7200, 0.6700, 0.0000, 0.7000, 0.7200, 0.7400, 0.0000, 0.6400, 0.8200, 0.5200, 0.8600, 0.8000, 0.7400, 0.8600, 0.7000, 0.9100, 0.8000, 0.8300, 0.5100, 0.6900, 0.5200, 0.7400, 0.9600, 0.8800, 0.6000, 0.0000, 0.6800, 0.9300, 0.8800, 0.5100, 0.9000, 0.8300, 0.8500, 0.7200, 0.8700, 0.7500, 0.8200, 0.7000, 0.6200, 0.0000, 0.8500, 0.0000, 0.9400, 0.8600, 0.7900, 0.8100, 0.5100, 0.6400, 0.6200, 0.9000, 0.9200, 0.7200, 0.6600, 0.8100, 0.0000, 0.7900, 0.9400, 0.7000, 0.6600, 0.0000, 0.0000, 0.8200, 0.8000, 0.9100, 0.8000, 0.8300, 0.6500, 0.6300, 0.9200, 0.7200, 0.6500, 0.0000, 0.5600, 0.7100, Acc_f: 0.0000, Acc_r: 0.7548\n",
      "0.9000, 0.8600, 0.6600, 0.5800, 0.6000, 0.7900, 0.8200, 0.7500, 0.8900, 0.8900, 0.5500, 0.4400, 0.8100, 0.7100, 0.7000, 0.7800, 0.7800, 0.8500, 0.6700, 0.0000, 0.8500, 0.8700, 0.7400, 0.8000, 0.8300, 0.6200, 0.7000, 0.6300, 0.0000, 0.7100, 0.6700, 0.7800, 0.0000, 0.6600, 0.8400, 0.5500, 0.8400, 0.7800, 0.7000, 0.9100, 0.7100, 0.8900, 0.7800, 0.8400, 0.5600, 0.7000, 0.5500, 0.6800, 0.9400, 0.8700, 0.6500, 0.0000, 0.6800, 0.9100, 0.9000, 0.4900, 0.9100, 0.7900, 0.8700, 0.7100, 0.9300, 0.7400, 0.7800, 0.7400, 0.6200, 0.0000, 0.8400, 0.0000, 0.9700, 0.8600, 0.7700, 0.8400, 0.5300, 0.6500, 0.5700, 0.9400, 0.9200, 0.8000, 0.6700, 0.8300, 0.0000, 0.8000, 0.9100, 0.7700, 0.7000, 0.0000, 0.0000, 0.8600, 0.8100, 0.9100, 0.7800, 0.8400, 0.6600, 0.5900, 0.9400, 0.7800, 0.6900, 0.0000, 0.6000, 0.6800, Acc_f: 0.0000, Acc_r: 0.7584\n",
      "0.9400, 0.8600, 0.6100, 0.6500, 0.6200, 0.7600, 0.8100, 0.7700, 0.9100, 0.8200, 0.5300, 0.5400, 0.8900, 0.6400, 0.6800, 0.8400, 0.7700, 0.8400, 0.6200, 0.0000, 0.8700, 0.8700, 0.7500, 0.8500, 0.8600, 0.6000, 0.7200, 0.6300, 0.0000, 0.6900, 0.6900, 0.7200, 0.0000, 0.6200, 0.8100, 0.5400, 0.8700, 0.8100, 0.7400, 0.8800, 0.6900, 0.9300, 0.7900, 0.8500, 0.5700, 0.6600, 0.5100, 0.7200, 0.9700, 0.9000, 0.6500, 0.0000, 0.7300, 0.9200, 0.9100, 0.5600, 0.8900, 0.8000, 0.8700, 0.7400, 0.9300, 0.6900, 0.7900, 0.6900, 0.6300, 0.0000, 0.7600, 0.0000, 0.9600, 0.8500, 0.8000, 0.8300, 0.5500, 0.6300, 0.6000, 0.9000, 0.9000, 0.7700, 0.6400, 0.8300, 0.0000, 0.7800, 0.9300, 0.7600, 0.7000, 0.0000, 0.0000, 0.8500, 0.8200, 0.8900, 0.8200, 0.8000, 0.6700, 0.6400, 0.9300, 0.7700, 0.6600, 0.0000, 0.6500, 0.7000, Acc_f: 0.0000, Acc_r: 0.7600\n",
      "Original model Acc_f: 72.07 \\pm 1.18\n",
      "Original model Acc_r: 75.36 \\pm 0.34\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 75.77 \\pm 0.22\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 10\n",
    "args.unlearn_class = dict[num_unlearn]['unlearn_class']\n",
    "arxiv_name_r = dict[num_unlearn]['arxiv_name']\n",
    "\n",
    "arxiv_name_o = 'original_model_12-20-05-11'\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_r}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "  \n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100, 0.8300, 0.6100, 0.6500, 0.6700, 0.7900, 0.8200, 0.7700, 0.9000, 0.8600, 0.5200, 0.5000, 0.7900, 0.7400, 0.7300, 0.7600, 0.7500, 0.9000, 0.5900, 0.7100, 0.8600, 0.9000, 0.7700, 0.8000, 0.8200, 0.5700, 0.7700, 0.6100, 0.8100, 0.7500, 0.6600, 0.7500, 0.6900, 0.6700, 0.8200, 0.4400, 0.8600, 0.7600, 0.7300, 0.8900, 0.6300, 0.9000, 0.8000, 0.8400, 0.5100, 0.6400, 0.5100, 0.7100, 0.9400, 0.8700, 0.5600, 0.7300, 0.6600, 0.9100, 0.8100, 0.5200, 0.8700, 0.8200, 0.8700, 0.7000, 0.8900, 0.7500, 0.7900, 0.7000, 0.5900, 0.6200, 0.8300, 0.7100, 0.9400, 0.8300, 0.7500, 0.7900, 0.4700, 0.6400, 0.5800, 0.9000, 0.9200, 0.7000, 0.7100, 0.8200, 0.6000, 0.7700, 0.9200, 0.7200, 0.6700, 0.8500, 0.7500, 0.9000, 0.8500, 0.8900, 0.8400, 0.8400, 0.6700, 0.5600, 0.9300, 0.7400, 0.6700, 0.8100, 0.5500, 0.7100, Acc_f: 0.7280, Acc_r: 0.7491\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 77/576\n",
      "Layer 3 : 248/576\n",
      "Layer 4 : 258/576\n",
      "Layer 5 : 445/576\n",
      "Layer 6 : 335/576\n",
      "Layer 7 : 930/1152\n",
      "Layer 8 : 52/64\n",
      "Layer 9 : 932/1152\n",
      "Layer 10 : 1016/1152\n",
      "Layer 11 : 952/1152\n",
      "Layer 12 : 1910/2304\n",
      "Layer 13 : 117/128\n",
      "Layer 14 : 1939/2304\n",
      "Layer 15 : 2003/2304\n",
      "Layer 16 : 1945/2304\n",
      "Layer 17 : 939/4608\n",
      "Layer 18 : 242/256\n",
      "Layer 19 : 916/4608\n",
      "Layer 20 : 847/4608\n",
      "Layer 21 : 118/512\n",
      "----------------------------------------\n",
      "0.8800, 0.8700, 0.6700, 0.6100, 0.7100, 0.8000, 0.8400, 0.7900, 0.8600, 0.8600, 0.4800, 0.4300, 0.8300, 0.7000, 0.7300, 0.8100, 0.8000, 0.8400, 0.6800, 0.6600, 0.9000, 0.9200, 0.7500, 0.8700, 0.8200, 0.6000, 0.7000, 0.6200, 0.8100, 0.7100, 0.7200, 0.7800, 0.6700, 0.6100, 0.7900, 0.5800, 0.8300, 0.7600, 0.7300, 0.8700, 0.7100, 0.9000, 0.7900, 0.8300, 0.5200, 0.7000, 0.5400, 0.7300, 0.9500, 0.8800, 0.5900, 0.7200, 0.6800, 0.9400, 0.8800, 0.5700, 0.8800, 0.8200, 0.8000, 0.7500, 0.9100, 0.7100, 0.7600, 0.7200, 0.5100, 0.5800, 0.8100, 0.6200, 0.9400, 0.8500, 0.7700, 0.8200, 0.5500, 0.6100, 0.5400, 0.8900, 0.9100, 0.7100, 0.6700, 0.8200, 0.6100, 0.8100, 0.8900, 0.7000, 0.7300, 0.8500, 0.7100, 0.8600, 0.7600, 0.9100, 0.8800, 0.8400, 0.6500, 0.5500, 0.9100, 0.7400, 0.6300, 0.8100, 0.5900, 0.7300, Acc_f: 0.7040, Acc_r: 0.7543\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 75/576\n",
      "Layer 3 : 242/576\n",
      "Layer 4 : 255/576\n",
      "Layer 5 : 446/576\n",
      "Layer 6 : 341/576\n",
      "Layer 7 : 946/1152\n",
      "Layer 8 : 53/64\n",
      "Layer 9 : 925/1152\n",
      "Layer 10 : 1010/1152\n",
      "Layer 11 : 948/1152\n",
      "Layer 12 : 1901/2304\n",
      "Layer 13 : 117/128\n",
      "Layer 14 : 1919/2304\n",
      "Layer 15 : 1995/2304\n",
      "Layer 16 : 1927/2304\n",
      "Layer 17 : 936/4608\n",
      "Layer 18 : 242/256\n",
      "Layer 19 : 914/4608\n",
      "Layer 20 : 845/4608\n",
      "Layer 21 : 116/512\n",
      "----------------------------------------\n",
      "0.9100, 0.8500, 0.6400, 0.6300, 0.6600, 0.7600, 0.8100, 0.7800, 0.8900, 0.8600, 0.4800, 0.4500, 0.8400, 0.7000, 0.6900, 0.8100, 0.7700, 0.8600, 0.6500, 0.7000, 0.8900, 0.9200, 0.7300, 0.8100, 0.8000, 0.5900, 0.7300, 0.6600, 0.7900, 0.7200, 0.6700, 0.7200, 0.7100, 0.6800, 0.8200, 0.6300, 0.8300, 0.8300, 0.6700, 0.8900, 0.6900, 0.8900, 0.8300, 0.8700, 0.4900, 0.7000, 0.5000, 0.7100, 0.9600, 0.9200, 0.6100, 0.7700, 0.6400, 0.9300, 0.8600, 0.5200, 0.9000, 0.7800, 0.9100, 0.6900, 0.9100, 0.7500, 0.8000, 0.7100, 0.5600, 0.6500, 0.8100, 0.6400, 0.9600, 0.8600, 0.7800, 0.8600, 0.5000, 0.5900, 0.5800, 0.9100, 0.9000, 0.7100, 0.6600, 0.8200, 0.6600, 0.8100, 0.9200, 0.7000, 0.6800, 0.8400, 0.7400, 0.8600, 0.8300, 0.9100, 0.8200, 0.8500, 0.7000, 0.6500, 0.9400, 0.7500, 0.6900, 0.8000, 0.6000, 0.7400, Acc_f: 0.7300, Acc_r: 0.7572\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 75/576\n",
      "Layer 3 : 240/576\n",
      "Layer 4 : 256/576\n",
      "Layer 5 : 448/576\n",
      "Layer 6 : 334/576\n",
      "Layer 7 : 937/1152\n",
      "Layer 8 : 52/64\n",
      "Layer 9 : 923/1152\n",
      "Layer 10 : 1016/1152\n",
      "Layer 11 : 946/1152\n",
      "Layer 12 : 1886/2304\n",
      "Layer 13 : 116/128\n",
      "Layer 14 : 1917/2304\n",
      "Layer 15 : 1992/2304\n",
      "Layer 16 : 1930/2304\n",
      "Layer 17 : 936/4608\n",
      "Layer 18 : 242/256\n",
      "Layer 19 : 913/4608\n",
      "Layer 20 : 843/4608\n",
      "Layer 21 : 119/512\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Proj_mat_lst =[]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    \n",
    "    feature_list = []\n",
    "    merged_feat_mat = []\n",
    "    for batch, (x, y) in enumerate(remain_train_loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        mat_list = get_representation_matrix(model, x, batch_list=[256]*30)\n",
    "        break\n",
    "    threshold = 0.99\n",
    "    merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "    proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "    Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100, 0.8300, 0.6100, 0.6500, 0.6700, 0.7900, 0.8200, 0.7700, 0.9000, 0.8600, 0.5200, 0.5000, 0.7900, 0.7400, 0.7300, 0.7600, 0.7500, 0.9000, 0.5900, 0.7100, 0.8600, 0.9000, 0.7700, 0.8000, 0.8200, 0.5700, 0.7700, 0.6100, 0.8100, 0.7500, 0.6600, 0.7500, 0.6900, 0.6700, 0.8200, 0.4400, 0.8600, 0.7600, 0.7300, 0.8900, 0.6300, 0.9000, 0.8000, 0.8400, 0.5100, 0.6400, 0.5100, 0.7100, 0.9400, 0.8700, 0.5600, 0.7300, 0.6600, 0.9100, 0.8100, 0.5200, 0.8700, 0.8200, 0.8700, 0.7000, 0.8900, 0.7500, 0.7900, 0.7000, 0.5900, 0.6200, 0.8300, 0.7100, 0.9400, 0.8300, 0.7500, 0.7900, 0.4700, 0.6400, 0.5800, 0.9000, 0.9200, 0.7000, 0.7100, 0.8200, 0.6000, 0.7700, 0.9200, 0.7200, 0.6700, 0.8500, 0.7500, 0.9000, 0.8500, 0.8900, 0.8400, 0.8400, 0.6700, 0.5600, 0.9300, 0.7400, 0.6700, 0.8100, 0.5500, 0.7100, Acc_f: 0.7280, Acc_r: 0.7491\n",
      "[train] epoch 0, batch 8, loss 1.8993651866912842\n",
      "0.9000, 0.8700, 0.5800, 0.7100, 0.6800, 0.7700, 0.8300, 0.7600, 0.9000, 0.8400, 0.6600, 0.4700, 0.7800, 0.7100, 0.6900, 0.7200, 0.8000, 0.8900, 0.5900, 0.1300, 0.8400, 0.9100, 0.7700, 0.8200, 0.8300, 0.6000, 0.7600, 0.6000, 0.0500, 0.7600, 0.6600, 0.8000, 0.0900, 0.6700, 0.8400, 0.4900, 0.8700, 0.7400, 0.7800, 0.9200, 0.6500, 0.9000, 0.7900, 0.8700, 0.5200, 0.6100, 0.5400, 0.6800, 0.9400, 0.8700, 0.6100, 0.2000, 0.7200, 0.9100, 0.8300, 0.5400, 0.8600, 0.8100, 0.8900, 0.7000, 0.8900, 0.7600, 0.7800, 0.6900, 0.6100, 0.0900, 0.8100, 0.0200, 0.9400, 0.8300, 0.7300, 0.7900, 0.4600, 0.7000, 0.5700, 0.8800, 0.9100, 0.7500, 0.7300, 0.8100, 0.0900, 0.8000, 0.9100, 0.7300, 0.6900, 0.2000, 0.1200, 0.9000, 0.8500, 0.9000, 0.8500, 0.8500, 0.6600, 0.6300, 0.8800, 0.7400, 0.6400, 0.2000, 0.5100, 0.6700, Acc_f: 0.1190, Acc_r: 0.7544\n",
      "[train] epoch 1, batch 8, loss 1.3634716272354126\n",
      "0.9200, 0.8500, 0.6000, 0.7100, 0.6800, 0.7700, 0.8100, 0.7600, 0.9000, 0.8200, 0.6800, 0.4900, 0.7800, 0.7000, 0.6800, 0.7600, 0.8200, 0.8900, 0.5700, 0.0500, 0.8400, 0.9000, 0.7700, 0.8300, 0.8200, 0.6000, 0.7600, 0.6200, 0.0100, 0.7500, 0.6600, 0.7900, 0.0100, 0.6600, 0.8400, 0.5000, 0.8700, 0.7600, 0.7700, 0.9100, 0.6800, 0.9000, 0.8200, 0.8700, 0.5400, 0.6100, 0.5600, 0.6800, 0.9400, 0.8700, 0.6300, 0.0900, 0.7200, 0.9100, 0.8300, 0.5200, 0.8300, 0.8200, 0.8800, 0.7100, 0.9000, 0.7200, 0.7700, 0.6900, 0.6100, 0.0600, 0.8100, 0.0100, 0.9400, 0.8100, 0.7300, 0.7700, 0.4600, 0.6800, 0.6000, 0.9100, 0.9100, 0.7800, 0.7200, 0.8200, 0.0400, 0.7900, 0.9000, 0.7100, 0.6700, 0.0500, 0.0800, 0.9000, 0.8400, 0.9100, 0.8500, 0.8400, 0.6800, 0.6200, 0.8800, 0.7400, 0.6400, 0.0300, 0.5100, 0.7000, Acc_f: 0.0430, Acc_r: 0.7552\n",
      "[train] epoch 2, batch 8, loss 1.1512203216552734\n",
      "0.9300, 0.8500, 0.5700, 0.7000, 0.6600, 0.7600, 0.8200, 0.7600, 0.8900, 0.8300, 0.6700, 0.4900, 0.7800, 0.7000, 0.6900, 0.7800, 0.8100, 0.9000, 0.5700, 0.0300, 0.8400, 0.9000, 0.7600, 0.8300, 0.8200, 0.6000, 0.7700, 0.6800, 0.0100, 0.7500, 0.6500, 0.7900, 0.0000, 0.6700, 0.8500, 0.5000, 0.8700, 0.7700, 0.7700, 0.9100, 0.6900, 0.9100, 0.8400, 0.8600, 0.5400, 0.6200, 0.5600, 0.6700, 0.9400, 0.8500, 0.6200, 0.0500, 0.7200, 0.9200, 0.8200, 0.5200, 0.8300, 0.8400, 0.8800, 0.7200, 0.9100, 0.7300, 0.7600, 0.6900, 0.6100, 0.0300, 0.8400, 0.0100, 0.9400, 0.8100, 0.7400, 0.7700, 0.4500, 0.6800, 0.6200, 0.9000, 0.9000, 0.7800, 0.7000, 0.8300, 0.0100, 0.7900, 0.9000, 0.7000, 0.6900, 0.0200, 0.0400, 0.9000, 0.8300, 0.9100, 0.8400, 0.8500, 0.6800, 0.6200, 0.8800, 0.7400, 0.6700, 0.0100, 0.5200, 0.7100, Acc_f: 0.0210, Acc_r: 0.7570\n",
      "[train] epoch 3, batch 8, loss 1.0510131120681763\n",
      "0.9300, 0.8500, 0.5800, 0.7200, 0.6700, 0.7700, 0.8100, 0.7600, 0.8900, 0.8300, 0.6600, 0.5000, 0.7800, 0.7000, 0.6900, 0.8000, 0.8100, 0.8900, 0.5700, 0.0200, 0.8400, 0.9000, 0.7500, 0.8300, 0.8200, 0.6000, 0.7500, 0.6700, 0.0000, 0.7500, 0.6500, 0.7900, 0.0100, 0.6700, 0.8500, 0.5000, 0.8800, 0.8000, 0.7700, 0.8800, 0.6900, 0.9000, 0.8300, 0.8400, 0.5400, 0.6300, 0.5500, 0.6700, 0.9400, 0.8500, 0.6400, 0.0200, 0.7100, 0.9200, 0.8200, 0.5300, 0.8200, 0.8400, 0.8700, 0.7400, 0.9000, 0.7300, 0.7500, 0.6900, 0.6100, 0.0200, 0.8300, 0.0000, 0.9400, 0.8100, 0.7400, 0.7700, 0.4700, 0.6700, 0.6200, 0.9100, 0.9000, 0.7700, 0.6900, 0.8300, 0.0100, 0.7900, 0.9000, 0.7000, 0.6700, 0.0200, 0.0200, 0.9000, 0.8300, 0.9000, 0.8300, 0.8400, 0.6900, 0.6200, 0.8700, 0.7400, 0.6900, 0.0000, 0.5200, 0.7200, Acc_f: 0.0120, Acc_r: 0.7564\n",
      "[train] epoch 4, batch 8, loss 0.9904049634933472\n",
      "0.9300, 0.8200, 0.5700, 0.7000, 0.6700, 0.7700, 0.8100, 0.7500, 0.8900, 0.8300, 0.6500, 0.5300, 0.7800, 0.7100, 0.7000, 0.8000, 0.8100, 0.8900, 0.5700, 0.0200, 0.8500, 0.9000, 0.7700, 0.8400, 0.8200, 0.6100, 0.7400, 0.6600, 0.0000, 0.7600, 0.6500, 0.7900, 0.0100, 0.6600, 0.8400, 0.5000, 0.8600, 0.7800, 0.7700, 0.8900, 0.6900, 0.9100, 0.8300, 0.8600, 0.5200, 0.6400, 0.5500, 0.6700, 0.9400, 0.8500, 0.6200, 0.0200, 0.6900, 0.9200, 0.8300, 0.5300, 0.8200, 0.8500, 0.8800, 0.7500, 0.9100, 0.7200, 0.7500, 0.7100, 0.5900, 0.0200, 0.8300, 0.0000, 0.9400, 0.8100, 0.7400, 0.7700, 0.4500, 0.6800, 0.6100, 0.9200, 0.9000, 0.7600, 0.6900, 0.8400, 0.0100, 0.7900, 0.9000, 0.7000, 0.6800, 0.0000, 0.0200, 0.9000, 0.8300, 0.9100, 0.8400, 0.8400, 0.6900, 0.6200, 0.8800, 0.7400, 0.7000, 0.0000, 0.5200, 0.7200, Acc_f: 0.0100, Acc_r: 0.7567\n",
      "[train] epoch 5, batch 8, loss 0.8879493474960327\n",
      "0.9300, 0.8000, 0.5900, 0.7100, 0.6700, 0.7800, 0.8000, 0.7500, 0.8900, 0.8300, 0.6500, 0.5400, 0.7800, 0.7100, 0.7000, 0.7600, 0.8100, 0.8900, 0.5700, 0.0200, 0.8500, 0.9000, 0.7800, 0.8400, 0.8200, 0.6200, 0.7400, 0.6600, 0.0000, 0.7500, 0.6500, 0.7900, 0.0100, 0.6500, 0.8500, 0.5000, 0.8600, 0.8000, 0.7600, 0.8800, 0.6800, 0.8900, 0.8500, 0.8600, 0.5300, 0.6300, 0.5500, 0.6500, 0.9400, 0.8500, 0.6000, 0.0100, 0.6800, 0.9200, 0.8300, 0.5300, 0.8200, 0.8600, 0.8800, 0.7500, 0.9100, 0.7100, 0.7500, 0.7100, 0.5800, 0.0100, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7700, 0.4700, 0.6600, 0.6200, 0.9200, 0.9000, 0.7600, 0.6800, 0.8400, 0.0100, 0.7900, 0.9200, 0.6900, 0.6700, 0.0000, 0.0200, 0.9000, 0.8300, 0.9200, 0.8300, 0.8500, 0.6900, 0.6300, 0.8900, 0.7400, 0.7200, 0.0000, 0.5200, 0.7300, Acc_f: 0.0080, Acc_r: 0.7568\n",
      "[train] epoch 6, batch 8, loss 0.8330430388450623\n",
      "0.9300, 0.7800, 0.5800, 0.7100, 0.6700, 0.7700, 0.8000, 0.7500, 0.8900, 0.8300, 0.6500, 0.5400, 0.7900, 0.7100, 0.7000, 0.8000, 0.8000, 0.8900, 0.5700, 0.0200, 0.8600, 0.9000, 0.7800, 0.8500, 0.8200, 0.6200, 0.7500, 0.6800, 0.0000, 0.7400, 0.6500, 0.7900, 0.0100, 0.6500, 0.8500, 0.5000, 0.8600, 0.8100, 0.7600, 0.8800, 0.6800, 0.8900, 0.8300, 0.8600, 0.5400, 0.6500, 0.5400, 0.6700, 0.9400, 0.8500, 0.6100, 0.0000, 0.6600, 0.9200, 0.8100, 0.5300, 0.8200, 0.8500, 0.8800, 0.7400, 0.9000, 0.6800, 0.7500, 0.7000, 0.6100, 0.0100, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7700, 0.4300, 0.7100, 0.6000, 0.9200, 0.9000, 0.7500, 0.6900, 0.8500, 0.0000, 0.7900, 0.9200, 0.6800, 0.6700, 0.0000, 0.0100, 0.9000, 0.8600, 0.9200, 0.8300, 0.8500, 0.7000, 0.6300, 0.8900, 0.7400, 0.7400, 0.0000, 0.5000, 0.7400, Acc_f: 0.0050, Acc_r: 0.7572\n",
      "[train] epoch 7, batch 8, loss 0.7476463317871094\n",
      "0.9300, 0.7800, 0.6000, 0.7100, 0.6600, 0.7700, 0.8000, 0.7600, 0.9000, 0.8300, 0.6400, 0.5200, 0.7900, 0.7100, 0.7000, 0.7800, 0.8000, 0.8900, 0.5800, 0.0200, 0.8500, 0.9000, 0.7900, 0.8400, 0.8200, 0.6200, 0.7600, 0.6800, 0.0000, 0.7300, 0.6500, 0.7900, 0.0100, 0.6400, 0.8400, 0.4800, 0.8600, 0.8100, 0.7600, 0.8800, 0.6800, 0.8900, 0.8600, 0.8500, 0.5400, 0.6700, 0.5300, 0.6600, 0.9500, 0.8500, 0.6300, 0.0000, 0.6500, 0.9200, 0.8200, 0.5000, 0.8200, 0.8500, 0.8800, 0.7400, 0.9000, 0.7100, 0.7500, 0.7100, 0.6000, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7700, 0.4400, 0.7000, 0.6000, 0.9200, 0.8900, 0.7400, 0.6800, 0.8400, 0.0000, 0.8000, 0.9200, 0.6800, 0.6700, 0.0000, 0.0200, 0.9100, 0.8500, 0.9200, 0.8200, 0.8500, 0.7000, 0.6300, 0.9100, 0.7400, 0.7500, 0.0000, 0.5200, 0.7400, Acc_f: 0.0050, Acc_r: 0.7572\n",
      "[train] epoch 8, batch 8, loss 0.7776649594306946\n",
      "0.9400, 0.7800, 0.5600, 0.7200, 0.6400, 0.7800, 0.8000, 0.7600, 0.8900, 0.8500, 0.6700, 0.5200, 0.7800, 0.7100, 0.7000, 0.7800, 0.7900, 0.8900, 0.5700, 0.0200, 0.8600, 0.8900, 0.7700, 0.8400, 0.8200, 0.6200, 0.7400, 0.6400, 0.0000, 0.7400, 0.6400, 0.7800, 0.0000, 0.6400, 0.8400, 0.4700, 0.8600, 0.8100, 0.7600, 0.8800, 0.6800, 0.8900, 0.8600, 0.8500, 0.5700, 0.6800, 0.5500, 0.6500, 0.9400, 0.8500, 0.6400, 0.0000, 0.6500, 0.9200, 0.8100, 0.5000, 0.8200, 0.8500, 0.8800, 0.7300, 0.9000, 0.6500, 0.7500, 0.7100, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7700, 0.4400, 0.6900, 0.5900, 0.9300, 0.8900, 0.7300, 0.6700, 0.8400, 0.0000, 0.8000, 0.9200, 0.6900, 0.6600, 0.0000, 0.0100, 0.9000, 0.8400, 0.9300, 0.8100, 0.8400, 0.7000, 0.6200, 0.9100, 0.7400, 0.7500, 0.0000, 0.5200, 0.7400, Acc_f: 0.0030, Acc_r: 0.7549\n",
      "[train] epoch 9, batch 8, loss 0.7310445308685303\n",
      "0.9400, 0.7900, 0.5500, 0.7100, 0.6500, 0.7800, 0.8000, 0.7600, 0.8900, 0.8500, 0.6600, 0.5400, 0.7900, 0.7100, 0.7000, 0.8200, 0.7900, 0.8900, 0.5700, 0.0200, 0.8600, 0.9000, 0.7900, 0.8300, 0.8100, 0.6200, 0.7400, 0.6400, 0.0000, 0.7300, 0.6500, 0.7800, 0.0000, 0.6400, 0.8400, 0.4800, 0.8600, 0.8100, 0.7600, 0.8800, 0.6900, 0.8900, 0.8300, 0.8400, 0.5600, 0.6700, 0.5400, 0.6500, 0.9400, 0.8500, 0.6100, 0.0000, 0.6500, 0.9200, 0.8100, 0.5100, 0.8400, 0.8400, 0.8800, 0.7400, 0.9000, 0.6800, 0.7600, 0.7200, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8200, 0.7400, 0.7600, 0.4500, 0.6900, 0.6100, 0.9300, 0.9000, 0.7500, 0.6500, 0.8400, 0.0000, 0.8000, 0.9200, 0.6800, 0.6600, 0.0000, 0.0100, 0.9000, 0.8500, 0.9400, 0.8400, 0.8400, 0.7000, 0.6300, 0.9200, 0.7400, 0.7500, 0.0000, 0.5300, 0.7400, Acc_f: 0.0030, Acc_r: 0.7567\n",
      "[train] epoch 10, batch 8, loss 0.644324779510498\n",
      "0.9400, 0.7900, 0.6000, 0.7300, 0.6600, 0.7800, 0.8000, 0.7600, 0.8900, 0.8400, 0.6600, 0.5300, 0.7900, 0.7100, 0.7000, 0.7900, 0.7800, 0.8900, 0.5900, 0.0100, 0.8500, 0.9000, 0.7800, 0.8400, 0.8100, 0.6200, 0.7300, 0.6500, 0.0000, 0.7500, 0.6400, 0.7700, 0.0000, 0.6400, 0.8400, 0.4700, 0.8600, 0.8000, 0.7600, 0.8800, 0.6900, 0.8800, 0.8500, 0.8500, 0.5400, 0.6700, 0.5300, 0.6500, 0.9500, 0.8500, 0.6500, 0.0000, 0.6500, 0.9200, 0.8200, 0.5200, 0.8200, 0.8500, 0.8700, 0.7500, 0.9000, 0.6900, 0.7700, 0.7200, 0.6000, 0.0000, 0.8400, 0.0000, 0.9400, 0.8200, 0.7400, 0.7600, 0.4400, 0.6800, 0.6000, 0.9200, 0.8900, 0.7500, 0.6500, 0.8400, 0.0000, 0.8000, 0.9300, 0.6800, 0.6500, 0.0000, 0.0100, 0.9100, 0.8500, 0.9300, 0.8500, 0.8400, 0.7000, 0.6300, 0.9200, 0.7400, 0.7500, 0.0000, 0.5300, 0.7400, Acc_f: 0.0020, Acc_r: 0.7570\n",
      "[train] epoch 11, batch 8, loss 0.6579180955886841\n",
      "0.9400, 0.7900, 0.5700, 0.7200, 0.6500, 0.7800, 0.8200, 0.7600, 0.8900, 0.8400, 0.6500, 0.5200, 0.7900, 0.7000, 0.7100, 0.8100, 0.7900, 0.8900, 0.5900, 0.0100, 0.8600, 0.9000, 0.7700, 0.8400, 0.8100, 0.6300, 0.7300, 0.6700, 0.0000, 0.7500, 0.6500, 0.7900, 0.0000, 0.6300, 0.8400, 0.4700, 0.8700, 0.8100, 0.7500, 0.8800, 0.6700, 0.8900, 0.8300, 0.8500, 0.5500, 0.6900, 0.5200, 0.6600, 0.9500, 0.8500, 0.6200, 0.0000, 0.6400, 0.9200, 0.8200, 0.5200, 0.8200, 0.8500, 0.8700, 0.7500, 0.9000, 0.6800, 0.7700, 0.7100, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7600, 0.4300, 0.6800, 0.5900, 0.9300, 0.8900, 0.7400, 0.6500, 0.8500, 0.0000, 0.8000, 0.9300, 0.6900, 0.6500, 0.0000, 0.0100, 0.9100, 0.8500, 0.9400, 0.8300, 0.8400, 0.7000, 0.6200, 0.9200, 0.7400, 0.7500, 0.0000, 0.5200, 0.7300, Acc_f: 0.0020, Acc_r: 0.7566\n",
      "[train] epoch 12, batch 8, loss 0.6379901170730591\n",
      "0.9400, 0.7900, 0.6100, 0.7200, 0.6600, 0.7700, 0.8100, 0.7500, 0.8900, 0.8300, 0.6500, 0.5000, 0.7900, 0.7100, 0.6900, 0.8300, 0.7900, 0.8900, 0.6000, 0.0000, 0.8600, 0.9000, 0.7600, 0.8300, 0.8100, 0.6200, 0.7300, 0.6500, 0.0000, 0.7400, 0.6400, 0.7700, 0.0000, 0.6300, 0.8400, 0.4900, 0.8600, 0.8000, 0.7700, 0.8700, 0.6600, 0.8800, 0.8400, 0.8400, 0.5600, 0.6700, 0.5200, 0.6500, 0.9500, 0.8500, 0.6600, 0.0000, 0.6400, 0.9200, 0.8100, 0.5100, 0.8200, 0.8500, 0.8700, 0.7600, 0.9000, 0.7000, 0.7800, 0.7200, 0.6100, 0.0000, 0.8300, 0.0000, 0.9400, 0.8300, 0.7400, 0.7600, 0.4500, 0.6800, 0.6000, 0.9300, 0.8900, 0.7300, 0.6600, 0.8500, 0.0000, 0.8000, 0.9300, 0.6800, 0.6500, 0.0000, 0.0100, 0.9100, 0.8300, 0.9300, 0.8300, 0.8400, 0.7000, 0.6300, 0.9200, 0.7400, 0.7400, 0.0000, 0.5300, 0.7400, Acc_f: 0.0010, Acc_r: 0.7561\n",
      "[train] epoch 13, batch 8, loss 0.6064724326133728\n",
      "0.9400, 0.8000, 0.6100, 0.7100, 0.6500, 0.7500, 0.8100, 0.7700, 0.9000, 0.8500, 0.6500, 0.5100, 0.7800, 0.7100, 0.6900, 0.8100, 0.7800, 0.8900, 0.6000, 0.0100, 0.8800, 0.9000, 0.7600, 0.8300, 0.8100, 0.6200, 0.7400, 0.6600, 0.0000, 0.7300, 0.6500, 0.7800, 0.0000, 0.6300, 0.8400, 0.4900, 0.8600, 0.8100, 0.7600, 0.8700, 0.6900, 0.8800, 0.8400, 0.8400, 0.5600, 0.6900, 0.5200, 0.6700, 0.9500, 0.8500, 0.6300, 0.0000, 0.6400, 0.9200, 0.8200, 0.5100, 0.8300, 0.8500, 0.8700, 0.7500, 0.9000, 0.7100, 0.7600, 0.7200, 0.5900, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7600, 0.4500, 0.6800, 0.5900, 0.9300, 0.8900, 0.7300, 0.6500, 0.8500, 0.0000, 0.8000, 0.9300, 0.6800, 0.6500, 0.0000, 0.0100, 0.9100, 0.8400, 0.9400, 0.8300, 0.8500, 0.7000, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5000, 0.7400, Acc_f: 0.0020, Acc_r: 0.7569\n",
      "[train] epoch 14, batch 8, loss 0.6078284382820129\n",
      "0.9400, 0.7900, 0.6000, 0.7200, 0.6400, 0.7600, 0.8100, 0.7500, 0.8900, 0.8400, 0.6600, 0.5200, 0.7800, 0.7000, 0.6900, 0.8300, 0.7800, 0.8700, 0.6000, 0.0100, 0.8500, 0.9000, 0.7700, 0.8400, 0.8100, 0.6200, 0.7500, 0.6600, 0.0000, 0.7500, 0.6600, 0.7700, 0.0000, 0.6300, 0.8400, 0.4900, 0.8700, 0.8100, 0.7500, 0.8700, 0.6700, 0.8800, 0.8300, 0.8400, 0.5500, 0.6900, 0.5200, 0.6600, 0.9500, 0.8500, 0.6600, 0.0000, 0.6400, 0.9200, 0.8200, 0.5200, 0.8300, 0.8500, 0.8700, 0.7500, 0.9000, 0.6900, 0.7800, 0.7200, 0.6000, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7600, 0.4400, 0.6800, 0.5900, 0.9400, 0.8900, 0.7400, 0.6600, 0.8600, 0.0000, 0.8100, 0.9400, 0.6500, 0.6600, 0.0000, 0.0100, 0.9100, 0.8300, 0.9300, 0.8200, 0.8400, 0.7000, 0.6300, 0.9200, 0.7400, 0.7500, 0.0000, 0.5300, 0.7300, Acc_f: 0.0020, Acc_r: 0.7567\n",
      "[train] epoch 15, batch 8, loss 0.6130837798118591\n",
      "0.9500, 0.8000, 0.6100, 0.6900, 0.6500, 0.7600, 0.8100, 0.7500, 0.8900, 0.8400, 0.6500, 0.5100, 0.7800, 0.7000, 0.7000, 0.8100, 0.7900, 0.8800, 0.5800, 0.0000, 0.8700, 0.9000, 0.7800, 0.8300, 0.8100, 0.6200, 0.7600, 0.6300, 0.0000, 0.7400, 0.6600, 0.7900, 0.0000, 0.6300, 0.8400, 0.4700, 0.8600, 0.8100, 0.7600, 0.8700, 0.6800, 0.8800, 0.8200, 0.8400, 0.5700, 0.6800, 0.5400, 0.6800, 0.9500, 0.8500, 0.6500, 0.0000, 0.6400, 0.9200, 0.8200, 0.5200, 0.8300, 0.8500, 0.8700, 0.7400, 0.9000, 0.6900, 0.7700, 0.7200, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7500, 0.4600, 0.6900, 0.5900, 0.9300, 0.9000, 0.7300, 0.6400, 0.8500, 0.0000, 0.8000, 0.9400, 0.6500, 0.6500, 0.0000, 0.0000, 0.9100, 0.8200, 0.9200, 0.8400, 0.8400, 0.7100, 0.6500, 0.9200, 0.7400, 0.7500, 0.0000, 0.5400, 0.7200, Acc_f: 0.0000, Acc_r: 0.7566\n",
      "[train] epoch 16, batch 8, loss 0.5560861825942993\n",
      "0.9400, 0.8000, 0.6100, 0.7100, 0.6500, 0.7700, 0.8100, 0.7700, 0.9000, 0.8500, 0.6200, 0.5300, 0.7900, 0.7600, 0.6900, 0.8000, 0.7700, 0.8800, 0.6000, 0.0100, 0.8700, 0.9000, 0.7600, 0.8300, 0.8100, 0.6200, 0.7500, 0.6600, 0.0000, 0.7400, 0.6300, 0.7800, 0.0000, 0.6300, 0.8400, 0.4800, 0.8700, 0.8000, 0.7500, 0.8700, 0.6700, 0.8800, 0.8300, 0.8500, 0.5600, 0.6800, 0.5300, 0.6600, 0.9500, 0.8500, 0.6600, 0.0000, 0.6500, 0.9200, 0.8200, 0.5100, 0.8300, 0.8300, 0.8700, 0.7200, 0.9000, 0.7100, 0.7600, 0.7200, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8200, 0.7400, 0.7500, 0.4500, 0.7000, 0.5900, 0.9400, 0.8900, 0.7300, 0.6400, 0.8600, 0.0000, 0.8000, 0.9400, 0.6600, 0.6600, 0.0000, 0.0000, 0.9100, 0.8100, 0.9300, 0.8300, 0.8400, 0.7000, 0.6300, 0.9200, 0.7400, 0.7500, 0.0000, 0.5200, 0.7400, Acc_f: 0.0010, Acc_r: 0.7564\n",
      "[train] epoch 17, batch 8, loss 0.6679028272628784\n",
      "0.9400, 0.8000, 0.6100, 0.7000, 0.6700, 0.7600, 0.8100, 0.7500, 0.9000, 0.8500, 0.6600, 0.5400, 0.7900, 0.7300, 0.6900, 0.7900, 0.7800, 0.8800, 0.5700, 0.0100, 0.8500, 0.8900, 0.7700, 0.8400, 0.8100, 0.6200, 0.7400, 0.6700, 0.0000, 0.7200, 0.6400, 0.7800, 0.0000, 0.6400, 0.8400, 0.4500, 0.8700, 0.8100, 0.7600, 0.8600, 0.6600, 0.8700, 0.8400, 0.8500, 0.5400, 0.6900, 0.5400, 0.6600, 0.9500, 0.8500, 0.6800, 0.0000, 0.6400, 0.9200, 0.8000, 0.5100, 0.8300, 0.8300, 0.8700, 0.7500, 0.9000, 0.7100, 0.7600, 0.7200, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7500, 0.7600, 0.4400, 0.6800, 0.5900, 0.9400, 0.8800, 0.7300, 0.6500, 0.8500, 0.0000, 0.8100, 0.9400, 0.6500, 0.6500, 0.0000, 0.0000, 0.9100, 0.8200, 0.9300, 0.8300, 0.8400, 0.7000, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5200, 0.7400, Acc_f: 0.0010, Acc_r: 0.7559\n",
      "[train] epoch 18, batch 8, loss 0.598790168762207\n",
      "0.9400, 0.8000, 0.6100, 0.6900, 0.6800, 0.7800, 0.8200, 0.7600, 0.9000, 0.8500, 0.6500, 0.5300, 0.7900, 0.7200, 0.6900, 0.7800, 0.7800, 0.8800, 0.5600, 0.0100, 0.8500, 0.9000, 0.7800, 0.8300, 0.8200, 0.6300, 0.7400, 0.6600, 0.0000, 0.7300, 0.6500, 0.7900, 0.0000, 0.6400, 0.8400, 0.4600, 0.8600, 0.7900, 0.7500, 0.8600, 0.6700, 0.8800, 0.8100, 0.8500, 0.5500, 0.6800, 0.5400, 0.6800, 0.9500, 0.8500, 0.6200, 0.0000, 0.6500, 0.9200, 0.8100, 0.4900, 0.8300, 0.8500, 0.8800, 0.7200, 0.9000, 0.6900, 0.7600, 0.7100, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7500, 0.7500, 0.4600, 0.6700, 0.6000, 0.9400, 0.9000, 0.7300, 0.6200, 0.8500, 0.0000, 0.8100, 0.9400, 0.6500, 0.6500, 0.0000, 0.0000, 0.9100, 0.8200, 0.9300, 0.8100, 0.8400, 0.7000, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5100, 0.7300, Acc_f: 0.0010, Acc_r: 0.7547\n",
      "[train] epoch 19, batch 8, loss 0.6719042658805847\n",
      "0.9500, 0.7900, 0.6100, 0.7100, 0.6700, 0.7500, 0.8200, 0.7700, 0.9000, 0.8500, 0.6300, 0.5300, 0.7900, 0.7500, 0.7200, 0.8100, 0.7800, 0.8800, 0.5800, 0.0000, 0.8700, 0.9000, 0.7800, 0.8400, 0.8100, 0.6300, 0.7300, 0.6600, 0.0000, 0.7300, 0.6500, 0.7900, 0.0000, 0.6400, 0.8400, 0.4700, 0.8700, 0.8000, 0.7500, 0.8600, 0.6800, 0.8800, 0.8300, 0.8200, 0.5400, 0.6900, 0.5400, 0.6600, 0.9500, 0.8500, 0.6700, 0.0000, 0.6400, 0.9200, 0.8100, 0.5100, 0.8300, 0.8300, 0.8700, 0.7300, 0.9000, 0.7000, 0.7600, 0.7100, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8200, 0.7500, 0.7500, 0.4400, 0.6700, 0.5900, 0.9400, 0.9000, 0.7300, 0.6400, 0.8500, 0.0000, 0.8000, 0.9400, 0.6600, 0.6500, 0.0000, 0.0100, 0.9000, 0.8200, 0.9300, 0.8300, 0.8400, 0.7000, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5200, 0.7400, Acc_f: 0.0010, Acc_r: 0.7564\n",
      "[train] epoch 20, batch 8, loss 0.521684467792511\n",
      "0.9500, 0.7900, 0.6100, 0.6900, 0.6600, 0.7700, 0.8300, 0.7600, 0.8900, 0.8600, 0.6600, 0.5300, 0.7900, 0.6900, 0.6900, 0.7800, 0.7700, 0.8800, 0.5900, 0.0000, 0.8800, 0.9000, 0.7700, 0.8400, 0.8100, 0.6100, 0.7300, 0.6900, 0.0000, 0.7300, 0.6500, 0.7900, 0.0000, 0.6400, 0.8400, 0.4700, 0.8600, 0.8000, 0.7500, 0.8600, 0.6900, 0.8700, 0.8200, 0.8400, 0.5500, 0.6800, 0.5400, 0.6700, 0.9500, 0.8500, 0.6600, 0.0000, 0.6400, 0.9300, 0.8100, 0.5100, 0.8300, 0.8400, 0.8800, 0.7400, 0.9000, 0.6900, 0.7600, 0.7200, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7500, 0.4500, 0.6800, 0.6000, 0.9400, 0.9000, 0.7300, 0.6400, 0.8500, 0.0000, 0.8100, 0.9300, 0.6500, 0.6500, 0.0000, 0.0100, 0.9000, 0.8200, 0.9300, 0.8200, 0.8400, 0.7400, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5100, 0.7400, Acc_f: 0.0010, Acc_r: 0.7563\n",
      "[train] epoch 21, batch 8, loss 0.5544286370277405\n",
      "0.9500, 0.7900, 0.6000, 0.7100, 0.6700, 0.7900, 0.8300, 0.7500, 0.9000, 0.8500, 0.6300, 0.5300, 0.7900, 0.7300, 0.6900, 0.7900, 0.7800, 0.8700, 0.5800, 0.0000, 0.8700, 0.9100, 0.7600, 0.8400, 0.8100, 0.6100, 0.7300, 0.6800, 0.0000, 0.7200, 0.6700, 0.7800, 0.0000, 0.6400, 0.8400, 0.4700, 0.8700, 0.7900, 0.7500, 0.8600, 0.6600, 0.8700, 0.8400, 0.8400, 0.5500, 0.6800, 0.5400, 0.6600, 0.9500, 0.8500, 0.6300, 0.0000, 0.6400, 0.9300, 0.8000, 0.4900, 0.8400, 0.8400, 0.8700, 0.7500, 0.9000, 0.6900, 0.7700, 0.7200, 0.6100, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7400, 0.7500, 0.4500, 0.6800, 0.6000, 0.9300, 0.9000, 0.7300, 0.6300, 0.8600, 0.0000, 0.8100, 0.9400, 0.6500, 0.6400, 0.0000, 0.0000, 0.9100, 0.8200, 0.9300, 0.8500, 0.8300, 0.7200, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5100, 0.7400, Acc_f: 0.0000, Acc_r: 0.7559\n",
      "[train] epoch 22, batch 8, loss 0.522753119468689\n",
      "0.9400, 0.7900, 0.6000, 0.7100, 0.6500, 0.7900, 0.8300, 0.7600, 0.9000, 0.8400, 0.6500, 0.5300, 0.7900, 0.7200, 0.6900, 0.7800, 0.7800, 0.8700, 0.6100, 0.0000, 0.8900, 0.9000, 0.7700, 0.8400, 0.8100, 0.6100, 0.7400, 0.6600, 0.0000, 0.7200, 0.6500, 0.7900, 0.0000, 0.6400, 0.8400, 0.4400, 0.8600, 0.7800, 0.7500, 0.8700, 0.6700, 0.8700, 0.8100, 0.8400, 0.5400, 0.6800, 0.5300, 0.6800, 0.9500, 0.8500, 0.6500, 0.0000, 0.6400, 0.9300, 0.8100, 0.5100, 0.8200, 0.8500, 0.8700, 0.7200, 0.9000, 0.6900, 0.7600, 0.7200, 0.6000, 0.0000, 0.8400, 0.0000, 0.9400, 0.8200, 0.7400, 0.7500, 0.4300, 0.6800, 0.6000, 0.9300, 0.9000, 0.7300, 0.6500, 0.8400, 0.0000, 0.8100, 0.9300, 0.6500, 0.6500, 0.0000, 0.0100, 0.8900, 0.8200, 0.9300, 0.8600, 0.8400, 0.7100, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5200, 0.7500, Acc_f: 0.0010, Acc_r: 0.7549\n",
      "[train] epoch 23, batch 8, loss 0.48246172070503235\n",
      "0.9400, 0.7900, 0.6000, 0.7100, 0.6800, 0.7900, 0.8200, 0.7600, 0.8900, 0.8500, 0.6300, 0.5200, 0.7700, 0.7300, 0.6900, 0.7500, 0.7700, 0.8800, 0.6100, 0.0000, 0.8600, 0.9000, 0.8100, 0.8400, 0.8200, 0.6200, 0.7300, 0.6900, 0.0000, 0.7300, 0.6400, 0.7600, 0.0000, 0.6300, 0.8400, 0.4500, 0.8600, 0.7800, 0.7700, 0.8700, 0.6700, 0.8700, 0.8400, 0.8400, 0.5200, 0.6800, 0.5200, 0.6800, 0.9500, 0.8500, 0.6700, 0.0000, 0.6300, 0.9300, 0.8100, 0.5000, 0.8200, 0.8400, 0.8700, 0.7300, 0.9000, 0.6800, 0.7700, 0.7100, 0.6000, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7300, 0.7500, 0.4500, 0.6700, 0.6000, 0.9300, 0.9000, 0.7400, 0.6300, 0.8400, 0.0000, 0.8200, 0.9400, 0.6200, 0.6500, 0.0000, 0.0000, 0.9000, 0.8000, 0.9200, 0.8700, 0.8400, 0.7200, 0.6400, 0.9200, 0.7400, 0.7500, 0.0000, 0.5300, 0.7500, Acc_f: 0.0000, Acc_r: 0.7547\n",
      "[train] epoch 24, batch 8, loss 0.5403985977172852\n",
      "0.9400, 0.7900, 0.6200, 0.7000, 0.6600, 0.7700, 0.8200, 0.7400, 0.9000, 0.8400, 0.6300, 0.5400, 0.7800, 0.7400, 0.7200, 0.7700, 0.7900, 0.8800, 0.6000, 0.0000, 0.8700, 0.9000, 0.7800, 0.8400, 0.8100, 0.6100, 0.7400, 0.6400, 0.0000, 0.7400, 0.6400, 0.7900, 0.0000, 0.6300, 0.8400, 0.4400, 0.9000, 0.7900, 0.7400, 0.8600, 0.6800, 0.8900, 0.8100, 0.8600, 0.5400, 0.6700, 0.5300, 0.6800, 0.9500, 0.8500, 0.6700, 0.0000, 0.6200, 0.9300, 0.8100, 0.4900, 0.8300, 0.8400, 0.8700, 0.7500, 0.9000, 0.7000, 0.7700, 0.7300, 0.6100, 0.0000, 0.8500, 0.0000, 0.9400, 0.8200, 0.7400, 0.7500, 0.4400, 0.6700, 0.6000, 0.9400, 0.9000, 0.7300, 0.6400, 0.8500, 0.0000, 0.8100, 0.9400, 0.6300, 0.6600, 0.0000, 0.0000, 0.8900, 0.8200, 0.9200, 0.8700, 0.8500, 0.7200, 0.6500, 0.9200, 0.7400, 0.7300, 0.0000, 0.5000, 0.7200, Acc_f: 0.0000, Acc_r: 0.7557\n",
      "0.8800, 0.8700, 0.6700, 0.6100, 0.7100, 0.8000, 0.8400, 0.7900, 0.8600, 0.8600, 0.4800, 0.4300, 0.8300, 0.7000, 0.7300, 0.8100, 0.8000, 0.8400, 0.6800, 0.6600, 0.9000, 0.9200, 0.7500, 0.8700, 0.8200, 0.6000, 0.7000, 0.6200, 0.8100, 0.7100, 0.7200, 0.7800, 0.6700, 0.6100, 0.7900, 0.5800, 0.8300, 0.7600, 0.7300, 0.8700, 0.7100, 0.9000, 0.7900, 0.8300, 0.5200, 0.7000, 0.5400, 0.7300, 0.9500, 0.8800, 0.5900, 0.7200, 0.6800, 0.9400, 0.8800, 0.5700, 0.8800, 0.8200, 0.8000, 0.7500, 0.9100, 0.7100, 0.7600, 0.7200, 0.5100, 0.5800, 0.8100, 0.6200, 0.9400, 0.8500, 0.7700, 0.8200, 0.5500, 0.6100, 0.5400, 0.8900, 0.9100, 0.7100, 0.6700, 0.8200, 0.6100, 0.8100, 0.8900, 0.7000, 0.7300, 0.8500, 0.7100, 0.8600, 0.7600, 0.9100, 0.8800, 0.8400, 0.6500, 0.5500, 0.9100, 0.7400, 0.6300, 0.8100, 0.5900, 0.7300, Acc_f: 0.7040, Acc_r: 0.7543\n",
      "[train] epoch 0, batch 8, loss 1.7764824628829956\n",
      "0.8900, 0.8600, 0.6900, 0.6400, 0.7300, 0.8200, 0.8400, 0.8000, 0.8600, 0.8600, 0.6800, 0.4100, 0.8600, 0.6500, 0.6600, 0.8200, 0.8100, 0.8500, 0.6900, 0.1300, 0.8900, 0.9200, 0.7600, 0.8500, 0.8200, 0.6300, 0.6900, 0.5900, 0.0000, 0.7300, 0.7200, 0.8400, 0.0700, 0.6600, 0.8200, 0.6000, 0.8800, 0.7800, 0.8000, 0.8900, 0.7400, 0.8800, 0.7800, 0.8700, 0.5500, 0.6900, 0.6700, 0.7300, 0.9500, 0.8800, 0.6300, 0.2400, 0.6800, 0.9500, 0.8600, 0.5800, 0.8800, 0.8400, 0.8700, 0.7200, 0.9100, 0.6500, 0.7700, 0.7100, 0.5900, 0.0900, 0.8200, 0.0600, 0.9400, 0.8500, 0.7700, 0.8200, 0.5400, 0.6200, 0.5000, 0.8900, 0.8800, 0.7300, 0.6700, 0.8400, 0.0300, 0.8200, 0.8900, 0.7000, 0.7300, 0.1700, 0.1400, 0.8600, 0.7700, 0.9100, 0.8800, 0.8600, 0.6400, 0.6100, 0.9200, 0.7500, 0.5800, 0.1200, 0.6100, 0.7100, Acc_f: 0.1050, Acc_r: 0.7642\n",
      "[train] epoch 1, batch 8, loss 1.500011682510376\n",
      "0.8800, 0.8600, 0.6800, 0.6300, 0.6700, 0.8200, 0.8200, 0.7800, 0.8700, 0.8500, 0.6800, 0.3900, 0.8600, 0.6400, 0.6800, 0.8100, 0.8100, 0.8400, 0.6800, 0.0500, 0.8800, 0.9200, 0.7600, 0.8700, 0.8200, 0.6400, 0.6900, 0.6400, 0.0000, 0.6900, 0.7200, 0.8300, 0.0000, 0.6500, 0.8500, 0.5600, 0.8800, 0.8100, 0.7700, 0.8900, 0.7400, 0.8600, 0.7700, 0.8600, 0.5400, 0.6600, 0.6400, 0.7300, 0.9500, 0.8800, 0.6700, 0.0900, 0.6800, 0.9500, 0.8600, 0.5700, 0.8900, 0.8300, 0.8800, 0.7300, 0.9100, 0.6600, 0.7600, 0.7100, 0.6200, 0.0200, 0.8200, 0.0200, 0.9400, 0.8500, 0.7700, 0.8100, 0.5500, 0.6400, 0.5400, 0.8900, 0.8800, 0.7600, 0.6700, 0.8500, 0.0000, 0.8100, 0.8900, 0.6900, 0.7300, 0.0300, 0.0800, 0.8600, 0.7800, 0.9100, 0.8700, 0.8600, 0.6600, 0.6200, 0.9200, 0.7500, 0.5800, 0.0600, 0.6100, 0.7100, Acc_f: 0.0350, Acc_r: 0.7632\n",
      "[train] epoch 2, batch 8, loss 1.1386185884475708\n",
      "0.8800, 0.8600, 0.6700, 0.6500, 0.6700, 0.8100, 0.8200, 0.7800, 0.8600, 0.8500, 0.6900, 0.3700, 0.8600, 0.6500, 0.6900, 0.8100, 0.8100, 0.8400, 0.6900, 0.0500, 0.8700, 0.9300, 0.7500, 0.8700, 0.8000, 0.6200, 0.6900, 0.6600, 0.0000, 0.6800, 0.7100, 0.8200, 0.0000, 0.6300, 0.8500, 0.5800, 0.8800, 0.8100, 0.7800, 0.9000, 0.7400, 0.8600, 0.7800, 0.8400, 0.5200, 0.6900, 0.6400, 0.7300, 0.9500, 0.8900, 0.6900, 0.0300, 0.6500, 0.9500, 0.8600, 0.5500, 0.8900, 0.8300, 0.8800, 0.7200, 0.9100, 0.6600, 0.7500, 0.7100, 0.6100, 0.0100, 0.8200, 0.0000, 0.9400, 0.8400, 0.7700, 0.8100, 0.5500, 0.6200, 0.5500, 0.8800, 0.8700, 0.7700, 0.6600, 0.8500, 0.0000, 0.8000, 0.8900, 0.6900, 0.7400, 0.0300, 0.0400, 0.8600, 0.7800, 0.9100, 0.8700, 0.8500, 0.6800, 0.6200, 0.9200, 0.7400, 0.6100, 0.0400, 0.6000, 0.7400, Acc_f: 0.0200, Acc_r: 0.7624\n",
      "[train] epoch 3, batch 8, loss 1.0020296573638916\n",
      "0.8900, 0.8600, 0.6700, 0.6600, 0.6700, 0.8200, 0.8300, 0.7800, 0.8700, 0.8500, 0.6700, 0.3800, 0.8800, 0.6400, 0.6800, 0.8200, 0.8200, 0.8400, 0.6900, 0.0300, 0.8700, 0.9400, 0.7600, 0.8900, 0.8000, 0.6300, 0.7100, 0.7000, 0.0000, 0.6900, 0.7100, 0.8500, 0.0100, 0.6200, 0.8500, 0.5800, 0.8700, 0.8100, 0.7900, 0.8900, 0.7500, 0.8600, 0.7800, 0.8500, 0.5200, 0.6700, 0.6600, 0.7200, 0.9400, 0.9000, 0.6800, 0.0300, 0.6300, 0.9500, 0.8600, 0.5500, 0.8800, 0.8400, 0.8700, 0.7200, 0.9100, 0.6800, 0.7400, 0.7100, 0.6100, 0.0000, 0.8200, 0.0000, 0.9400, 0.8300, 0.7700, 0.8100, 0.5500, 0.6300, 0.5800, 0.8800, 0.8700, 0.7800, 0.6600, 0.8500, 0.0000, 0.8000, 0.8900, 0.6800, 0.7400, 0.0200, 0.0200, 0.8500, 0.7700, 0.9100, 0.8700, 0.8500, 0.7100, 0.6100, 0.9300, 0.7400, 0.6300, 0.0100, 0.5700, 0.7600, Acc_f: 0.0120, Acc_r: 0.7649\n",
      "[train] epoch 4, batch 8, loss 0.8835572004318237\n",
      "0.8900, 0.8600, 0.6600, 0.6600, 0.6500, 0.8200, 0.8300, 0.7800, 0.8700, 0.8500, 0.6700, 0.3800, 0.8800, 0.6600, 0.7100, 0.8200, 0.8200, 0.8500, 0.6900, 0.0200, 0.8700, 0.9400, 0.7500, 0.8900, 0.8000, 0.6200, 0.7000, 0.7000, 0.0000, 0.6800, 0.7000, 0.8400, 0.0000, 0.6100, 0.8500, 0.5800, 0.8700, 0.8100, 0.7800, 0.8900, 0.7300, 0.8600, 0.8000, 0.8500, 0.5200, 0.6800, 0.6600, 0.7200, 0.9400, 0.9000, 0.6800, 0.0300, 0.6300, 0.9500, 0.8600, 0.5200, 0.8900, 0.8400, 0.8700, 0.7200, 0.9100, 0.6600, 0.7400, 0.7200, 0.5800, 0.0000, 0.8200, 0.0000, 0.9400, 0.8300, 0.7600, 0.8100, 0.5500, 0.6300, 0.5700, 0.8800, 0.8600, 0.7500, 0.6500, 0.8400, 0.0000, 0.7900, 0.8900, 0.6800, 0.7300, 0.0100, 0.0100, 0.8500, 0.7500, 0.9100, 0.8700, 0.8500, 0.7000, 0.6500, 0.9300, 0.7400, 0.6400, 0.0000, 0.5800, 0.7600, Acc_f: 0.0070, Acc_r: 0.7630\n",
      "[train] epoch 5, batch 8, loss 0.8672982454299927\n",
      "0.8800, 0.8700, 0.6700, 0.6500, 0.6500, 0.8100, 0.8300, 0.7800, 0.8600, 0.8500, 0.6800, 0.3800, 0.8800, 0.6700, 0.7200, 0.8200, 0.8200, 0.8500, 0.6900, 0.0200, 0.8600, 0.9400, 0.7600, 0.9000, 0.8000, 0.6200, 0.7000, 0.6900, 0.0000, 0.6800, 0.7200, 0.8500, 0.0000, 0.6200, 0.8500, 0.5700, 0.8700, 0.8100, 0.7900, 0.8900, 0.7300, 0.8600, 0.8000, 0.8400, 0.5200, 0.6800, 0.6500, 0.7200, 0.9500, 0.9100, 0.7000, 0.0300, 0.6300, 0.9600, 0.8600, 0.5400, 0.8900, 0.8400, 0.8700, 0.7300, 0.9100, 0.6800, 0.7400, 0.7100, 0.5800, 0.0000, 0.8400, 0.0000, 0.9400, 0.8300, 0.7600, 0.8100, 0.5400, 0.6300, 0.5700, 0.8700, 0.8600, 0.7600, 0.6500, 0.8300, 0.0000, 0.7900, 0.8900, 0.6800, 0.7300, 0.0100, 0.0000, 0.8500, 0.7500, 0.9100, 0.8600, 0.8600, 0.7100, 0.6300, 0.9300, 0.7300, 0.6300, 0.0000, 0.5800, 0.7500, Acc_f: 0.0060, Acc_r: 0.7639\n",
      "[train] epoch 6, batch 8, loss 0.8489975929260254\n",
      "0.8800, 0.8700, 0.6600, 0.6500, 0.6500, 0.8100, 0.8200, 0.7800, 0.8400, 0.8500, 0.6700, 0.4000, 0.8700, 0.6700, 0.7200, 0.8200, 0.8300, 0.8500, 0.6900, 0.0100, 0.8700, 0.9400, 0.7800, 0.8900, 0.8000, 0.6200, 0.6900, 0.7000, 0.0000, 0.6800, 0.7100, 0.8400, 0.0000, 0.6100, 0.8600, 0.5700, 0.8600, 0.8100, 0.7800, 0.8900, 0.7300, 0.8600, 0.8000, 0.8400, 0.5200, 0.7000, 0.6500, 0.7100, 0.9400, 0.9000, 0.6700, 0.0200, 0.6300, 0.9500, 0.8700, 0.5300, 0.8900, 0.8400, 0.8700, 0.7200, 0.9100, 0.6700, 0.7300, 0.7300, 0.5900, 0.0000, 0.8300, 0.0000, 0.9300, 0.8300, 0.7500, 0.8100, 0.5200, 0.6100, 0.5700, 0.8700, 0.8600, 0.7700, 0.6300, 0.8300, 0.0000, 0.7800, 0.9000, 0.6800, 0.7300, 0.0000, 0.0000, 0.8600, 0.7500, 0.9100, 0.8600, 0.8400, 0.7100, 0.6400, 0.9300, 0.7300, 0.6400, 0.0000, 0.5800, 0.7600, Acc_f: 0.0030, Acc_r: 0.7621\n",
      "[train] epoch 7, batch 8, loss 0.7957639098167419\n",
      "0.8800, 0.8600, 0.6500, 0.6400, 0.6500, 0.8100, 0.8200, 0.7800, 0.8400, 0.8500, 0.6300, 0.4000, 0.8700, 0.7000, 0.7300, 0.8200, 0.8200, 0.8500, 0.6700, 0.0100, 0.8700, 0.9400, 0.7700, 0.8900, 0.8100, 0.6200, 0.6900, 0.6800, 0.0000, 0.7000, 0.7000, 0.8400, 0.0100, 0.6300, 0.8500, 0.5600, 0.8600, 0.8200, 0.7800, 0.8900, 0.7400, 0.8600, 0.8000, 0.8300, 0.5200, 0.7100, 0.6500, 0.7100, 0.9400, 0.8800, 0.6800, 0.0100, 0.6300, 0.9600, 0.8800, 0.5600, 0.8900, 0.8400, 0.8700, 0.7300, 0.9100, 0.6800, 0.7200, 0.7300, 0.5700, 0.0000, 0.8200, 0.0000, 0.9300, 0.8300, 0.7500, 0.8100, 0.5500, 0.6400, 0.5800, 0.8600, 0.8800, 0.7600, 0.6400, 0.8300, 0.0000, 0.7700, 0.9000, 0.6700, 0.7300, 0.0100, 0.0000, 0.8500, 0.7500, 0.9100, 0.8600, 0.8400, 0.7000, 0.6400, 0.9100, 0.7400, 0.6400, 0.0000, 0.5900, 0.7700, Acc_f: 0.0040, Acc_r: 0.7623\n",
      "[train] epoch 8, batch 8, loss 0.7639195919036865\n",
      "0.8800, 0.8600, 0.6500, 0.6300, 0.6200, 0.8100, 0.8200, 0.7800, 0.8400, 0.8400, 0.6500, 0.4000, 0.8600, 0.7100, 0.7400, 0.8200, 0.8200, 0.8500, 0.6700, 0.0100, 0.8700, 0.9400, 0.7700, 0.8800, 0.8100, 0.6300, 0.7000, 0.6900, 0.0000, 0.6800, 0.7200, 0.8400, 0.0100, 0.6300, 0.8500, 0.5600, 0.8700, 0.8200, 0.7800, 0.8900, 0.7200, 0.8600, 0.8000, 0.8300, 0.5200, 0.7100, 0.6400, 0.6900, 0.9400, 0.9000, 0.6900, 0.0300, 0.6300, 0.9500, 0.8800, 0.5400, 0.8900, 0.8400, 0.8700, 0.7300, 0.9100, 0.6800, 0.7300, 0.7300, 0.5700, 0.0000, 0.8200, 0.0100, 0.9300, 0.8300, 0.7500, 0.8100, 0.5300, 0.6400, 0.6100, 0.8600, 0.8800, 0.7700, 0.6400, 0.8300, 0.0000, 0.7800, 0.9000, 0.6700, 0.7300, 0.0100, 0.0000, 0.8600, 0.7500, 0.9100, 0.8600, 0.8500, 0.7300, 0.6300, 0.9100, 0.7400, 0.6600, 0.0000, 0.5800, 0.7600, Acc_f: 0.0070, Acc_r: 0.7628\n",
      "[train] epoch 9, batch 8, loss 0.6884444355964661\n",
      "0.8800, 0.8600, 0.6500, 0.6700, 0.6400, 0.8100, 0.8200, 0.7700, 0.8400, 0.8400, 0.6100, 0.4000, 0.8500, 0.7000, 0.7200, 0.8200, 0.8100, 0.8500, 0.6600, 0.0100, 0.8700, 0.9400, 0.7600, 0.8800, 0.8100, 0.6200, 0.7000, 0.6800, 0.0000, 0.6800, 0.7100, 0.8400, 0.0100, 0.6300, 0.8600, 0.5800, 0.8700, 0.8200, 0.7800, 0.8900, 0.7200, 0.8600, 0.8000, 0.8300, 0.5100, 0.7100, 0.6300, 0.6900, 0.9400, 0.8900, 0.7100, 0.0000, 0.6300, 0.9600, 0.8800, 0.5400, 0.8800, 0.8400, 0.8700, 0.7400, 0.9200, 0.6800, 0.7300, 0.7300, 0.5800, 0.0000, 0.8400, 0.0000, 0.9300, 0.8500, 0.7500, 0.8000, 0.5300, 0.6300, 0.5800, 0.8800, 0.8800, 0.7900, 0.6400, 0.8300, 0.0000, 0.7700, 0.9000, 0.6800, 0.7300, 0.0000, 0.0000, 0.8600, 0.7500, 0.9100, 0.8600, 0.8500, 0.7300, 0.6400, 0.9200, 0.7500, 0.6600, 0.0000, 0.5700, 0.7400, Acc_f: 0.0020, Acc_r: 0.7627\n",
      "[train] epoch 10, batch 8, loss 0.7374418377876282\n",
      "0.8800, 0.8600, 0.6300, 0.6500, 0.6400, 0.8100, 0.8200, 0.7700, 0.8300, 0.8400, 0.6500, 0.3900, 0.8600, 0.7000, 0.7400, 0.8300, 0.8200, 0.8500, 0.6600, 0.0100, 0.8700, 0.9500, 0.7600, 0.8900, 0.8100, 0.6100, 0.6900, 0.6600, 0.0000, 0.6800, 0.7100, 0.8400, 0.0000, 0.6300, 0.8500, 0.5600, 0.8700, 0.8200, 0.7800, 0.8800, 0.7200, 0.8600, 0.8000, 0.8300, 0.5400, 0.7000, 0.6400, 0.6900, 0.9500, 0.8900, 0.6700, 0.0000, 0.6400, 0.9600, 0.8800, 0.5400, 0.8800, 0.8400, 0.8800, 0.7500, 0.9100, 0.6700, 0.7300, 0.7300, 0.5700, 0.0000, 0.8400, 0.0100, 0.9300, 0.8300, 0.7400, 0.8000, 0.5400, 0.6200, 0.6000, 0.8800, 0.8800, 0.7700, 0.6600, 0.8300, 0.0000, 0.7600, 0.9000, 0.6700, 0.7300, 0.0000, 0.0000, 0.8700, 0.7500, 0.9100, 0.8500, 0.8500, 0.7300, 0.6400, 0.9100, 0.7500, 0.6400, 0.0000, 0.5800, 0.7500, Acc_f: 0.0020, Acc_r: 0.7619\n",
      "[train] epoch 11, batch 8, loss 0.7995558977127075\n",
      "0.8800, 0.8600, 0.6300, 0.6300, 0.6400, 0.8100, 0.8200, 0.7600, 0.8500, 0.8500, 0.6400, 0.4000, 0.8600, 0.7100, 0.7500, 0.8200, 0.8000, 0.8500, 0.6500, 0.0100, 0.8700, 0.9400, 0.7800, 0.8800, 0.8100, 0.6200, 0.6800, 0.6600, 0.0000, 0.6700, 0.7300, 0.8400, 0.0100, 0.6300, 0.8500, 0.5700, 0.8700, 0.8200, 0.7800, 0.8800, 0.7100, 0.8600, 0.8000, 0.8400, 0.5300, 0.6900, 0.6300, 0.6800, 0.9400, 0.8900, 0.6800, 0.0000, 0.6100, 0.9400, 0.8800, 0.5500, 0.8700, 0.8400, 0.8800, 0.7300, 0.9200, 0.6700, 0.7300, 0.7300, 0.5700, 0.0000, 0.8300, 0.0100, 0.9300, 0.8400, 0.7500, 0.8000, 0.5400, 0.6300, 0.5800, 0.8800, 0.8800, 0.7700, 0.6300, 0.8300, 0.0000, 0.7700, 0.8900, 0.6700, 0.7300, 0.0000, 0.0000, 0.8800, 0.7600, 0.9100, 0.8600, 0.8500, 0.7100, 0.6300, 0.9100, 0.7500, 0.6600, 0.0000, 0.5800, 0.7500, Acc_f: 0.0030, Acc_r: 0.7607\n",
      "[train] epoch 12, batch 8, loss 0.6848949193954468\n",
      "0.8800, 0.8500, 0.6000, 0.6200, 0.6300, 0.8100, 0.8100, 0.7600, 0.8500, 0.8500, 0.6400, 0.4000, 0.8600, 0.6800, 0.7300, 0.8400, 0.8100, 0.8500, 0.6400, 0.0100, 0.8700, 0.9300, 0.7700, 0.8900, 0.8200, 0.6200, 0.6900, 0.6800, 0.0000, 0.7100, 0.7400, 0.8400, 0.0100, 0.6300, 0.8500, 0.5800, 0.8700, 0.8200, 0.7800, 0.8900, 0.7200, 0.8600, 0.8000, 0.8400, 0.5300, 0.6900, 0.6500, 0.6900, 0.9500, 0.8900, 0.6700, 0.0000, 0.6300, 0.9400, 0.8700, 0.5400, 0.8900, 0.8400, 0.8800, 0.7300, 0.9200, 0.6800, 0.7300, 0.7300, 0.5700, 0.0000, 0.8600, 0.0000, 0.9300, 0.8300, 0.7500, 0.8000, 0.5300, 0.6300, 0.6000, 0.8800, 0.8700, 0.7600, 0.6400, 0.8300, 0.0000, 0.7700, 0.8900, 0.6700, 0.7400, 0.0000, 0.0000, 0.8700, 0.7400, 0.9100, 0.8500, 0.8600, 0.7100, 0.6200, 0.9200, 0.7400, 0.6500, 0.0000, 0.5800, 0.7600, Acc_f: 0.0020, Acc_r: 0.7613\n",
      "[train] epoch 13, batch 8, loss 0.6699174046516418\n",
      "0.8800, 0.8500, 0.6300, 0.6200, 0.6300, 0.8100, 0.8100, 0.7600, 0.8300, 0.8500, 0.6400, 0.4100, 0.8700, 0.7000, 0.7300, 0.8400, 0.8100, 0.8500, 0.6500, 0.0100, 0.8700, 0.9500, 0.7800, 0.8900, 0.8100, 0.6200, 0.6800, 0.6900, 0.0000, 0.7000, 0.7200, 0.8300, 0.0000, 0.6200, 0.8500, 0.5500, 0.9000, 0.8200, 0.7800, 0.8800, 0.7100, 0.8600, 0.7900, 0.8500, 0.5200, 0.6900, 0.6400, 0.6800, 0.9600, 0.9000, 0.6400, 0.0000, 0.5900, 0.9500, 0.8800, 0.5400, 0.8900, 0.8400, 0.8700, 0.7400, 0.9200, 0.6800, 0.7400, 0.7300, 0.5600, 0.0000, 0.8500, 0.0000, 0.9300, 0.8300, 0.7400, 0.8000, 0.5600, 0.6600, 0.5700, 0.8700, 0.8800, 0.7500, 0.6100, 0.8300, 0.0000, 0.7700, 0.8900, 0.6600, 0.7200, 0.0000, 0.0000, 0.8600, 0.7300, 0.9100, 0.8600, 0.8600, 0.7000, 0.6300, 0.9200, 0.7400, 0.6700, 0.0000, 0.5800, 0.7600, Acc_f: 0.0010, Acc_r: 0.7602\n",
      "[train] epoch 14, batch 8, loss 0.5693676471710205\n",
      "0.8800, 0.8500, 0.6200, 0.6400, 0.6300, 0.8200, 0.8200, 0.7600, 0.8400, 0.8500, 0.6400, 0.4000, 0.8800, 0.6800, 0.7300, 0.8500, 0.8100, 0.8400, 0.6400, 0.0100, 0.8700, 0.9500, 0.7700, 0.8900, 0.8200, 0.6100, 0.6800, 0.6800, 0.0000, 0.6900, 0.7500, 0.8300, 0.0000, 0.6300, 0.8600, 0.5300, 0.9000, 0.8200, 0.7800, 0.8800, 0.7200, 0.8600, 0.7800, 0.8500, 0.5300, 0.6800, 0.6500, 0.6800, 0.9400, 0.9000, 0.6400, 0.0000, 0.6000, 0.9500, 0.8700, 0.5200, 0.8700, 0.8400, 0.8700, 0.7500, 0.9200, 0.6700, 0.7400, 0.7300, 0.5700, 0.0000, 0.8500, 0.0000, 0.9300, 0.8500, 0.7500, 0.8000, 0.5400, 0.6400, 0.5700, 0.8800, 0.8800, 0.7500, 0.6100, 0.8300, 0.0000, 0.7600, 0.8900, 0.6600, 0.7400, 0.0000, 0.0000, 0.8500, 0.7500, 0.9100, 0.8700, 0.8500, 0.7100, 0.6300, 0.9200, 0.7300, 0.6800, 0.0000, 0.5800, 0.7700, Acc_f: 0.0010, Acc_r: 0.7603\n",
      "[train] epoch 15, batch 8, loss 0.6416544318199158\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.04)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(25):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100, 0.8300, 0.6100, 0.6500, 0.6700, 0.7900, 0.8200, 0.7700, 0.9000, 0.8600, 0.5200, 0.5000, 0.7900, 0.7400, 0.7300, 0.7600, 0.7500, 0.9000, 0.5900, 0.7100, 0.8600, 0.9000, 0.7700, 0.8000, 0.8200, 0.5700, 0.7700, 0.6100, 0.8100, 0.7500, 0.6600, 0.7500, 0.6900, 0.6700, 0.8200, 0.4400, 0.8600, 0.7600, 0.7300, 0.8900, 0.6300, 0.9000, 0.8000, 0.8400, 0.5100, 0.6400, 0.5100, 0.7100, 0.9400, 0.8700, 0.5600, 0.7300, 0.6600, 0.9100, 0.8100, 0.5200, 0.8700, 0.8200, 0.8700, 0.7000, 0.8900, 0.7500, 0.7900, 0.7000, 0.5900, 0.6200, 0.8300, 0.7100, 0.9400, 0.8300, 0.7500, 0.7900, 0.4700, 0.6400, 0.5800, 0.9000, 0.9200, 0.7000, 0.7100, 0.8200, 0.6000, 0.7700, 0.9200, 0.7200, 0.6700, 0.8500, 0.7500, 0.9000, 0.8500, 0.8900, 0.8400, 0.8400, 0.6700, 0.5600, 0.9300, 0.7400, 0.6700, 0.8100, 0.5500, 0.7100, Acc_f: 0.7530, Acc_r: 0.7455\n",
      "0.8800, 0.8700, 0.6700, 0.6100, 0.7100, 0.8000, 0.8400, 0.7900, 0.8600, 0.8600, 0.4800, 0.4300, 0.8300, 0.7000, 0.7300, 0.8100, 0.8000, 0.8400, 0.6800, 0.6600, 0.9000, 0.9200, 0.7500, 0.8700, 0.8200, 0.6000, 0.7000, 0.6200, 0.8100, 0.7100, 0.7200, 0.7800, 0.6700, 0.6100, 0.7900, 0.5800, 0.8300, 0.7600, 0.7300, 0.8700, 0.7100, 0.9000, 0.7900, 0.8300, 0.5200, 0.7000, 0.5400, 0.7300, 0.9500, 0.8800, 0.5900, 0.7200, 0.6800, 0.9400, 0.8800, 0.5700, 0.8800, 0.8200, 0.8000, 0.7500, 0.9100, 0.7100, 0.7600, 0.7200, 0.5100, 0.5800, 0.8100, 0.6200, 0.9400, 0.8500, 0.7700, 0.8200, 0.5500, 0.6100, 0.5400, 0.8900, 0.9100, 0.7100, 0.6700, 0.8200, 0.6100, 0.8100, 0.8900, 0.7000, 0.7300, 0.8500, 0.7100, 0.8600, 0.7600, 0.9100, 0.8800, 0.8400, 0.6500, 0.5500, 0.9100, 0.7400, 0.6300, 0.8100, 0.5900, 0.7300, Acc_f: 0.7435, Acc_r: 0.7508\n",
      "0.9100, 0.8500, 0.6400, 0.6300, 0.6600, 0.7600, 0.8100, 0.7800, 0.8900, 0.8600, 0.4800, 0.4500, 0.8400, 0.7000, 0.6900, 0.8100, 0.7700, 0.8600, 0.6500, 0.7000, 0.8900, 0.9200, 0.7300, 0.8100, 0.8000, 0.5900, 0.7300, 0.6600, 0.7900, 0.7200, 0.6700, 0.7200, 0.7100, 0.6800, 0.8200, 0.6300, 0.8300, 0.8300, 0.6700, 0.8900, 0.6900, 0.8900, 0.8300, 0.8700, 0.4900, 0.7000, 0.5000, 0.7100, 0.9600, 0.9200, 0.6100, 0.7700, 0.6400, 0.9300, 0.8600, 0.5200, 0.9000, 0.7800, 0.9100, 0.6900, 0.9100, 0.7500, 0.8000, 0.7100, 0.5600, 0.6500, 0.8100, 0.6400, 0.9600, 0.8600, 0.7800, 0.8600, 0.5000, 0.5900, 0.5800, 0.9100, 0.9000, 0.7100, 0.6600, 0.8200, 0.6600, 0.8100, 0.9200, 0.7000, 0.6800, 0.8400, 0.7400, 0.8600, 0.8300, 0.9100, 0.8200, 0.8500, 0.7000, 0.6500, 0.9400, 0.7500, 0.6900, 0.8000, 0.6000, 0.7400, Acc_f: 0.7595, Acc_r: 0.7533\n",
      "------------ Retrained model ------------\n",
      "0.8900, 0.8600, 0.6200, 0.6400, 0.6400, 0.8000, 0.8200, 0.7700, 0.8700, 0.8700, 0.5900, 0.0000, 0.9000, 0.7000, 0.6800, 0.8300, 0.7600, 0.9300, 0.6300, 0.0000, 0.8600, 0.9000, 0.7400, 0.8300, 0.0000, 0.5900, 0.7400, 0.6700, 0.0000, 0.7500, 0.7500, 0.7800, 0.0000, 0.6300, 0.8300, 0.6100, 0.8900, 0.0000, 0.7000, 0.0000, 0.6800, 0.9400, 0.0000, 0.8300, 0.5100, 0.7100, 0.6000, 0.7200, 0.9800, 0.8900, 0.6200, 0.0000, 0.6800, 0.9400, 0.8800, 0.6300, 0.0000, 0.7800, 0.8800, 0.7000, 0.9100, 0.6800, 0.7800, 0.7200, 0.6300, 0.0000, 0.8100, 0.0000, 0.0000, 0.8400, 0.7800, 0.7800, 0.0000, 0.6200, 0.6100, 0.9300, 0.9100, 0.7300, 0.6000, 0.8100, 0.0000, 0.8600, 0.9500, 0.7100, 0.7500, 0.0000, 0.0000, 0.9000, 0.8700, 0.0000, 0.0000, 0.8500, 0.6400, 0.6400, 0.9400, 0.7000, 0.6600, 0.0000, 0.6000, 0.7400, Acc_f: 0.0000, Acc_r: 0.7624\n",
      "0.9000, 0.8600, 0.6800, 0.6500, 0.6800, 0.8100, 0.8300, 0.8100, 0.8900, 0.8500, 0.5300, 0.0000, 0.8600, 0.6700, 0.6700, 0.8100, 0.7800, 0.9000, 0.5800, 0.0000, 0.8800, 0.8900, 0.8000, 0.8800, 0.0000, 0.6300, 0.6900, 0.6500, 0.0000, 0.7400, 0.7400, 0.7500, 0.0000, 0.6800, 0.8300, 0.6300, 0.8600, 0.0000, 0.7100, 0.0000, 0.7100, 0.9300, 0.0000, 0.8500, 0.6100, 0.6600, 0.5900, 0.7500, 0.9400, 0.9100, 0.6200, 0.0000, 0.6700, 0.9400, 0.8900, 0.5600, 0.0000, 0.7500, 0.8600, 0.7300, 0.8900, 0.7200, 0.7600, 0.7400, 0.6100, 0.0000, 0.8200, 0.0000, 0.0000, 0.8500, 0.7700, 0.8300, 0.0000, 0.7000, 0.6100, 0.8900, 0.9300, 0.7600, 0.6400, 0.8100, 0.0000, 0.8700, 0.9100, 0.7000, 0.6700, 0.0000, 0.0000, 0.8600, 0.8600, 0.0000, 0.0000, 0.8400, 0.7200, 0.6000, 0.9500, 0.7300, 0.6400, 0.0000, 0.5700, 0.7400, Acc_f: 0.0000, Acc_r: 0.7635\n",
      "0.9100, 0.8700, 0.6900, 0.6700, 0.6400, 0.8300, 0.8600, 0.8600, 0.9200, 0.8300, 0.5500, 0.0000, 0.8400, 0.7100, 0.6800, 0.8100, 0.7900, 0.9400, 0.6500, 0.0000, 0.9000, 0.8900, 0.7100, 0.8700, 0.0000, 0.6700, 0.7400, 0.6300, 0.0000, 0.7300, 0.7200, 0.8100, 0.0000, 0.6800, 0.8700, 0.6400, 0.8700, 0.0000, 0.6700, 0.0000, 0.7300, 0.9300, 0.0000, 0.8100, 0.5100, 0.6400, 0.6400, 0.7300, 0.9500, 0.8800, 0.6200, 0.0000, 0.6700, 0.9600, 0.8800, 0.6300, 0.0000, 0.7900, 0.8900, 0.7300, 0.9200, 0.7200, 0.8100, 0.7300, 0.6100, 0.0000, 0.8100, 0.0000, 0.0000, 0.8400, 0.7800, 0.8300, 0.0000, 0.6300, 0.5500, 0.9500, 0.9100, 0.7500, 0.6900, 0.7800, 0.0000, 0.8500, 0.9200, 0.7300, 0.6400, 0.0000, 0.0000, 0.8700, 0.8500, 0.0000, 0.0000, 0.8500, 0.6600, 0.6800, 0.9300, 0.7500, 0.6500, 0.0000, 0.5400, 0.7100, Acc_f: 0.0000, Acc_r: 0.7672\n",
      "Original model Acc_f: 75.20 \\pm 0.66\n",
      "Original model Acc_r: 74.98 \\pm 0.32\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 76.44 \\pm 0.21\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 20\n",
    "args.unlearn_class = dict[num_unlearn]['unlearn_class']\n",
    "arxiv_name_r = dict[num_unlearn]['arxiv_name']\n",
    "\n",
    "arxiv_name_o = 'original_model_12-20-05-11'\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_r}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "  \n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100, 0.8300, 0.6100, 0.6500, 0.6700, 0.7900, 0.8200, 0.7700, 0.9000, 0.8600, 0.5200, 0.5000, 0.7900, 0.7400, 0.7300, 0.7600, 0.7500, 0.9000, 0.5900, 0.7100, 0.8600, 0.9000, 0.7700, 0.8000, 0.8200, 0.5700, 0.7700, 0.6100, 0.8100, 0.7500, 0.6600, 0.7500, 0.6900, 0.6700, 0.8200, 0.4400, 0.8600, 0.7600, 0.7300, 0.8900, 0.6300, 0.9000, 0.8000, 0.8400, 0.5100, 0.6400, 0.5100, 0.7100, 0.9400, 0.8700, 0.5600, 0.7300, 0.6600, 0.9100, 0.8100, 0.5200, 0.8700, 0.8200, 0.8700, 0.7000, 0.8900, 0.7500, 0.7900, 0.7000, 0.5900, 0.6200, 0.8300, 0.7100, 0.9400, 0.8300, 0.7500, 0.7900, 0.4700, 0.6400, 0.5800, 0.9000, 0.9200, 0.7000, 0.7100, 0.8200, 0.6000, 0.7700, 0.9200, 0.7200, 0.6700, 0.8500, 0.7500, 0.9000, 0.8500, 0.8900, 0.8400, 0.8400, 0.6700, 0.5600, 0.9300, 0.7400, 0.6700, 0.8100, 0.5500, 0.7100, Acc_f: 0.7530, Acc_r: 0.7455\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 77/576\n",
      "Layer 3 : 247/576\n",
      "Layer 4 : 258/576\n",
      "Layer 5 : 444/576\n",
      "Layer 6 : 335/576\n",
      "Layer 7 : 930/1152\n",
      "Layer 8 : 52/64\n",
      "Layer 9 : 929/1152\n",
      "Layer 10 : 1016/1152\n",
      "Layer 11 : 949/1152\n",
      "Layer 12 : 1908/2304\n",
      "Layer 13 : 117/128\n",
      "Layer 14 : 1934/2304\n",
      "Layer 15 : 1998/2304\n",
      "Layer 16 : 1937/2304\n",
      "Layer 17 : 934/4608\n",
      "Layer 18 : 242/256\n",
      "Layer 19 : 909/4608\n",
      "Layer 20 : 837/4608\n",
      "Layer 21 : 116/512\n",
      "----------------------------------------\n",
      "0.8800, 0.8700, 0.6700, 0.6100, 0.7100, 0.8000, 0.8400, 0.7900, 0.8600, 0.8600, 0.4800, 0.4300, 0.8300, 0.7000, 0.7300, 0.8100, 0.8000, 0.8400, 0.6800, 0.6600, 0.9000, 0.9200, 0.7500, 0.8700, 0.8200, 0.6000, 0.7000, 0.6200, 0.8100, 0.7100, 0.7200, 0.7800, 0.6700, 0.6100, 0.7900, 0.5800, 0.8300, 0.7600, 0.7300, 0.8700, 0.7100, 0.9000, 0.7900, 0.8300, 0.5200, 0.7000, 0.5400, 0.7300, 0.9500, 0.8800, 0.5900, 0.7200, 0.6800, 0.9400, 0.8800, 0.5700, 0.8800, 0.8200, 0.8000, 0.7500, 0.9100, 0.7100, 0.7600, 0.7200, 0.5100, 0.5800, 0.8100, 0.6200, 0.9400, 0.8500, 0.7700, 0.8200, 0.5500, 0.6100, 0.5400, 0.8900, 0.9100, 0.7100, 0.6700, 0.8200, 0.6100, 0.8100, 0.8900, 0.7000, 0.7300, 0.8500, 0.7100, 0.8600, 0.7600, 0.9100, 0.8800, 0.8400, 0.6500, 0.5500, 0.9100, 0.7400, 0.6300, 0.8100, 0.5900, 0.7300, Acc_f: 0.7435, Acc_r: 0.7508\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 80/576\n",
      "Layer 3 : 250/576\n",
      "Layer 4 : 260/576\n",
      "Layer 5 : 451/576\n",
      "Layer 6 : 344/576\n",
      "Layer 7 : 947/1152\n",
      "Layer 8 : 53/64\n",
      "Layer 9 : 928/1152\n",
      "Layer 10 : 1011/1152\n",
      "Layer 11 : 951/1152\n",
      "Layer 12 : 1909/2304\n",
      "Layer 13 : 117/128\n",
      "Layer 14 : 1926/2304\n",
      "Layer 15 : 2001/2304\n",
      "Layer 16 : 1934/2304\n",
      "Layer 17 : 937/4608\n",
      "Layer 18 : 242/256\n",
      "Layer 19 : 916/4608\n",
      "Layer 20 : 851/4608\n",
      "Layer 21 : 115/512\n",
      "----------------------------------------\n",
      "0.9100, 0.8500, 0.6400, 0.6300, 0.6600, 0.7600, 0.8100, 0.7800, 0.8900, 0.8600, 0.4800, 0.4500, 0.8400, 0.7000, 0.6900, 0.8100, 0.7700, 0.8600, 0.6500, 0.7000, 0.8900, 0.9200, 0.7300, 0.8100, 0.8000, 0.5900, 0.7300, 0.6600, 0.7900, 0.7200, 0.6700, 0.7200, 0.7100, 0.6800, 0.8200, 0.6300, 0.8300, 0.8300, 0.6700, 0.8900, 0.6900, 0.8900, 0.8300, 0.8700, 0.4900, 0.7000, 0.5000, 0.7100, 0.9600, 0.9200, 0.6100, 0.7700, 0.6400, 0.9300, 0.8600, 0.5200, 0.9000, 0.7800, 0.9100, 0.6900, 0.9100, 0.7500, 0.8000, 0.7100, 0.5600, 0.6500, 0.8100, 0.6400, 0.9600, 0.8600, 0.7800, 0.8600, 0.5000, 0.5900, 0.5800, 0.9100, 0.9000, 0.7100, 0.6600, 0.8200, 0.6600, 0.8100, 0.9200, 0.7000, 0.6800, 0.8400, 0.7400, 0.8600, 0.8300, 0.9100, 0.8200, 0.8500, 0.7000, 0.6500, 0.9400, 0.7500, 0.6900, 0.8000, 0.6000, 0.7400, Acc_f: 0.7595, Acc_r: 0.7533\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 78/576\n",
      "Layer 3 : 247/576\n",
      "Layer 4 : 261/576\n",
      "Layer 5 : 453/576\n",
      "Layer 6 : 337/576\n",
      "Layer 7 : 941/1152\n",
      "Layer 8 : 52/64\n",
      "Layer 9 : 927/1152\n",
      "Layer 10 : 1017/1152\n",
      "Layer 11 : 949/1152\n",
      "Layer 12 : 1890/2304\n",
      "Layer 13 : 116/128\n",
      "Layer 14 : 1921/2304\n",
      "Layer 15 : 1992/2304\n",
      "Layer 16 : 1934/2304\n",
      "Layer 17 : 938/4608\n",
      "Layer 18 : 242/256\n",
      "Layer 19 : 915/4608\n",
      "Layer 20 : 846/4608\n",
      "Layer 21 : 116/512\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Proj_mat_lst =[]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    \n",
    "    feature_list = []\n",
    "    merged_feat_mat = []\n",
    "    for batch, (x, y) in enumerate(remain_train_loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        mat_list = get_representation_matrix(model, x, batch_list=[256]*30)\n",
    "        break\n",
    "    threshold = 0.99\n",
    "    merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "    proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "    Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.56)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(25):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mince Acc_f: 0.60 \\pm 0.33\n",
      "Mince Acc_r: 92.73 \\pm 0.25\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9245, 0.9305, 0.9270])\n",
    "Acc_f = 100*np.array([0.0020, 0.0060, 0.0100])\n",
    "\n",
    "print(f'Mince Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Mince Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460, 0.9610, 0.8810, 0.7980, 0.9060, 0.8590, 0.9580, 0.8990, 0.9420, 0.9070, Acc_f: 0.8727, Acc_r: 0.9199\n",
      "0.9250, 0.9540, 0.8760, 0.8210, 0.9110, 0.8740, 0.9270, 0.9390, 0.9370, 0.9420, Acc_f: 0.8830, Acc_r: 0.9224\n",
      "0.9270, 0.9530, 0.8860, 0.7790, 0.9280, 0.8710, 0.9290, 0.9280, 0.9530, 0.9470, Acc_f: 0.8677, Acc_r: 0.9283\n",
      "------------ Retrained model ------------\n",
      "0.9350, 0.0000, 0.8920, 0.0000, 0.9370, 0.0000, 0.9640, 0.9460, 0.9480, 0.9730, Acc_f: 0.0000, Acc_r: 0.9421\n",
      "0.9300, 0.0000, 0.8990, 0.0000, 0.9490, 0.0000, 0.9450, 0.9560, 0.9450, 0.9590, Acc_f: 0.0000, Acc_r: 0.9404\n",
      "0.9420, 0.0000, 0.9120, 0.0000, 0.9570, 0.0000, 0.9590, 0.9390, 0.9390, 0.9640, Acc_f: 0.0000, Acc_r: 0.9446\n",
      "Original model Acc_f: 87.44 \\pm 0.64\n",
      "Original model Acc_r: 92.35 \\pm 0.35\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 94.24 \\pm 0.17\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = dict[3]['arxiv_name']\n",
    "args.unlearn_class = dict[3]['unlearn_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_12-20-02-15_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n",
    "\n",
    "arxiv_name = 'original_model_12-20-02-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460, 0.9610, 0.8810, 0.7980, 0.9060, 0.8590, 0.9580, 0.8990, 0.9420, 0.9070, Acc_f: 0.8727, Acc_r: 0.9199\n",
      "[train] epoch 0, batch 26, loss 1.7692084312438965\n",
      "0.8860, 0.4480, 0.8700, 0.1550, 0.9100, 0.0760, 0.9480, 0.8980, 0.9690, 0.9470, Acc_f: 0.2263, Acc_r: 0.9183\n",
      "[train] epoch 1, batch 26, loss 1.0091466903686523\n",
      "0.9080, 0.2070, 0.8830, 0.1140, 0.9200, 0.0530, 0.9590, 0.9200, 0.9630, 0.9430, Acc_f: 0.1247, Acc_r: 0.9280\n",
      "[train] epoch 2, batch 26, loss 0.6982631087303162\n",
      "0.9220, 0.1080, 0.8830, 0.0890, 0.9120, 0.0380, 0.9680, 0.9170, 0.9620, 0.9390, Acc_f: 0.0783, Acc_r: 0.9290\n",
      "[train] epoch 3, batch 26, loss 0.5673158764839172\n",
      "0.9350, 0.0560, 0.8860, 0.0800, 0.9240, 0.0340, 0.9670, 0.9240, 0.9590, 0.9400, Acc_f: 0.0567, Acc_r: 0.9336\n",
      "[train] epoch 4, batch 26, loss 0.5916770100593567\n",
      "0.9430, 0.0360, 0.8880, 0.0580, 0.9180, 0.0270, 0.9680, 0.9290, 0.9540, 0.9360, Acc_f: 0.0403, Acc_r: 0.9337\n",
      "[train] epoch 5, batch 26, loss 0.5266324281692505\n",
      "0.9480, 0.0200, 0.8870, 0.0500, 0.9090, 0.0240, 0.9690, 0.9290, 0.9530, 0.9380, Acc_f: 0.0313, Acc_r: 0.9333\n",
      "[train] epoch 6, batch 26, loss 0.5006242394447327\n",
      "0.9480, 0.0110, 0.8850, 0.0420, 0.9090, 0.0210, 0.9690, 0.9310, 0.9500, 0.9390, Acc_f: 0.0247, Acc_r: 0.9330\n",
      "[train] epoch 7, batch 26, loss 0.34113752841949463\n",
      "0.9510, 0.0070, 0.8800, 0.0390, 0.9110, 0.0180, 0.9680, 0.9320, 0.9510, 0.9410, Acc_f: 0.0213, Acc_r: 0.9334\n",
      "[train] epoch 8, batch 26, loss 0.4131404161453247\n",
      "0.9500, 0.0050, 0.8790, 0.0350, 0.9100, 0.0160, 0.9690, 0.9330, 0.9510, 0.9430, Acc_f: 0.0187, Acc_r: 0.9336\n",
      "[train] epoch 9, batch 26, loss 0.42921799421310425\n",
      "0.9510, 0.0020, 0.8740, 0.0340, 0.9110, 0.0150, 0.9720, 0.9330, 0.9520, 0.9450, Acc_f: 0.0170, Acc_r: 0.9340\n",
      "[train] epoch 10, batch 26, loss 0.4241725504398346\n",
      "0.9510, 0.0000, 0.8780, 0.0310, 0.9130, 0.0150, 0.9710, 0.9320, 0.9500, 0.9460, Acc_f: 0.0153, Acc_r: 0.9344\n",
      "[train] epoch 11, batch 26, loss 0.5177747011184692\n",
      "0.9500, 0.0000, 0.8780, 0.0230, 0.9110, 0.0120, 0.9730, 0.9330, 0.9500, 0.9460, Acc_f: 0.0117, Acc_r: 0.9344\n",
      "[train] epoch 12, batch 26, loss 0.39717304706573486\n",
      "0.9510, 0.0000, 0.8800, 0.0190, 0.9080, 0.0110, 0.9730, 0.9310, 0.9490, 0.9470, Acc_f: 0.0100, Acc_r: 0.9341\n",
      "[train] epoch 13, batch 26, loss 0.5129648447036743\n",
      "0.9500, 0.0000, 0.8800, 0.0160, 0.9090, 0.0100, 0.9720, 0.9310, 0.9490, 0.9480, Acc_f: 0.0087, Acc_r: 0.9341\n",
      "[train] epoch 14, batch 26, loss 0.39828503131866455\n",
      "0.9520, 0.0000, 0.8770, 0.0140, 0.9150, 0.0070, 0.9720, 0.9320, 0.9490, 0.9500, Acc_f: 0.0070, Acc_r: 0.9353\n",
      "[train] epoch 15, batch 26, loss 0.29899725317955017\n",
      "0.9520, 0.0000, 0.8810, 0.0120, 0.9130, 0.0060, 0.9710, 0.9320, 0.9480, 0.9480, Acc_f: 0.0060, Acc_r: 0.9350\n",
      "[train] epoch 16, batch 26, loss 0.37921297550201416\n",
      "0.9520, 0.0000, 0.8780, 0.0100, 0.9130, 0.0060, 0.9720, 0.9320, 0.9500, 0.9500, Acc_f: 0.0053, Acc_r: 0.9353\n",
      "[train] epoch 17, batch 26, loss 0.3435928523540497\n",
      "0.9530, 0.0000, 0.8800, 0.0090, 0.9130, 0.0060, 0.9710, 0.9350, 0.9480, 0.9490, Acc_f: 0.0050, Acc_r: 0.9356\n",
      "[train] epoch 18, batch 26, loss 0.3069719672203064\n",
      "0.9530, 0.0000, 0.8800, 0.0090, 0.9140, 0.0060, 0.9710, 0.9340, 0.9500, 0.9490, Acc_f: 0.0050, Acc_r: 0.9359\n",
      "[train] epoch 19, batch 26, loss 0.38145551085472107\n",
      "0.9530, 0.0000, 0.8800, 0.0090, 0.9140, 0.0060, 0.9720, 0.9320, 0.9480, 0.9510, Acc_f: 0.0050, Acc_r: 0.9357\n",
      "[train] epoch 20, batch 26, loss 0.3740042746067047\n",
      "0.9520, 0.0000, 0.8790, 0.0070, 0.9190, 0.0060, 0.9720, 0.9350, 0.9500, 0.9510, Acc_f: 0.0043, Acc_r: 0.9369\n",
      "[train] epoch 21, batch 26, loss 0.3112730085849762\n",
      "0.9520, 0.0000, 0.8830, 0.0070, 0.9120, 0.0060, 0.9720, 0.9340, 0.9460, 0.9510, Acc_f: 0.0043, Acc_r: 0.9357\n",
      "[train] epoch 22, batch 26, loss 0.30402085185050964\n",
      "0.9500, 0.0000, 0.8820, 0.0060, 0.9160, 0.0060, 0.9720, 0.9330, 0.9470, 0.9510, Acc_f: 0.0040, Acc_r: 0.9359\n",
      "[train] epoch 23, batch 26, loss 0.40167734026908875\n",
      "0.9510, 0.0000, 0.8820, 0.0050, 0.9140, 0.0050, 0.9710, 0.9340, 0.9470, 0.9510, Acc_f: 0.0033, Acc_r: 0.9357\n",
      "[train] epoch 24, batch 26, loss 0.33487948775291443\n",
      "0.9500, 0.0000, 0.8830, 0.0050, 0.9100, 0.0050, 0.9700, 0.9330, 0.9470, 0.9520, Acc_f: 0.0033, Acc_r: 0.9350\n",
      "0.9250, 0.9540, 0.8760, 0.8210, 0.9110, 0.8740, 0.9270, 0.9390, 0.9370, 0.9420, Acc_f: 0.8830, Acc_r: 0.9224\n",
      "[train] epoch 0, batch 26, loss 1.2200292348861694\n",
      "0.8310, 0.1050, 0.7260, 0.0410, 0.8990, 0.0550, 0.8320, 0.9640, 0.8680, 0.9690, Acc_f: 0.0670, Acc_r: 0.8699\n",
      "[train] epoch 1, batch 26, loss 0.7834221124649048\n",
      "0.9170, 0.0670, 0.8740, 0.0780, 0.9200, 0.0530, 0.9310, 0.9420, 0.9050, 0.9730, Acc_f: 0.0660, Acc_r: 0.9231\n",
      "[train] epoch 2, batch 26, loss 0.662040114402771\n",
      "0.9290, 0.0400, 0.8740, 0.0550, 0.9050, 0.0280, 0.9140, 0.9620, 0.9400, 0.9660, Acc_f: 0.0410, Acc_r: 0.9271\n",
      "[train] epoch 3, batch 26, loss 0.7118478417396545\n",
      "0.9380, 0.0340, 0.8670, 0.0550, 0.9210, 0.0330, 0.9380, 0.9630, 0.9350, 0.9720, Acc_f: 0.0407, Acc_r: 0.9334\n",
      "[train] epoch 4, batch 26, loss 0.42120516300201416\n",
      "0.9280, 0.0390, 0.8930, 0.0480, 0.9240, 0.0290, 0.9520, 0.9540, 0.9340, 0.9680, Acc_f: 0.0387, Acc_r: 0.9361\n",
      "[train] epoch 5, batch 26, loss 0.48219946026802063\n",
      "0.9340, 0.0220, 0.8720, 0.0260, 0.9260, 0.0170, 0.9240, 0.9700, 0.9420, 0.9730, Acc_f: 0.0217, Acc_r: 0.9344\n",
      "[train] epoch 6, batch 26, loss 0.42895010113716125\n",
      "0.9270, 0.0200, 0.8810, 0.0290, 0.9100, 0.0210, 0.9340, 0.9730, 0.9450, 0.9730, Acc_f: 0.0233, Acc_r: 0.9347\n",
      "[train] epoch 7, batch 26, loss 0.4893246293067932\n",
      "0.9300, 0.0150, 0.8940, 0.0240, 0.9000, 0.0110, 0.9370, 0.9720, 0.9520, 0.9720, Acc_f: 0.0167, Acc_r: 0.9367\n",
      "[train] epoch 8, batch 26, loss 0.4553273916244507\n",
      "0.9370, 0.0160, 0.8940, 0.0250, 0.8930, 0.0140, 0.9310, 0.9720, 0.9400, 0.9710, Acc_f: 0.0183, Acc_r: 0.9340\n",
      "[train] epoch 9, batch 26, loss 0.407753050327301\n",
      "0.9300, 0.0140, 0.8860, 0.0170, 0.9250, 0.0100, 0.9500, 0.9640, 0.9430, 0.9730, Acc_f: 0.0137, Acc_r: 0.9387\n",
      "[train] epoch 10, batch 26, loss 0.5035009384155273\n",
      "0.9290, 0.0100, 0.8530, 0.0140, 0.8970, 0.0080, 0.8900, 0.9770, 0.9410, 0.9750, Acc_f: 0.0107, Acc_r: 0.9231\n",
      "[train] epoch 11, batch 26, loss 0.4696825444698334\n",
      "0.9390, 0.0100, 0.8890, 0.0210, 0.8920, 0.0150, 0.9410, 0.9710, 0.9450, 0.9710, Acc_f: 0.0153, Acc_r: 0.9354\n",
      "[train] epoch 12, batch 26, loss 0.37276002764701843\n",
      "0.9220, 0.0110, 0.8880, 0.0180, 0.9230, 0.0100, 0.9500, 0.9700, 0.9470, 0.9780, Acc_f: 0.0130, Acc_r: 0.9397\n",
      "[train] epoch 13, batch 26, loss 0.39190855622291565\n",
      "0.9380, 0.0100, 0.8940, 0.0140, 0.9130, 0.0090, 0.9290, 0.9700, 0.9470, 0.9730, Acc_f: 0.0110, Acc_r: 0.9377\n",
      "[train] epoch 14, batch 26, loss 0.3799382150173187\n",
      "0.9320, 0.0130, 0.8960, 0.0120, 0.9380, 0.0110, 0.9630, 0.9480, 0.9380, 0.9650, Acc_f: 0.0120, Acc_r: 0.9400\n",
      "[train] epoch 15, batch 26, loss 0.3921247124671936\n",
      "0.9270, 0.0060, 0.8880, 0.0160, 0.9270, 0.0080, 0.9510, 0.9680, 0.9530, 0.9690, Acc_f: 0.0100, Acc_r: 0.9404\n",
      "[train] epoch 16, batch 26, loss 0.3712095618247986\n",
      "0.9320, 0.0040, 0.8840, 0.0120, 0.8850, 0.0070, 0.9470, 0.9710, 0.9450, 0.9790, Acc_f: 0.0077, Acc_r: 0.9347\n",
      "[train] epoch 17, batch 26, loss 0.34007367491722107\n",
      "0.9330, 0.0080, 0.8960, 0.0120, 0.9280, 0.0130, 0.9500, 0.9660, 0.9410, 0.9750, Acc_f: 0.0110, Acc_r: 0.9413\n",
      "[train] epoch 18, batch 26, loss 0.3716970682144165\n",
      "0.9290, 0.0050, 0.8930, 0.0090, 0.9220, 0.0070, 0.9420, 0.9680, 0.9490, 0.9710, Acc_f: 0.0070, Acc_r: 0.9391\n",
      "[train] epoch 19, batch 26, loss 0.34018391370773315\n",
      "0.9330, 0.0070, 0.8870, 0.0090, 0.9280, 0.0080, 0.9630, 0.9660, 0.9440, 0.9710, Acc_f: 0.0080, Acc_r: 0.9417\n",
      "[train] epoch 20, batch 26, loss 0.42577171325683594\n",
      "0.9370, 0.0050, 0.8940, 0.0070, 0.9310, 0.0040, 0.9690, 0.9550, 0.9440, 0.9660, Acc_f: 0.0053, Acc_r: 0.9423\n",
      "[train] epoch 21, batch 26, loss 0.34282517433166504\n",
      "0.9360, 0.0060, 0.8950, 0.0090, 0.9270, 0.0060, 0.9540, 0.9660, 0.9490, 0.9650, Acc_f: 0.0070, Acc_r: 0.9417\n",
      "[train] epoch 22, batch 26, loss 0.38748985528945923\n",
      "0.9380, 0.0040, 0.8900, 0.0080, 0.9130, 0.0060, 0.9420, 0.9740, 0.9450, 0.9730, Acc_f: 0.0060, Acc_r: 0.9393\n",
      "[train] epoch 23, batch 26, loss 0.28116270899772644\n",
      "0.9360, 0.0060, 0.8970, 0.0080, 0.9360, 0.0080, 0.9520, 0.9620, 0.9470, 0.9710, Acc_f: 0.0073, Acc_r: 0.9430\n",
      "[train] epoch 24, batch 26, loss 0.3791363835334778\n",
      "0.9400, 0.0040, 0.8910, 0.0070, 0.9160, 0.0040, 0.9660, 0.9650, 0.9430, 0.9710, Acc_f: 0.0050, Acc_r: 0.9417\n",
      "0.9270, 0.9530, 0.8860, 0.7790, 0.9280, 0.8710, 0.9290, 0.9280, 0.9530, 0.9470, Acc_f: 0.8677, Acc_r: 0.9283\n",
      "[train] epoch 0, batch 26, loss 1.3177087306976318\n",
      "0.8800, 0.1370, 0.8400, 0.0480, 0.8230, 0.0250, 0.8800, 0.9080, 0.7110, 0.9640, Acc_f: 0.0700, Acc_r: 0.8580\n",
      "[train] epoch 1, batch 26, loss 0.8095312714576721\n",
      "0.8880, 0.0720, 0.8730, 0.0430, 0.8300, 0.0380, 0.9090, 0.9290, 0.8490, 0.9610, Acc_f: 0.0510, Acc_r: 0.8913\n",
      "[train] epoch 2, batch 26, loss 0.8968367576599121\n",
      "0.9050, 0.0440, 0.8550, 0.0400, 0.8740, 0.0420, 0.9390, 0.9320, 0.8980, 0.9660, Acc_f: 0.0420, Acc_r: 0.9099\n",
      "[train] epoch 3, batch 26, loss 0.7734481692314148\n",
      "0.9200, 0.0540, 0.8730, 0.0370, 0.8780, 0.0300, 0.9280, 0.9290, 0.8930, 0.9610, Acc_f: 0.0403, Acc_r: 0.9117\n",
      "[train] epoch 4, batch 26, loss 0.7836394309997559\n",
      "0.8870, 0.0360, 0.8770, 0.0370, 0.9170, 0.0480, 0.9440, 0.9250, 0.9260, 0.9640, Acc_f: 0.0403, Acc_r: 0.9200\n",
      "[train] epoch 5, batch 26, loss 0.5330976247787476\n",
      "0.9140, 0.0390, 0.8740, 0.0370, 0.9190, 0.0460, 0.9290, 0.9340, 0.9360, 0.9600, Acc_f: 0.0407, Acc_r: 0.9237\n",
      "[train] epoch 6, batch 26, loss 0.618005096912384\n",
      "0.9080, 0.0330, 0.8780, 0.0390, 0.9240, 0.0470, 0.9030, 0.9440, 0.9430, 0.9650, Acc_f: 0.0397, Acc_r: 0.9236\n",
      "[train] epoch 7, batch 26, loss 0.5624178051948547\n",
      "0.9220, 0.0330, 0.8800, 0.0350, 0.9220, 0.0460, 0.8990, 0.9480, 0.9430, 0.9640, Acc_f: 0.0380, Acc_r: 0.9254\n",
      "[train] epoch 8, batch 26, loss 0.544209361076355\n",
      "0.9310, 0.0240, 0.8920, 0.0220, 0.8960, 0.0190, 0.9340, 0.9170, 0.9390, 0.9520, Acc_f: 0.0217, Acc_r: 0.9230\n",
      "[train] epoch 9, batch 26, loss 0.5122095346450806\n",
      "0.9100, 0.0190, 0.8830, 0.0210, 0.9260, 0.0190, 0.9380, 0.9310, 0.9410, 0.9600, Acc_f: 0.0197, Acc_r: 0.9270\n",
      "[train] epoch 10, batch 26, loss 0.6261149048805237\n",
      "0.9230, 0.0320, 0.8990, 0.0260, 0.9200, 0.0270, 0.9300, 0.9230, 0.9350, 0.9540, Acc_f: 0.0283, Acc_r: 0.9263\n",
      "[train] epoch 11, batch 26, loss 0.406993567943573\n",
      "0.9330, 0.0180, 0.8780, 0.0220, 0.9310, 0.0240, 0.9250, 0.9430, 0.9480, 0.9600, Acc_f: 0.0213, Acc_r: 0.9311\n",
      "[train] epoch 12, batch 26, loss 0.45675984025001526\n",
      "0.9080, 0.0100, 0.8820, 0.0240, 0.9350, 0.0290, 0.9160, 0.9440, 0.9560, 0.9650, Acc_f: 0.0210, Acc_r: 0.9294\n",
      "[train] epoch 13, batch 26, loss 0.43199387192726135\n",
      "0.9330, 0.0160, 0.8780, 0.0240, 0.9310, 0.0280, 0.9330, 0.9380, 0.9520, 0.9590, Acc_f: 0.0227, Acc_r: 0.9320\n",
      "[train] epoch 14, batch 26, loss 0.4600633680820465\n",
      "0.9260, 0.0160, 0.8870, 0.0240, 0.9250, 0.0280, 0.9110, 0.9480, 0.9530, 0.9610, Acc_f: 0.0227, Acc_r: 0.9301\n",
      "[train] epoch 15, batch 26, loss 0.39069056510925293\n",
      "0.9190, 0.0090, 0.8790, 0.0230, 0.9290, 0.0270, 0.9180, 0.9460, 0.9580, 0.9640, Acc_f: 0.0197, Acc_r: 0.9304\n",
      "[train] epoch 16, batch 26, loss 0.36038902401924133\n",
      "0.9210, 0.0110, 0.8800, 0.0170, 0.9310, 0.0240, 0.9420, 0.9420, 0.9600, 0.9600, Acc_f: 0.0173, Acc_r: 0.9337\n",
      "[train] epoch 17, batch 26, loss 0.4014970362186432\n",
      "0.9350, 0.0090, 0.8770, 0.0220, 0.9350, 0.0250, 0.9290, 0.9420, 0.9580, 0.9630, Acc_f: 0.0187, Acc_r: 0.9341\n",
      "[train] epoch 18, batch 26, loss 0.4310663640499115\n",
      "0.9340, 0.0090, 0.8950, 0.0200, 0.9260, 0.0220, 0.9270, 0.9340, 0.9540, 0.9620, Acc_f: 0.0170, Acc_r: 0.9331\n",
      "[train] epoch 19, batch 26, loss 0.4118087887763977\n",
      "0.9310, 0.0090, 0.8730, 0.0160, 0.9340, 0.0170, 0.9480, 0.9370, 0.9550, 0.9630, Acc_f: 0.0140, Acc_r: 0.9344\n",
      "[train] epoch 20, batch 26, loss 0.45858705043792725\n",
      "0.9420, 0.0120, 0.8790, 0.0240, 0.9380, 0.0270, 0.9320, 0.9360, 0.9550, 0.9610, Acc_f: 0.0210, Acc_r: 0.9347\n",
      "[train] epoch 21, batch 26, loss 0.32839399576187134\n",
      "0.9300, 0.0070, 0.8930, 0.0190, 0.9290, 0.0210, 0.9270, 0.9400, 0.9560, 0.9620, Acc_f: 0.0157, Acc_r: 0.9339\n",
      "[train] epoch 22, batch 26, loss 0.4111242890357971\n",
      "0.9360, 0.0060, 0.8890, 0.0130, 0.9180, 0.0140, 0.9330, 0.9370, 0.9530, 0.9610, Acc_f: 0.0110, Acc_r: 0.9324\n",
      "[train] epoch 23, batch 26, loss 0.4729559123516083\n",
      "0.9390, 0.0090, 0.8900, 0.0160, 0.9250, 0.0200, 0.9380, 0.9420, 0.9540, 0.9600, Acc_f: 0.0150, Acc_r: 0.9354\n",
      "[train] epoch 24, batch 26, loss 0.4255433976650238\n",
      "0.9330, 0.0080, 0.8920, 0.0170, 0.9300, 0.0180, 0.9260, 0.9390, 0.9560, 0.9610, Acc_f: 0.0143, Acc_r: 0.9339\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.04)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(25):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mince Acc_f: 0.89 \\pm 0.45\n",
      "Mince Acc_r: 93.84 \\pm 0.33\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9369,0.9430, 0.9354])\n",
    "Acc_f = 100*np.array([0.0043,0.0073,0.0150])\n",
    "\n",
    "print(f'Mince Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Mince Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460, 0.9610, 0.8810, 0.7980, 0.9060, 0.8590, 0.9580, 0.8990, 0.9420, 0.9070, Acc_f: 0.8792, Acc_r: 0.9233\n",
      "0.9250, 0.9540, 0.8760, 0.8210, 0.9110, 0.8740, 0.9270, 0.9390, 0.9370, 0.9420, Acc_f: 0.8970, Acc_r: 0.9197\n",
      "0.9270, 0.9530, 0.8860, 0.7790, 0.9280, 0.8710, 0.9290, 0.9280, 0.9530, 0.9470, Acc_f: 0.8827, Acc_r: 0.9283\n",
      "------------ Retrained model ------------\n",
      "0.9430, 0.0000, 0.9100, 0.0000, 0.9560, 0.0000, 0.9540, 0.0000, 0.9560, 0.9680, Acc_f: 0.0000, Acc_r: 0.9478\n",
      "0.9210, 0.0000, 0.8900, 0.0000, 0.9400, 0.0000, 0.9520, 0.0000, 0.9580, 0.9760, Acc_f: 0.0000, Acc_r: 0.9395\n",
      "0.9270, 0.0000, 0.9130, 0.0000, 0.9380, 0.0000, 0.9520, 0.0000, 0.9580, 0.9580, Acc_f: 0.0000, Acc_r: 0.9410\n",
      "Original model Acc_f: 88.63 \\pm 0.77\n",
      "Original model Acc_r: 92.38 \\pm 0.36\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 94.28 \\pm 0.36\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = dict[4]['arxiv_name']\n",
    "args.unlearn_class = dict[4]['unlearn_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_12-20-02-15_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n",
    "\n",
    "arxiv_name = 'original_model_12-20-02-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 49/432\n",
      "Layer 3 : 183/432\n",
      "Layer 4 : 555/864\n",
      "Layer 5 : 619/864\n",
      "Layer 6 : 621/864\n",
      "Layer 7 : 665/864\n",
      "Layer 8 : 75/96\n",
      "Layer 9 : 4/96\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 60/432\n",
      "Layer 3 : 215/432\n",
      "Layer 4 : 644/864\n",
      "Layer 5 : 690/864\n",
      "Layer 6 : 678/864\n",
      "Layer 7 : 756/864\n",
      "Layer 8 : 88/96\n",
      "Layer 9 : 8/96\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 70/432\n",
      "Layer 3 : 244/432\n",
      "Layer 4 : 708/864\n",
      "Layer 5 : 739/864\n",
      "Layer 6 : 721/864\n",
      "Layer 7 : 788/864\n",
      "Layer 8 : 92/96\n",
      "Layer 9 : 10/96\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/27\n",
      "Layer 2 : 105/432\n",
      "Layer 3 : 288/432\n",
      "Layer 4 : 758/864\n",
      "Layer 5 : 778/864\n",
      "Layer 6 : 761/864\n",
      "Layer 7 : 811/864\n",
      "Layer 8 : 92/96\n",
      "Layer 9 : 11/96\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 136/432\n",
      "Layer 3 : 322/432\n",
      "Layer 4 : 786/864\n",
      "Layer 5 : 805/864\n",
      "Layer 6 : 791/864\n",
      "Layer 7 : 830/864\n",
      "Layer 8 : 93/96\n",
      "Layer 9 : 13/96\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 13/27\n",
      "Layer 2 : 203/432\n",
      "Layer 3 : 370/432\n",
      "Layer 4 : 828/864\n",
      "Layer 5 : 834/864\n",
      "Layer 6 : 827/864\n",
      "Layer 7 : 849/864\n",
      "Layer 8 : 95/96\n",
      "Layer 9 : 16/96\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 50/432\n",
      "Layer 3 : 192/432\n",
      "Layer 4 : 562/864\n",
      "Layer 5 : 616/864\n",
      "Layer 6 : 616/864\n",
      "Layer 7 : 671/864\n",
      "Layer 8 : 78/96\n",
      "Layer 9 : 5/96\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 7/27\n",
      "Layer 2 : 63/432\n",
      "Layer 3 : 216/432\n",
      "Layer 4 : 649/864\n",
      "Layer 5 : 694/864\n",
      "Layer 6 : 671/864\n",
      "Layer 7 : 760/864\n",
      "Layer 8 : 88/96\n",
      "Layer 9 : 8/96\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 76/432\n",
      "Layer 3 : 245/432\n",
      "Layer 4 : 697/864\n",
      "Layer 5 : 740/864\n",
      "Layer 6 : 711/864\n",
      "Layer 7 : 789/864\n",
      "Layer 8 : 91/96\n",
      "Layer 9 : 9/96\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/27\n",
      "Layer 2 : 108/432\n",
      "Layer 3 : 294/432\n",
      "Layer 4 : 751/864\n",
      "Layer 5 : 782/864\n",
      "Layer 6 : 760/864\n",
      "Layer 7 : 815/864\n",
      "Layer 8 : 93/96\n",
      "Layer 9 : 11/96\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 8\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 134/432\n",
      "Layer 3 : 328/432\n",
      "Layer 4 : 785/864\n",
      "Layer 5 : 808/864\n",
      "Layer 6 : 791/864\n",
      "Layer 7 : 832/864\n",
      "Layer 8 : 93/96\n",
      "Layer 9 : 13/96\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 13/27\n",
      "Layer 2 : 202/432\n",
      "Layer 3 : 373/432\n",
      "Layer 4 : 826/864\n",
      "Layer 5 : 835/864\n",
      "Layer 6 : 825/864\n",
      "Layer 7 : 849/864\n",
      "Layer 8 : 95/96\n",
      "Layer 9 : 15/96\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 48/432\n",
      "Layer 3 : 186/432\n",
      "Layer 4 : 541/864\n",
      "Layer 5 : 609/864\n",
      "Layer 6 : 607/864\n",
      "Layer 7 : 668/864\n",
      "Layer 8 : 79/96\n",
      "Layer 9 : 5/96\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 58/432\n",
      "Layer 3 : 216/432\n",
      "Layer 4 : 635/864\n",
      "Layer 5 : 683/864\n",
      "Layer 6 : 667/864\n",
      "Layer 7 : 755/864\n",
      "Layer 8 : 87/96\n",
      "Layer 9 : 8/96\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 67/432\n",
      "Layer 3 : 242/432\n",
      "Layer 4 : 695/864\n",
      "Layer 5 : 731/864\n",
      "Layer 6 : 712/864\n",
      "Layer 7 : 785/864\n",
      "Layer 8 : 90/96\n",
      "Layer 9 : 10/96\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/27\n",
      "Layer 2 : 94/432\n",
      "Layer 3 : 284/432\n",
      "Layer 4 : 754/864\n",
      "Layer 5 : 774/864\n",
      "Layer 6 : 756/864\n",
      "Layer 7 : 809/864\n",
      "Layer 8 : 93/96\n",
      "Layer 9 : 12/96\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 121/432\n",
      "Layer 3 : 322/432\n",
      "Layer 4 : 782/864\n",
      "Layer 5 : 801/864\n",
      "Layer 6 : 786/864\n",
      "Layer 7 : 831/864\n",
      "Layer 8 : 95/96\n",
      "Layer 9 : 14/96\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 13/27\n",
      "Layer 2 : 186/432\n",
      "Layer 3 : 368/432\n",
      "Layer 4 : 826/864\n",
      "Layer 5 : 833/864\n",
      "Layer 6 : 825/864\n",
      "Layer 7 : 849/864\n",
      "Layer 8 : 96/96\n",
      "Layer 9 : 17/96\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unlearn_indices = np.where(np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "\n",
    "# conver to the original indices\n",
    "unlearn_indices = train_loader.sampler.indices[unlearn_indices]\n",
    "\n",
    "unlearn_sampler = torch.utils.data.SubsetRandomSampler(unlearn_indices)\n",
    "unlearn_subset_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                    batch_size=args.batch_size, \n",
    "                                                    sampler=unlearn_sampler)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "remain_indices = np.where(~np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "remain_indices = train_loader.sampler.indices[remain_indices]\n",
    "\n",
    "remain_sampler = torch.utils.data.SubsetRandomSampler(remain_indices)\n",
    "remain_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            sampler=remain_sampler)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460, 0.9610, 0.8810, 0.7980, 0.9060, 0.8590, 0.9580, 0.8990, 0.9420, 0.9070, Acc_f: 0.8792, Acc_r: 0.9233\n",
      "[train] epoch 0, batch 35, loss 1.23568594455719\n",
      "0.9010, 0.4310, 0.8760, 0.1330, 0.9190, 0.0530, 0.9720, 0.0800, 0.9650, 0.9410, Acc_f: 0.1743, Acc_r: 0.9290\n",
      "[train] epoch 1, batch 35, loss 0.7198920845985413\n",
      "0.9180, 0.1840, 0.8850, 0.0890, 0.9280, 0.0430, 0.9690, 0.0460, 0.9610, 0.9410, Acc_f: 0.0905, Acc_r: 0.9337\n",
      "[train] epoch 2, batch 35, loss 0.6311068534851074\n",
      "0.9360, 0.1010, 0.8830, 0.0660, 0.9220, 0.0320, 0.9740, 0.0310, 0.9560, 0.9400, Acc_f: 0.0575, Acc_r: 0.9352\n",
      "[train] epoch 3, batch 35, loss 0.7038785815238953\n",
      "0.9410, 0.0540, 0.8890, 0.0450, 0.9190, 0.0210, 0.9730, 0.0220, 0.9520, 0.9410, Acc_f: 0.0355, Acc_r: 0.9358\n",
      "[train] epoch 4, batch 35, loss 0.6717749834060669\n",
      "0.9430, 0.0340, 0.8840, 0.0390, 0.9270, 0.0170, 0.9710, 0.0190, 0.9490, 0.9410, Acc_f: 0.0273, Acc_r: 0.9358\n",
      "[train] epoch 5, batch 35, loss 0.33119139075279236\n",
      "0.9420, 0.0210, 0.8800, 0.0320, 0.9240, 0.0120, 0.9730, 0.0140, 0.9490, 0.9450, Acc_f: 0.0198, Acc_r: 0.9355\n",
      "[train] epoch 6, batch 35, loss 0.36045655608177185\n",
      "0.9430, 0.0150, 0.8880, 0.0270, 0.9260, 0.0120, 0.9720, 0.0140, 0.9470, 0.9490, Acc_f: 0.0170, Acc_r: 0.9375\n",
      "[train] epoch 7, batch 35, loss 0.34180232882499695\n",
      "0.9420, 0.0110, 0.8870, 0.0210, 0.9210, 0.0100, 0.9720, 0.0090, 0.9460, 0.9500, Acc_f: 0.0128, Acc_r: 0.9363\n",
      "[train] epoch 8, batch 35, loss 0.4186604619026184\n",
      "0.9410, 0.0080, 0.8890, 0.0180, 0.9220, 0.0080, 0.9710, 0.0050, 0.9470, 0.9500, Acc_f: 0.0097, Acc_r: 0.9367\n",
      "[train] epoch 9, batch 35, loss 0.4230968952178955\n",
      "0.9450, 0.0040, 0.8870, 0.0140, 0.9240, 0.0060, 0.9720, 0.0050, 0.9480, 0.9510, Acc_f: 0.0073, Acc_r: 0.9378\n",
      "[train] epoch 10, batch 35, loss 0.24113766849040985\n",
      "0.9480, 0.0020, 0.8880, 0.0120, 0.9240, 0.0040, 0.9710, 0.0050, 0.9480, 0.9560, Acc_f: 0.0058, Acc_r: 0.9392\n",
      "[train] epoch 11, batch 35, loss 0.32363495230674744\n",
      "0.9450, 0.0010, 0.8900, 0.0100, 0.9240, 0.0040, 0.9720, 0.0040, 0.9460, 0.9550, Acc_f: 0.0047, Acc_r: 0.9387\n",
      "[train] epoch 12, batch 35, loss 0.2736654579639435\n",
      "0.9470, 0.0000, 0.8900, 0.0070, 0.9250, 0.0030, 0.9700, 0.0040, 0.9450, 0.9560, Acc_f: 0.0035, Acc_r: 0.9388\n",
      "[train] epoch 13, batch 35, loss 0.30870553851127625\n",
      "0.9460, 0.0000, 0.8890, 0.0060, 0.9280, 0.0010, 0.9720, 0.0040, 0.9440, 0.9560, Acc_f: 0.0027, Acc_r: 0.9392\n",
      "[train] epoch 14, batch 35, loss 0.3539218604564667\n",
      "0.9500, 0.0000, 0.8860, 0.0060, 0.9200, 0.0010, 0.9730, 0.0040, 0.9450, 0.9580, Acc_f: 0.0027, Acc_r: 0.9387\n",
      "[train] epoch 15, batch 35, loss 0.23131053149700165\n",
      "0.9490, 0.0000, 0.8890, 0.0050, 0.9230, 0.0020, 0.9710, 0.0020, 0.9440, 0.9580, Acc_f: 0.0023, Acc_r: 0.9390\n",
      "[train] epoch 16, batch 35, loss 0.3265753388404846\n",
      "0.9480, 0.0000, 0.8860, 0.0040, 0.9230, 0.0000, 0.9730, 0.0010, 0.9460, 0.9580, Acc_f: 0.0013, Acc_r: 0.9390\n",
      "[train] epoch 17, batch 35, loss 0.23176144063472748\n",
      "0.9490, 0.0000, 0.8880, 0.0030, 0.9230, 0.0000, 0.9720, 0.0010, 0.9450, 0.9580, Acc_f: 0.0010, Acc_r: 0.9392\n",
      "[train] epoch 18, batch 35, loss 0.31477510929107666\n",
      "0.9510, 0.0000, 0.8890, 0.0030, 0.9220, 0.0000, 0.9730, 0.0010, 0.9440, 0.9580, Acc_f: 0.0010, Acc_r: 0.9395\n",
      "[train] epoch 19, batch 35, loss 0.29241201281547546\n",
      "0.9470, 0.0000, 0.8870, 0.0030, 0.9240, 0.0000, 0.9720, 0.0000, 0.9440, 0.9590, Acc_f: 0.0008, Acc_r: 0.9388\n",
      "[train] epoch 20, batch 35, loss 0.2644854784011841\n",
      "0.9490, 0.0000, 0.8870, 0.0030, 0.9210, 0.0000, 0.9730, 0.0010, 0.9440, 0.9590, Acc_f: 0.0010, Acc_r: 0.9388\n",
      "[train] epoch 21, batch 35, loss 0.360984742641449\n",
      "0.9490, 0.0000, 0.8860, 0.0020, 0.9310, 0.0000, 0.9720, 0.0000, 0.9440, 0.9590, Acc_f: 0.0005, Acc_r: 0.9402\n",
      "[train] epoch 22, batch 35, loss 0.28425028920173645\n",
      "0.9500, 0.0000, 0.8880, 0.0020, 0.9270, 0.0000, 0.9710, 0.0000, 0.9440, 0.9590, Acc_f: 0.0005, Acc_r: 0.9398\n",
      "[train] epoch 23, batch 35, loss 0.37418264150619507\n",
      "0.9500, 0.0000, 0.8900, 0.0010, 0.9260, 0.0000, 0.9710, 0.0000, 0.9430, 0.9580, Acc_f: 0.0003, Acc_r: 0.9397\n",
      "[train] epoch 24, batch 35, loss 0.3655208349227905\n",
      "0.9490, 0.0000, 0.8890, 0.0010, 0.9220, 0.0000, 0.9730, 0.0000, 0.9440, 0.9600, Acc_f: 0.0003, Acc_r: 0.9395\n",
      "0.9250, 0.9540, 0.8760, 0.8210, 0.9110, 0.8740, 0.9270, 0.9390, 0.9370, 0.9420, Acc_f: 0.8970, Acc_r: 0.9197\n",
      "[train] epoch 0, batch 35, loss 1.2176315784454346\n",
      "0.8130, 0.1110, 0.7740, 0.0080, 0.9380, 0.0120, 0.8430, 0.0140, 0.8100, 0.9170, Acc_f: 0.0363, Acc_r: 0.8492\n",
      "[train] epoch 1, batch 35, loss 0.8323593735694885\n",
      "0.9190, 0.0270, 0.7320, 0.0150, 0.9110, 0.0210, 0.8570, 0.0210, 0.8980, 0.9660, Acc_f: 0.0210, Acc_r: 0.8805\n",
      "[train] epoch 2, batch 35, loss 0.6819568276405334\n",
      "0.8950, 0.0430, 0.8810, 0.0350, 0.9490, 0.0340, 0.9420, 0.0250, 0.9200, 0.9790, Acc_f: 0.0343, Acc_r: 0.9277\n",
      "[train] epoch 3, batch 35, loss 0.8094990253448486\n",
      "0.8990, 0.0320, 0.9010, 0.0310, 0.9110, 0.0320, 0.9640, 0.0160, 0.9500, 0.9610, Acc_f: 0.0278, Acc_r: 0.9310\n",
      "[train] epoch 4, batch 35, loss 0.4882477819919586\n",
      "0.9210, 0.0110, 0.8720, 0.0200, 0.9480, 0.0230, 0.9210, 0.0190, 0.9540, 0.9730, Acc_f: 0.0182, Acc_r: 0.9315\n",
      "[train] epoch 5, batch 35, loss 0.41488251090049744\n",
      "0.9530, 0.0180, 0.8590, 0.0210, 0.9330, 0.0260, 0.9570, 0.0140, 0.9410, 0.9590, Acc_f: 0.0198, Acc_r: 0.9337\n",
      "[train] epoch 6, batch 35, loss 0.5382733941078186\n",
      "0.9280, 0.0060, 0.8140, 0.0120, 0.9530, 0.0130, 0.9130, 0.0090, 0.9350, 0.9840, Acc_f: 0.0100, Acc_r: 0.9212\n",
      "[train] epoch 7, batch 35, loss 0.6039811372756958\n",
      "0.9250, 0.0020, 0.8260, 0.0050, 0.9010, 0.0090, 0.9240, 0.0050, 0.9460, 0.9810, Acc_f: 0.0053, Acc_r: 0.9172\n",
      "[train] epoch 8, batch 35, loss 0.43832260370254517\n",
      "0.9040, 0.0130, 0.9210, 0.0120, 0.9140, 0.0160, 0.9300, 0.0140, 0.9620, 0.9690, Acc_f: 0.0138, Acc_r: 0.9333\n",
      "[train] epoch 9, batch 35, loss 0.3648748993873596\n",
      "0.9240, 0.0070, 0.8810, 0.0080, 0.9530, 0.0050, 0.9280, 0.0050, 0.9520, 0.9730, Acc_f: 0.0063, Acc_r: 0.9352\n",
      "[train] epoch 10, batch 35, loss 0.3983112573623657\n",
      "0.9240, 0.0090, 0.8900, 0.0090, 0.9460, 0.0160, 0.9630, 0.0060, 0.9560, 0.9680, Acc_f: 0.0100, Acc_r: 0.9412\n",
      "[train] epoch 11, batch 35, loss 0.2749917805194855\n",
      "0.9120, 0.0060, 0.8900, 0.0060, 0.9470, 0.0090, 0.9720, 0.0040, 0.9520, 0.9760, Acc_f: 0.0062, Acc_r: 0.9415\n",
      "[train] epoch 12, batch 35, loss 0.32513606548309326\n",
      "0.9300, 0.0080, 0.8960, 0.0060, 0.9460, 0.0030, 0.9760, 0.0050, 0.9490, 0.9610, Acc_f: 0.0055, Acc_r: 0.9430\n",
      "[train] epoch 13, batch 35, loss 0.38226234912872314\n",
      "0.9280, 0.0080, 0.8790, 0.0060, 0.9540, 0.0080, 0.9660, 0.0040, 0.9510, 0.9720, Acc_f: 0.0065, Acc_r: 0.9417\n",
      "[train] epoch 14, batch 35, loss 0.41098397970199585\n",
      "0.9150, 0.0070, 0.8620, 0.0050, 0.9680, 0.0080, 0.9650, 0.0030, 0.9460, 0.9660, Acc_f: 0.0057, Acc_r: 0.9370\n",
      "[train] epoch 15, batch 35, loss 0.34193357825279236\n",
      "0.9220, 0.0060, 0.8970, 0.0060, 0.9540, 0.0070, 0.9560, 0.0040, 0.9570, 0.9680, Acc_f: 0.0057, Acc_r: 0.9423\n",
      "[train] epoch 16, batch 35, loss 0.3190864324569702\n",
      "0.9350, 0.0050, 0.8940, 0.0020, 0.9420, 0.0060, 0.9670, 0.0010, 0.9480, 0.9760, Acc_f: 0.0035, Acc_r: 0.9437\n",
      "[train] epoch 17, batch 35, loss 0.342714786529541\n",
      "0.9230, 0.0040, 0.8930, 0.0030, 0.9570, 0.0080, 0.9440, 0.0050, 0.9580, 0.9720, Acc_f: 0.0050, Acc_r: 0.9412\n",
      "[train] epoch 18, batch 35, loss 0.3888426721096039\n",
      "0.9340, 0.0070, 0.8990, 0.0040, 0.9470, 0.0070, 0.9510, 0.0030, 0.9470, 0.9790, Acc_f: 0.0052, Acc_r: 0.9428\n",
      "[train] epoch 19, batch 35, loss 0.41497233510017395\n",
      "0.9200, 0.0030, 0.8970, 0.0010, 0.9400, 0.0060, 0.9780, 0.0030, 0.9680, 0.9550, Acc_f: 0.0033, Acc_r: 0.9430\n",
      "[train] epoch 20, batch 35, loss 0.2894214689731598\n",
      "0.9260, 0.0080, 0.8900, 0.0040, 0.9690, 0.0040, 0.9460, 0.0010, 0.9550, 0.9520, Acc_f: 0.0043, Acc_r: 0.9397\n",
      "[train] epoch 21, batch 35, loss 0.3548663854598999\n",
      "0.9320, 0.0040, 0.8770, 0.0030, 0.9590, 0.0050, 0.9400, 0.0020, 0.9590, 0.9740, Acc_f: 0.0035, Acc_r: 0.9402\n",
      "[train] epoch 22, batch 35, loss 0.2762885093688965\n",
      "0.9310, 0.0030, 0.9000, 0.0030, 0.9500, 0.0050, 0.9490, 0.0030, 0.9610, 0.9650, Acc_f: 0.0035, Acc_r: 0.9427\n",
      "[train] epoch 23, batch 35, loss 0.327582448720932\n",
      "0.9310, 0.0010, 0.8960, 0.0010, 0.9500, 0.0010, 0.9240, 0.0000, 0.9520, 0.9750, Acc_f: 0.0008, Acc_r: 0.9380\n",
      "[train] epoch 24, batch 35, loss 0.2373741716146469\n",
      "0.9330, 0.0040, 0.9010, 0.0020, 0.9610, 0.0050, 0.9490, 0.0010, 0.9560, 0.9620, Acc_f: 0.0030, Acc_r: 0.9437\n",
      "0.9270, 0.9530, 0.8860, 0.7790, 0.9280, 0.8710, 0.9290, 0.9280, 0.9530, 0.9470, Acc_f: 0.8827, Acc_r: 0.9283\n",
      "[train] epoch 0, batch 35, loss 0.8884304761886597\n",
      "0.7670, 0.0810, 0.7370, 0.0440, 0.9230, 0.0090, 0.7930, 0.0610, 0.6820, 0.9400, Acc_f: 0.0488, Acc_r: 0.8070\n",
      "[train] epoch 1, batch 35, loss 0.7650300860404968\n",
      "0.8250, 0.0410, 0.6800, 0.0430, 0.9700, 0.0200, 0.8550, 0.0260, 0.8480, 0.9390, Acc_f: 0.0325, Acc_r: 0.8528\n",
      "[train] epoch 2, batch 35, loss 0.7275253534317017\n",
      "0.8390, 0.0780, 0.8680, 0.0580, 0.9260, 0.0330, 0.9060, 0.0400, 0.9080, 0.9620, Acc_f: 0.0523, Acc_r: 0.9015\n",
      "[train] epoch 3, batch 35, loss 0.5830979347229004\n",
      "0.9280, 0.0640, 0.8440, 0.0430, 0.9030, 0.0310, 0.9250, 0.0560, 0.9240, 0.9690, Acc_f: 0.0485, Acc_r: 0.9155\n",
      "[train] epoch 4, batch 35, loss 0.4202384054660797\n",
      "0.9010, 0.0240, 0.8400, 0.0310, 0.9460, 0.0190, 0.9300, 0.0270, 0.9320, 0.9610, Acc_f: 0.0252, Acc_r: 0.9183\n",
      "[train] epoch 5, batch 35, loss 0.582794725894928\n",
      "0.9150, 0.0190, 0.8670, 0.0270, 0.9070, 0.0120, 0.9300, 0.0270, 0.9270, 0.9680, Acc_f: 0.0212, Acc_r: 0.9190\n",
      "[train] epoch 6, batch 35, loss 0.6227972507476807\n",
      "0.8850, 0.0290, 0.9010, 0.0320, 0.9300, 0.0230, 0.9260, 0.0300, 0.9380, 0.9570, Acc_f: 0.0285, Acc_r: 0.9228\n",
      "[train] epoch 7, batch 35, loss 0.5223423838615417\n",
      "0.8950, 0.0200, 0.8910, 0.0250, 0.9380, 0.0180, 0.9390, 0.0200, 0.9350, 0.9610, Acc_f: 0.0208, Acc_r: 0.9265\n",
      "[train] epoch 8, batch 35, loss 0.7502084374427795\n",
      "0.9310, 0.0060, 0.8490, 0.0130, 0.9010, 0.0140, 0.9300, 0.0180, 0.9400, 0.9690, Acc_f: 0.0128, Acc_r: 0.9200\n",
      "[train] epoch 9, batch 35, loss 0.3110770881175995\n",
      "0.9090, 0.0190, 0.9060, 0.0250, 0.9270, 0.0190, 0.9320, 0.0190, 0.9450, 0.9580, Acc_f: 0.0205, Acc_r: 0.9295\n",
      "[train] epoch 10, batch 35, loss 0.5591931939125061\n",
      "0.9420, 0.0250, 0.8660, 0.0260, 0.9480, 0.0340, 0.9210, 0.0430, 0.9460, 0.9690, Acc_f: 0.0320, Acc_r: 0.9320\n",
      "[train] epoch 11, batch 35, loss 0.4251728355884552\n",
      "0.8940, 0.0090, 0.8680, 0.0190, 0.9590, 0.0300, 0.9390, 0.0270, 0.9510, 0.9690, Acc_f: 0.0212, Acc_r: 0.9300\n",
      "[train] epoch 12, batch 35, loss 0.5656486749649048\n",
      "0.9010, 0.0140, 0.8940, 0.0190, 0.9410, 0.0190, 0.9470, 0.0110, 0.9430, 0.9570, Acc_f: 0.0158, Acc_r: 0.9305\n",
      "[train] epoch 13, batch 35, loss 0.3524005115032196\n",
      "0.9250, 0.0080, 0.9020, 0.0140, 0.9280, 0.0130, 0.9390, 0.0120, 0.9580, 0.9660, Acc_f: 0.0118, Acc_r: 0.9363\n",
      "[train] epoch 14, batch 35, loss 0.4314556121826172\n",
      "0.8940, 0.0070, 0.8950, 0.0120, 0.9450, 0.0150, 0.9480, 0.0130, 0.9450, 0.9680, Acc_f: 0.0118, Acc_r: 0.9325\n",
      "[train] epoch 15, batch 35, loss 0.4531683623790741\n",
      "0.8840, 0.0150, 0.8570, 0.0100, 0.9720, 0.0210, 0.9500, 0.0050, 0.9390, 0.9550, Acc_f: 0.0127, Acc_r: 0.9262\n",
      "[train] epoch 16, batch 35, loss 0.48817527294158936\n",
      "0.9220, 0.0090, 0.8890, 0.0090, 0.9410, 0.0130, 0.9440, 0.0070, 0.9580, 0.9660, Acc_f: 0.0095, Acc_r: 0.9367\n",
      "[train] epoch 17, batch 35, loss 0.4373575747013092\n",
      "0.9200, 0.0080, 0.8970, 0.0050, 0.9330, 0.0090, 0.9560, 0.0080, 0.9540, 0.9690, Acc_f: 0.0075, Acc_r: 0.9382\n",
      "[train] epoch 18, batch 35, loss 0.24926148355007172\n",
      "0.8930, 0.0080, 0.9020, 0.0160, 0.9450, 0.0170, 0.9450, 0.0070, 0.9620, 0.9690, Acc_f: 0.0120, Acc_r: 0.9360\n",
      "[train] epoch 19, batch 35, loss 0.4522777795791626\n",
      "0.8930, 0.0110, 0.9170, 0.0070, 0.9210, 0.0070, 0.9580, 0.0020, 0.9520, 0.9520, Acc_f: 0.0067, Acc_r: 0.9322\n",
      "[train] epoch 20, batch 35, loss 0.2610137462615967\n",
      "0.9090, 0.0070, 0.8790, 0.0060, 0.9580, 0.0170, 0.9490, 0.0080, 0.9580, 0.9590, Acc_f: 0.0095, Acc_r: 0.9353\n",
      "[train] epoch 21, batch 35, loss 0.39865508675575256\n",
      "0.8940, 0.0050, 0.8870, 0.0050, 0.9460, 0.0150, 0.9490, 0.0080, 0.9580, 0.9660, Acc_f: 0.0083, Acc_r: 0.9333\n",
      "[train] epoch 22, batch 35, loss 0.26743432879447937\n",
      "0.9080, 0.0050, 0.8730, 0.0050, 0.9440, 0.0180, 0.9580, 0.0080, 0.9630, 0.9660, Acc_f: 0.0090, Acc_r: 0.9353\n",
      "[train] epoch 23, batch 35, loss 0.43107524514198303\n",
      "0.9350, 0.0070, 0.8940, 0.0050, 0.9170, 0.0100, 0.9420, 0.0070, 0.9530, 0.9710, Acc_f: 0.0072, Acc_r: 0.9353\n",
      "[train] epoch 24, batch 35, loss 0.4067722260951996\n",
      "0.9280, 0.0040, 0.9060, 0.0050, 0.9200, 0.0090, 0.9520, 0.0060, 0.9570, 0.9720, Acc_f: 0.0060, Acc_r: 0.9392\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.04)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(25):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader:\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mince Acc_f: 0.31 \\pm 0.23\n",
      "Mince Acc_r: 94.08 \\pm 0.21\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9395, 0.9437, 0.9392])\n",
    "Acc_f = 100*np.array([0.0003,  0.0030, 0.0060])\n",
    "\n",
    "print(f'Mince Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Mince Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460, 0.9610, 0.8810, 0.7980, 0.9060, 0.8590, 0.9580, 0.8990, 0.9420, 0.9070, Acc_f: 0.8796, Acc_r: 0.9318\n",
      "0.9250, 0.9540, 0.8760, 0.8210, 0.9110, 0.8740, 0.9270, 0.9390, 0.9370, 0.9420, Acc_f: 0.8928, Acc_r: 0.9284\n",
      "0.9270, 0.9530, 0.8860, 0.7790, 0.9280, 0.8710, 0.9290, 0.9280, 0.9530, 0.9470, Acc_f: 0.8834, Acc_r: 0.9368\n",
      "------------ Retrained model ------------\n",
      "0.9450, 0.0000, 0.0000, 0.0000, 0.9620, 0.0000, 0.9620, 0.0000, 0.9610, 0.9660, Acc_f: 0.0000, Acc_r: 0.9592\n",
      "0.9530, 0.0000, 0.0000, 0.0000, 0.9710, 0.0000, 0.9640, 0.0000, 0.9520, 0.9810, Acc_f: 0.0000, Acc_r: 0.9642\n",
      "0.9550, 0.0000, 0.0000, 0.0000, 0.9600, 0.0000, 0.9650, 0.0000, 0.9580, 0.9630, Acc_f: 0.0000, Acc_r: 0.9602\n",
      "Original model Acc_f: 88.53 \\pm 0.55\n",
      "Original model Acc_r: 93.23 \\pm 0.34\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 96.12 \\pm 0.22\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = dict[5]['arxiv_name']\n",
    "args.unlearn_class = dict[5]['unlearn_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_12-20-02-15_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n",
    "\n",
    "arxiv_name = 'original_model_12-20-02-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 48/432\n",
      "Layer 3 : 180/432\n",
      "Layer 4 : 552/864\n",
      "Layer 5 : 618/864\n",
      "Layer 6 : 617/864\n",
      "Layer 7 : 666/864\n",
      "Layer 8 : 76/96\n",
      "Layer 9 : 4/96\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 70/432\n",
      "Layer 3 : 240/432\n",
      "Layer 4 : 700/864\n",
      "Layer 5 : 733/864\n",
      "Layer 6 : 716/864\n",
      "Layer 7 : 779/864\n",
      "Layer 8 : 90/96\n",
      "Layer 9 : 8/96\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 107/432\n",
      "Layer 3 : 288/432\n",
      "Layer 4 : 758/864\n",
      "Layer 5 : 779/864\n",
      "Layer 6 : 761/864\n",
      "Layer 7 : 809/864\n",
      "Layer 8 : 93/96\n",
      "Layer 9 : 10/96\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 135/432\n",
      "Layer 3 : 321/432\n",
      "Layer 4 : 787/864\n",
      "Layer 5 : 805/864\n",
      "Layer 6 : 791/864\n",
      "Layer 7 : 830/864\n",
      "Layer 8 : 94/96\n",
      "Layer 9 : 12/96\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 14/27\n",
      "Layer 2 : 203/432\n",
      "Layer 3 : 370/432\n",
      "Layer 4 : 828/864\n",
      "Layer 5 : 835/864\n",
      "Layer 6 : 827/864\n",
      "Layer 7 : 849/864\n",
      "Layer 8 : 95/96\n",
      "Layer 9 : 15/96\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 47/432\n",
      "Layer 3 : 187/432\n",
      "Layer 4 : 558/864\n",
      "Layer 5 : 613/864\n",
      "Layer 6 : 608/864\n",
      "Layer 7 : 666/864\n",
      "Layer 8 : 77/96\n",
      "Layer 9 : 4/96\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 76/432\n",
      "Layer 3 : 245/432\n",
      "Layer 4 : 693/864\n",
      "Layer 5 : 738/864\n",
      "Layer 6 : 710/864\n",
      "Layer 7 : 784/864\n",
      "Layer 8 : 89/96\n",
      "Layer 9 : 8/96\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/27\n",
      "Layer 2 : 108/432\n",
      "Layer 3 : 294/432\n",
      "Layer 4 : 751/864\n",
      "Layer 5 : 783/864\n",
      "Layer 6 : 761/864\n",
      "Layer 7 : 816/864\n",
      "Layer 8 : 92/96\n",
      "Layer 9 : 10/96\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 133/432\n",
      "Layer 3 : 328/432\n",
      "Layer 4 : 785/864\n",
      "Layer 5 : 807/864\n",
      "Layer 6 : 790/864\n",
      "Layer 7 : 831/864\n",
      "Layer 8 : 94/96\n",
      "Layer 9 : 12/96\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 13/27\n",
      "Layer 2 : 204/432\n",
      "Layer 3 : 374/432\n",
      "Layer 4 : 826/864\n",
      "Layer 5 : 835/864\n",
      "Layer 6 : 825/864\n",
      "Layer 7 : 848/864\n",
      "Layer 8 : 95/96\n",
      "Layer 9 : 15/96\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 45/432\n",
      "Layer 3 : 181/432\n",
      "Layer 4 : 537/864\n",
      "Layer 5 : 610/864\n",
      "Layer 6 : 603/864\n",
      "Layer 7 : 663/864\n",
      "Layer 8 : 78/96\n",
      "Layer 9 : 5/96\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/27\n",
      "Layer 2 : 68/432\n",
      "Layer 3 : 243/432\n",
      "Layer 4 : 694/864\n",
      "Layer 5 : 730/864\n",
      "Layer 6 : 712/864\n",
      "Layer 7 : 777/864\n",
      "Layer 8 : 88/96\n",
      "Layer 9 : 9/96\n",
      "----------------------------------------\n",
      "Threshold:  0.988\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/27\n",
      "Layer 2 : 96/432\n",
      "Layer 3 : 286/432\n",
      "Layer 4 : 754/864\n",
      "Layer 5 : 773/864\n",
      "Layer 6 : 756/864\n",
      "Layer 7 : 808/864\n",
      "Layer 8 : 92/96\n",
      "Layer 9 : 11/96\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/27\n",
      "Layer 2 : 120/432\n",
      "Layer 3 : 321/432\n",
      "Layer 4 : 781/864\n",
      "Layer 5 : 801/864\n",
      "Layer 6 : 787/864\n",
      "Layer 7 : 830/864\n",
      "Layer 8 : 94/96\n",
      "Layer 9 : 13/96\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 14/27\n",
      "Layer 2 : 185/432\n",
      "Layer 3 : 368/432\n",
      "Layer 4 : 827/864\n",
      "Layer 5 : 833/864\n",
      "Layer 6 : 824/864\n",
      "Layer 7 : 849/864\n",
      "Layer 8 : 96/96\n",
      "Layer 9 : 16/96\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unlearn_indices = np.where(np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "\n",
    "# conver to the original indices\n",
    "unlearn_indices = train_loader.sampler.indices[unlearn_indices]\n",
    "\n",
    "unlearn_sampler = torch.utils.data.SubsetRandomSampler(unlearn_indices)\n",
    "unlearn_subset_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                    batch_size=args.batch_size, \n",
    "                                                    sampler=unlearn_sampler)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "remain_indices = np.where(~np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "remain_indices = train_loader.sampler.indices[remain_indices]\n",
    "\n",
    "remain_sampler = torch.utils.data.SubsetRandomSampler(remain_indices)\n",
    "remain_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            sampler=remain_sampler)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460, 0.9610, 0.8810, 0.7980, 0.9060, 0.8590, 0.9580, 0.8990, 0.9420, 0.9070, Acc_f: 0.8796, Acc_r: 0.9318\n",
      "[train] epoch 0, batch 43, loss 1.216407299041748\n",
      "0.8700, 0.2700, 0.2570, 0.0780, 0.9180, 0.0200, 0.9700, 0.0490, 0.9660, 0.9230, Acc_f: 0.1348, Acc_r: 0.9294\n",
      "[train] epoch 1, batch 43, loss 0.9499720931053162\n",
      "0.9100, 0.1340, 0.1460, 0.0580, 0.9240, 0.0210, 0.9750, 0.0210, 0.9620, 0.9420, Acc_f: 0.0760, Acc_r: 0.9426\n",
      "[train] epoch 2, batch 43, loss 0.6702789664268494\n",
      "0.9290, 0.0680, 0.1140, 0.0500, 0.9270, 0.0210, 0.9790, 0.0160, 0.9580, 0.9430, Acc_f: 0.0538, Acc_r: 0.9472\n",
      "[train] epoch 3, batch 43, loss 0.6160269379615784\n",
      "0.9430, 0.0490, 0.0950, 0.0400, 0.9310, 0.0210, 0.9790, 0.0130, 0.9500, 0.9430, Acc_f: 0.0436, Acc_r: 0.9492\n",
      "[train] epoch 4, batch 43, loss 0.56300950050354\n",
      "0.9500, 0.0360, 0.0800, 0.0320, 0.9220, 0.0180, 0.9810, 0.0130, 0.9500, 0.9480, Acc_f: 0.0358, Acc_r: 0.9502\n",
      "[train] epoch 5, batch 43, loss 0.4867788255214691\n",
      "0.9540, 0.0260, 0.0740, 0.0260, 0.9290, 0.0140, 0.9810, 0.0100, 0.9540, 0.9510, Acc_f: 0.0300, Acc_r: 0.9538\n",
      "[train] epoch 6, batch 43, loss 0.5018036961555481\n",
      "0.9540, 0.0180, 0.0610, 0.0210, 0.9290, 0.0110, 0.9820, 0.0080, 0.9550, 0.9510, Acc_f: 0.0238, Acc_r: 0.9542\n",
      "[train] epoch 7, batch 43, loss 0.36910316348075867\n",
      "0.9530, 0.0130, 0.0540, 0.0160, 0.9200, 0.0100, 0.9830, 0.0080, 0.9550, 0.9550, Acc_f: 0.0202, Acc_r: 0.9532\n",
      "[train] epoch 8, batch 43, loss 0.404628187417984\n",
      "0.9570, 0.0110, 0.0530, 0.0120, 0.9160, 0.0090, 0.9840, 0.0070, 0.9560, 0.9550, Acc_f: 0.0184, Acc_r: 0.9536\n",
      "[train] epoch 9, batch 43, loss 0.3834337890148163\n",
      "0.9610, 0.0080, 0.0370, 0.0090, 0.9260, 0.0050, 0.9830, 0.0030, 0.9520, 0.9560, Acc_f: 0.0124, Acc_r: 0.9556\n",
      "[train] epoch 10, batch 43, loss 0.3111693859100342\n",
      "0.9620, 0.0070, 0.0270, 0.0090, 0.9330, 0.0020, 0.9810, 0.0030, 0.9520, 0.9560, Acc_f: 0.0096, Acc_r: 0.9568\n",
      "[train] epoch 11, batch 43, loss 0.3014039993286133\n",
      "0.9660, 0.0040, 0.0200, 0.0070, 0.9380, 0.0020, 0.9800, 0.0020, 0.9500, 0.9550, Acc_f: 0.0070, Acc_r: 0.9578\n",
      "[train] epoch 12, batch 43, loss 0.3175337612628937\n",
      "0.9650, 0.0020, 0.0120, 0.0060, 0.9380, 0.0010, 0.9790, 0.0020, 0.9500, 0.9590, Acc_f: 0.0046, Acc_r: 0.9582\n",
      "[train] epoch 13, batch 43, loss 0.2978116273880005\n",
      "0.9650, 0.0000, 0.0090, 0.0050, 0.9370, 0.0010, 0.9800, 0.0020, 0.9480, 0.9580, Acc_f: 0.0034, Acc_r: 0.9576\n",
      "[train] epoch 14, batch 43, loss 0.3123125731945038\n",
      "0.9630, 0.0000, 0.0050, 0.0040, 0.9340, 0.0010, 0.9800, 0.0020, 0.9480, 0.9580, Acc_f: 0.0024, Acc_r: 0.9566\n",
      "[train] epoch 15, batch 43, loss 0.32319948077201843\n",
      "0.9680, 0.0000, 0.0040, 0.0040, 0.9290, 0.0010, 0.9800, 0.0020, 0.9470, 0.9580, Acc_f: 0.0022, Acc_r: 0.9564\n",
      "[train] epoch 16, batch 43, loss 0.2860731780529022\n",
      "0.9600, 0.0000, 0.0030, 0.0040, 0.9300, 0.0000, 0.9820, 0.0020, 0.9500, 0.9580, Acc_f: 0.0018, Acc_r: 0.9560\n",
      "[train] epoch 17, batch 43, loss 0.2754608690738678\n",
      "0.9650, 0.0000, 0.0020, 0.0040, 0.9340, 0.0000, 0.9800, 0.0020, 0.9470, 0.9580, Acc_f: 0.0016, Acc_r: 0.9568\n",
      "[train] epoch 18, batch 43, loss 0.2856983542442322\n",
      "0.9640, 0.0000, 0.0020, 0.0040, 0.9270, 0.0000, 0.9840, 0.0020, 0.9490, 0.9580, Acc_f: 0.0016, Acc_r: 0.9564\n",
      "[train] epoch 19, batch 43, loss 0.3127959668636322\n",
      "0.9670, 0.0000, 0.0020, 0.0040, 0.9280, 0.0000, 0.9840, 0.0020, 0.9470, 0.9570, Acc_f: 0.0016, Acc_r: 0.9566\n",
      "[train] epoch 20, batch 43, loss 0.30384156107902527\n",
      "0.9680, 0.0000, 0.0010, 0.0040, 0.9250, 0.0000, 0.9870, 0.0020, 0.9480, 0.9540, Acc_f: 0.0014, Acc_r: 0.9564\n",
      "[train] epoch 21, batch 43, loss 0.25477391481399536\n",
      "0.9660, 0.0000, 0.0020, 0.0040, 0.9300, 0.0000, 0.9830, 0.0020, 0.9480, 0.9570, Acc_f: 0.0016, Acc_r: 0.9568\n",
      "[train] epoch 22, batch 43, loss 0.2816023826599121\n",
      "0.9660, 0.0000, 0.0010, 0.0040, 0.9290, 0.0000, 0.9850, 0.0020, 0.9490, 0.9560, Acc_f: 0.0014, Acc_r: 0.9570\n",
      "[train] epoch 23, batch 43, loss 0.2961173355579376\n",
      "0.9630, 0.0000, 0.0010, 0.0040, 0.9510, 0.0000, 0.9790, 0.0010, 0.9480, 0.9590, Acc_f: 0.0012, Acc_r: 0.9600\n",
      "[train] epoch 24, batch 43, loss 0.30019500851631165\n",
      "0.9640, 0.0000, 0.0010, 0.0040, 0.9260, 0.0000, 0.9870, 0.0010, 0.9480, 0.9570, Acc_f: 0.0012, Acc_r: 0.9564\n",
      "0.9250, 0.9540, 0.8760, 0.8210, 0.9110, 0.8740, 0.9270, 0.9390, 0.9370, 0.9420, Acc_f: 0.8928, Acc_r: 0.9284\n",
      "[train] epoch 0, batch 43, loss 0.9985930919647217\n",
      "0.9050, 0.0360, 0.0350, 0.0140, 0.9240, 0.0060, 0.8560, 0.0130, 0.8380, 0.9410, Acc_f: 0.0208, Acc_r: 0.8928\n",
      "[train] epoch 1, batch 43, loss 0.8347622156143188\n",
      "0.9020, 0.0320, 0.0320, 0.0170, 0.9380, 0.0110, 0.9300, 0.0110, 0.8900, 0.9680, Acc_f: 0.0206, Acc_r: 0.9256\n",
      "[train] epoch 2, batch 43, loss 0.6286561489105225\n",
      "0.8980, 0.0220, 0.0430, 0.0200, 0.9630, 0.0240, 0.9350, 0.0180, 0.9150, 0.9760, Acc_f: 0.0254, Acc_r: 0.9374\n",
      "[train] epoch 3, batch 43, loss 0.6456166505813599\n",
      "0.9240, 0.0150, 0.0210, 0.0160, 0.9560, 0.0080, 0.9390, 0.0110, 0.9320, 0.9680, Acc_f: 0.0142, Acc_r: 0.9438\n",
      "[train] epoch 4, batch 43, loss 0.5767730474472046\n",
      "0.9280, 0.0140, 0.0290, 0.0200, 0.9530, 0.0190, 0.9580, 0.0140, 0.9480, 0.9700, Acc_f: 0.0192, Acc_r: 0.9514\n",
      "[train] epoch 5, batch 43, loss 0.5227478742599487\n",
      "0.9430, 0.0100, 0.0280, 0.0160, 0.9710, 0.0150, 0.9500, 0.0130, 0.9420, 0.9660, Acc_f: 0.0164, Acc_r: 0.9544\n",
      "[train] epoch 6, batch 43, loss 0.5536054968833923\n",
      "0.9410, 0.0080, 0.0350, 0.0200, 0.9590, 0.0230, 0.9710, 0.0140, 0.9530, 0.9640, Acc_f: 0.0200, Acc_r: 0.9576\n",
      "[train] epoch 7, batch 43, loss 0.43268901109695435\n",
      "0.9450, 0.0030, 0.0190, 0.0090, 0.9570, 0.0120, 0.9640, 0.0110, 0.9530, 0.9680, Acc_f: 0.0108, Acc_r: 0.9574\n",
      "[train] epoch 8, batch 43, loss 0.4911707043647766\n",
      "0.9400, 0.0040, 0.0280, 0.0130, 0.9700, 0.0180, 0.9730, 0.0100, 0.9460, 0.9680, Acc_f: 0.0146, Acc_r: 0.9594\n",
      "[train] epoch 9, batch 43, loss 0.39331960678100586\n",
      "0.9460, 0.0030, 0.0110, 0.0080, 0.9630, 0.0130, 0.9670, 0.0100, 0.9560, 0.9680, Acc_f: 0.0090, Acc_r: 0.9600\n",
      "[train] epoch 10, batch 43, loss 0.4067707657814026\n",
      "0.9530, 0.0030, 0.0110, 0.0060, 0.9550, 0.0100, 0.9630, 0.0090, 0.9500, 0.9750, Acc_f: 0.0078, Acc_r: 0.9592\n",
      "[train] epoch 11, batch 43, loss 0.36476442217826843\n",
      "0.9490, 0.0010, 0.0070, 0.0090, 0.9670, 0.0090, 0.9730, 0.0070, 0.9510, 0.9680, Acc_f: 0.0066, Acc_r: 0.9616\n",
      "[train] epoch 12, batch 43, loss 0.32370057702064514\n",
      "0.9540, 0.0020, 0.0080, 0.0050, 0.9640, 0.0100, 0.9630, 0.0090, 0.9490, 0.9740, Acc_f: 0.0068, Acc_r: 0.9608\n",
      "[train] epoch 13, batch 43, loss 0.34812021255493164\n",
      "0.9560, 0.0010, 0.0050, 0.0050, 0.9560, 0.0080, 0.9700, 0.0080, 0.9480, 0.9700, Acc_f: 0.0054, Acc_r: 0.9600\n",
      "[train] epoch 14, batch 43, loss 0.3253834843635559\n",
      "0.9570, 0.0010, 0.0060, 0.0040, 0.9650, 0.0090, 0.9710, 0.0070, 0.9510, 0.9720, Acc_f: 0.0054, Acc_r: 0.9632\n",
      "[train] epoch 15, batch 43, loss 0.3605573773384094\n",
      "0.9590, 0.0010, 0.0050, 0.0040, 0.9690, 0.0080, 0.9730, 0.0050, 0.9510, 0.9630, Acc_f: 0.0046, Acc_r: 0.9630\n",
      "[train] epoch 16, batch 43, loss 0.30316659808158875\n",
      "0.9590, 0.0020, 0.0040, 0.0040, 0.9600, 0.0080, 0.9680, 0.0070, 0.9510, 0.9650, Acc_f: 0.0050, Acc_r: 0.9606\n",
      "[train] epoch 17, batch 43, loss 0.3168072998523712\n",
      "0.9550, 0.0010, 0.0030, 0.0050, 0.9680, 0.0080, 0.9710, 0.0060, 0.9520, 0.9660, Acc_f: 0.0046, Acc_r: 0.9624\n",
      "[train] epoch 18, batch 43, loss 0.30586743354797363\n",
      "0.9580, 0.0000, 0.0010, 0.0040, 0.9660, 0.0080, 0.9640, 0.0050, 0.9470, 0.9700, Acc_f: 0.0036, Acc_r: 0.9610\n",
      "[train] epoch 19, batch 43, loss 0.31365180015563965\n",
      "0.9600, 0.0000, 0.0020, 0.0050, 0.9660, 0.0080, 0.9710, 0.0050, 0.9440, 0.9700, Acc_f: 0.0040, Acc_r: 0.9622\n",
      "[train] epoch 20, batch 43, loss 0.30100899934768677\n",
      "0.9590, 0.0000, 0.0000, 0.0050, 0.9710, 0.0080, 0.9700, 0.0020, 0.9460, 0.9670, Acc_f: 0.0030, Acc_r: 0.9626\n",
      "[train] epoch 21, batch 43, loss 0.28984710574150085\n",
      "0.9580, 0.0000, 0.0010, 0.0040, 0.9590, 0.0060, 0.9730, 0.0040, 0.9450, 0.9730, Acc_f: 0.0030, Acc_r: 0.9616\n",
      "[train] epoch 22, batch 43, loss 0.31343019008636475\n",
      "0.9630, 0.0000, 0.0000, 0.0030, 0.9620, 0.0070, 0.9650, 0.0020, 0.9480, 0.9680, Acc_f: 0.0024, Acc_r: 0.9612\n",
      "[train] epoch 23, batch 43, loss 0.31511032581329346\n",
      "0.9540, 0.0000, 0.0000, 0.0040, 0.9670, 0.0060, 0.9780, 0.0010, 0.9430, 0.9650, Acc_f: 0.0022, Acc_r: 0.9614\n",
      "[train] epoch 24, batch 43, loss 0.26849475502967834\n",
      "0.9580, 0.0000, 0.0000, 0.0020, 0.9600, 0.0050, 0.9700, 0.0010, 0.9490, 0.9710, Acc_f: 0.0016, Acc_r: 0.9616\n",
      "0.9270, 0.9530, 0.8860, 0.7790, 0.9280, 0.8710, 0.9290, 0.9280, 0.9530, 0.9470, Acc_f: 0.8834, Acc_r: 0.9368\n",
      "[train] epoch 0, batch 43, loss 1.0556325912475586\n",
      "0.9060, 0.1080, 0.0630, 0.0190, 0.9220, 0.0270, 0.7790, 0.0450, 0.7560, 0.9310, Acc_f: 0.0524, Acc_r: 0.8588\n",
      "[train] epoch 1, batch 43, loss 0.8459689021110535\n",
      "0.9280, 0.0730, 0.0410, 0.0220, 0.9210, 0.0370, 0.8590, 0.0430, 0.8550, 0.9510, Acc_f: 0.0432, Acc_r: 0.9028\n",
      "[train] epoch 2, batch 43, loss 0.6874141693115234\n",
      "0.9070, 0.0520, 0.0360, 0.0240, 0.9540, 0.0330, 0.8970, 0.0250, 0.8850, 0.9590, Acc_f: 0.0340, Acc_r: 0.9204\n",
      "[train] epoch 3, batch 43, loss 0.7708525061607361\n",
      "0.9140, 0.0370, 0.0280, 0.0200, 0.9580, 0.0320, 0.9060, 0.0180, 0.8950, 0.9640, Acc_f: 0.0270, Acc_r: 0.9274\n",
      "[train] epoch 4, batch 43, loss 0.6634972095489502\n",
      "0.9250, 0.0450, 0.0330, 0.0220, 0.9550, 0.0190, 0.9300, 0.0130, 0.9070, 0.9650, Acc_f: 0.0264, Acc_r: 0.9364\n",
      "[train] epoch 5, batch 43, loss 0.6404613852500916\n",
      "0.9370, 0.0360, 0.0300, 0.0230, 0.9610, 0.0270, 0.9250, 0.0120, 0.9160, 0.9650, Acc_f: 0.0256, Acc_r: 0.9408\n",
      "[train] epoch 6, batch 43, loss 0.6236792206764221\n",
      "0.9380, 0.0380, 0.0310, 0.0200, 0.9540, 0.0160, 0.9470, 0.0090, 0.9200, 0.9640, Acc_f: 0.0228, Acc_r: 0.9446\n",
      "[train] epoch 7, batch 43, loss 0.5727813839912415\n",
      "0.9360, 0.0220, 0.0270, 0.0180, 0.9630, 0.0170, 0.9430, 0.0090, 0.9320, 0.9670, Acc_f: 0.0186, Acc_r: 0.9482\n",
      "[train] epoch 8, batch 43, loss 0.5445970296859741\n",
      "0.9440, 0.0250, 0.0240, 0.0170, 0.9580, 0.0220, 0.9390, 0.0110, 0.9390, 0.9700, Acc_f: 0.0198, Acc_r: 0.9500\n",
      "[train] epoch 9, batch 43, loss 0.5158625245094299\n",
      "0.9370, 0.0190, 0.0210, 0.0160, 0.9750, 0.0150, 0.9290, 0.0090, 0.9480, 0.9680, Acc_f: 0.0160, Acc_r: 0.9514\n",
      "[train] epoch 10, batch 43, loss 0.4109309911727905\n",
      "0.9410, 0.0210, 0.0210, 0.0130, 0.9630, 0.0230, 0.9560, 0.0090, 0.9500, 0.9640, Acc_f: 0.0174, Acc_r: 0.9548\n",
      "[train] epoch 11, batch 43, loss 0.5001550912857056\n",
      "0.9470, 0.0180, 0.0190, 0.0100, 0.9620, 0.0190, 0.9520, 0.0090, 0.9500, 0.9670, Acc_f: 0.0150, Acc_r: 0.9556\n",
      "[train] epoch 12, batch 43, loss 0.43770331144332886\n",
      "0.9450, 0.0160, 0.0140, 0.0090, 0.9660, 0.0160, 0.9480, 0.0080, 0.9530, 0.9650, Acc_f: 0.0126, Acc_r: 0.9554\n",
      "[train] epoch 13, batch 43, loss 0.41924935579299927\n",
      "0.9440, 0.0210, 0.0230, 0.0140, 0.9720, 0.0230, 0.9520, 0.0110, 0.9560, 0.9690, Acc_f: 0.0184, Acc_r: 0.9586\n",
      "[train] epoch 14, batch 43, loss 0.4183279275894165\n",
      "0.9400, 0.0160, 0.0160, 0.0090, 0.9680, 0.0180, 0.9560, 0.0080, 0.9560, 0.9680, Acc_f: 0.0134, Acc_r: 0.9576\n",
      "[train] epoch 15, batch 43, loss 0.3965085446834564\n",
      "0.9360, 0.0170, 0.0150, 0.0090, 0.9740, 0.0180, 0.9560, 0.0060, 0.9620, 0.9700, Acc_f: 0.0130, Acc_r: 0.9596\n",
      "[train] epoch 16, batch 43, loss 0.42923155426979065\n",
      "0.9410, 0.0190, 0.0160, 0.0090, 0.9740, 0.0210, 0.9470, 0.0070, 0.9630, 0.9700, Acc_f: 0.0144, Acc_r: 0.9590\n",
      "[train] epoch 17, batch 43, loss 0.36094868183135986\n",
      "0.9410, 0.0180, 0.0160, 0.0090, 0.9710, 0.0190, 0.9540, 0.0100, 0.9610, 0.9690, Acc_f: 0.0144, Acc_r: 0.9592\n",
      "[train] epoch 18, batch 43, loss 0.40698400139808655\n",
      "0.9430, 0.0200, 0.0150, 0.0070, 0.9700, 0.0150, 0.9560, 0.0070, 0.9600, 0.9680, Acc_f: 0.0128, Acc_r: 0.9594\n",
      "[train] epoch 19, batch 43, loss 0.38130226731300354\n",
      "0.9440, 0.0190, 0.0130, 0.0080, 0.9740, 0.0220, 0.9600, 0.0070, 0.9600, 0.9690, Acc_f: 0.0138, Acc_r: 0.9614\n",
      "[train] epoch 20, batch 43, loss 0.39535394310951233\n",
      "0.9500, 0.0190, 0.0130, 0.0080, 0.9770, 0.0210, 0.9550, 0.0100, 0.9570, 0.9690, Acc_f: 0.0142, Acc_r: 0.9616\n",
      "[train] epoch 21, batch 43, loss 0.36070719361305237\n",
      "0.9390, 0.0170, 0.0120, 0.0070, 0.9740, 0.0150, 0.9580, 0.0050, 0.9650, 0.9680, Acc_f: 0.0112, Acc_r: 0.9608\n",
      "[train] epoch 22, batch 43, loss 0.3760964572429657\n",
      "0.9340, 0.0180, 0.0120, 0.0080, 0.9780, 0.0180, 0.9600, 0.0040, 0.9650, 0.9700, Acc_f: 0.0120, Acc_r: 0.9614\n",
      "[train] epoch 23, batch 43, loss 0.30652350187301636\n",
      "0.9470, 0.0190, 0.0130, 0.0070, 0.9720, 0.0210, 0.9580, 0.0070, 0.9630, 0.9670, Acc_f: 0.0134, Acc_r: 0.9614\n",
      "[train] epoch 24, batch 43, loss 0.297757625579834\n",
      "0.9450, 0.0200, 0.0100, 0.0070, 0.9680, 0.0180, 0.9640, 0.0070, 0.9630, 0.9660, Acc_f: 0.0124, Acc_r: 0.9612\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.04)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(25):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader:\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mince Acc_f: 0.68 \\pm 0.52\n",
      "Mince Acc_r: 96.15 \\pm 0.13\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9600, 0.9632, 0.9614])\n",
    "Acc_f = 100*np.array([0.0012, 0.0054, 0.0138])\n",
    "\n",
    "print(f'Mince Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Mince Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
