{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils\n",
    "from agents import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seeds',          type=int,       default=[2023, 2024, 2025])\n",
    "parser.add_argument('--dataset',        type=str,       default='fmnist')\n",
    "parser.add_argument('--batch_size',     type=int,       default=512)\n",
    "parser.add_argument('--model_name',     type=str,       default='alexnet')\n",
    "parser.add_argument('--retrain',        type=bool,      default=False)\n",
    "parser.add_argument('--unlearn_class',  type=list,      default=3)\n",
    "args = parser.parse_args(\"\")\n",
    "args.time_str = time.strftime(\"%m-%d-%H-%M\", time.localtime())\n",
    "if args.dataset.lower() == 'fmnist':\n",
    "    args.n_channels = 1\n",
    "else:\n",
    "    args.n_channels = 3\n",
    "\n",
    "if args.dataset.lower() == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "else:\n",
    "    args.num_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    1: {'unlean_class': [6], 'arxiv_name': '12-27-21-31'},\n",
    "    2: {'unlean_class': [0, 6], 'arxiv_name': '12-28-20-44'}, \n",
    "    3: {'unlean_class': [0, 4, 6], 'arxiv_name': '12-28-22-00'},\n",
    "    4: {'unlean_class': [0, 1, 4, 6], 'arxiv_name': '12-28-22-51'},\n",
    "    5: {'unlean_class': [0, 1, 4, 6, 9], 'arxiv_name': '12-28-23-25'},\n",
    "}\n",
    "\n",
    "num_unlearn = 1\n",
    "args.unlean_class = dict[num_unlearn]['unlean_class']\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "\n",
    "arxiv_name_o = '12-27-20-22'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 1 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "------------ Retrained model ------------\n",
      "0.9080, 0.9820, 0.9320, 0.9510, 0.9010, 0.9560, [0.0000], 0.9830, 0.9830, 0.9690, Acc_f: 0.0000, Acc_r: 0.9517\n",
      "0.9230, 0.9800, 0.9420, 0.9410, 0.8960, 0.9360, [0.0000], 0.9770, 0.9860, 0.9650, Acc_f: 0.0000, Acc_r: 0.9496\n",
      "0.9330, 0.9810, 0.9450, 0.9430, 0.8950, 0.9570, [0.0000], 0.9730, 0.9930, 0.9700, Acc_f: 0.0000, Acc_r: 0.9544\n",
      "Original model Acc_f: 80.97 \\pm 1.44\n",
      "Original model Acc_r: 93.58 \\pm 0.28\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 95.19 \\pm 0.20\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 1\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 3/16\n",
      "Layer 2 : 101/576\n",
      "Layer 3 : 284/512\n",
      "Layer 4 : 91/1024\n",
      "Layer 5 : 86/2048\n",
      "Layer 6 : 6/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.973\n",
      "Skip Updating GPM for layer: 2\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/16\n",
      "Layer 2 : 101/576\n",
      "Layer 3 : 343/512\n",
      "Layer 4 : 162/1024\n",
      "Layer 5 : 129/2048\n",
      "Layer 6 : 7/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 139/576\n",
      "Layer 3 : 394/512\n",
      "Layer 4 : 252/1024\n",
      "Layer 5 : 211/2048\n",
      "Layer 6 : 11/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 146/576\n",
      "Layer 3 : 424/512\n",
      "Layer 4 : 341/1024\n",
      "Layer 5 : 299/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/16\n",
      "Layer 2 : 164/576\n",
      "Layer 3 : 432/512\n",
      "Layer 4 : 429/1024\n",
      "Layer 5 : 391/2048\n",
      "Layer 6 : 18/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 228/576\n",
      "Layer 3 : 468/512\n",
      "Layer 4 : 521/1024\n",
      "Layer 5 : 466/2048\n",
      "Layer 6 : 21/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 262/576\n",
      "Layer 3 : 480/512\n",
      "Layer 4 : 613/1024\n",
      "Layer 5 : 534/2048\n",
      "Layer 6 : 23/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 321/576\n",
      "Layer 3 : 489/512\n",
      "Layer 4 : 710/1024\n",
      "Layer 5 : 633/2048\n",
      "Layer 6 : 38/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 365/576\n",
      "Layer 3 : 499/512\n",
      "Layer 4 : 802/1024\n",
      "Layer 5 : 731/2048\n",
      "Layer 6 : 57/2048\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 3/16\n",
      "Layer 2 : 111/576\n",
      "Layer 3 : 299/512\n",
      "Layer 4 : 92/1024\n",
      "Layer 5 : 85/2048\n",
      "Layer 6 : 5/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.973\n",
      "Skip Updating GPM for layer: 2\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/16\n",
      "Layer 2 : 111/576\n",
      "Layer 3 : 349/512\n",
      "Layer 4 : 164/1024\n",
      "Layer 5 : 123/2048\n",
      "Layer 6 : 7/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 141/576\n",
      "Layer 3 : 399/512\n",
      "Layer 4 : 255/1024\n",
      "Layer 5 : 204/2048\n",
      "Layer 6 : 10/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 149/576\n",
      "Layer 3 : 429/512\n",
      "Layer 4 : 348/1024\n",
      "Layer 5 : 295/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/16\n",
      "Layer 2 : 171/576\n",
      "Layer 3 : 437/512\n",
      "Layer 4 : 437/1024\n",
      "Layer 5 : 386/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 229/576\n",
      "Layer 3 : 471/512\n",
      "Layer 4 : 534/1024\n",
      "Layer 5 : 463/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 256/576\n",
      "Layer 3 : 484/512\n",
      "Layer 4 : 626/1024\n",
      "Layer 5 : 530/2048\n",
      "Layer 6 : 21/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 322/576\n",
      "Layer 3 : 491/512\n",
      "Layer 4 : 722/1024\n",
      "Layer 5 : 628/2048\n",
      "Layer 6 : 38/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 369/576\n",
      "Layer 3 : 500/512\n",
      "Layer 4 : 816/1024\n",
      "Layer 5 : 727/2048\n",
      "Layer 6 : 58/2048\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.97\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 103/576\n",
      "Layer 3 : 256/512\n",
      "Layer 4 : 81/1024\n",
      "Layer 5 : 68/2048\n",
      "Layer 6 : 8/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.973\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 2\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 103/576\n",
      "Layer 3 : 304/512\n",
      "Layer 4 : 136/1024\n",
      "Layer 5 : 90/2048\n",
      "Layer 6 : 9/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 131/576\n",
      "Layer 3 : 353/512\n",
      "Layer 4 : 215/1024\n",
      "Layer 5 : 155/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "Skip Updating GPM for layer: 2\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 131/576\n",
      "Layer 3 : 387/512\n",
      "Layer 4 : 296/1024\n",
      "Layer 5 : 229/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/16\n",
      "Layer 2 : 148/576\n",
      "Layer 3 : 395/512\n",
      "Layer 4 : 373/1024\n",
      "Layer 5 : 301/2048\n",
      "Layer 6 : 23/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 212/576\n",
      "Layer 3 : 451/512\n",
      "Layer 4 : 463/1024\n",
      "Layer 5 : 365/2048\n",
      "Layer 6 : 26/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 247/576\n",
      "Layer 3 : 468/512\n",
      "Layer 4 : 549/1024\n",
      "Layer 5 : 419/2048\n",
      "Layer 6 : 28/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 305/576\n",
      "Layer 3 : 478/512\n",
      "Layer 4 : 641/1024\n",
      "Layer 5 : 508/2048\n",
      "Layer 6 : 44/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 352/576\n",
      "Layer 3 : 492/512\n",
      "Layer 4 : 731/1024\n",
      "Layer 5 : 594/2048\n",
      "Layer 6 : 62/2048\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.targets)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 125, 125, 250, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "[train] epoch 0, batch 10, loss 5.916708946228027\n",
      "0.8740, 0.9800, 0.9400, 0.9530, 0.8950, 0.9570, [0.6140], 0.9850, 0.9890, 0.9590, Acc_f: 0.6140, Acc_r: 0.9480\n",
      "[train] epoch 1, batch 10, loss 3.784639596939087\n",
      "0.8830, 0.9830, 0.9400, 0.9590, 0.8970, 0.9540, [0.2740], 0.9850, 0.9900, 0.9590, Acc_f: 0.2740, Acc_r: 0.9500\n",
      "[train] epoch 2, batch 10, loss 3.376570224761963\n",
      "0.8860, 0.9830, 0.9430, 0.9640, 0.8900, 0.9530, [0.1690], 0.9850, 0.9880, 0.9590, Acc_f: 0.1690, Acc_r: 0.9501\n",
      "[train] epoch 3, batch 10, loss 3.2416114807128906\n",
      "0.8920, 0.9830, 0.9420, 0.9620, 0.8840, 0.9530, [0.1130], 0.9850, 0.9870, 0.9590, Acc_f: 0.1130, Acc_r: 0.9497\n",
      "[train] epoch 4, batch 10, loss 3.068338632583618\n",
      "0.8940, 0.9830, 0.9410, 0.9580, 0.8870, 0.9530, [0.0830], 0.9860, 0.9870, 0.9620, Acc_f: 0.0830, Acc_r: 0.9501\n",
      "[train] epoch 5, batch 10, loss 2.955772876739502\n",
      "0.8960, 0.9850, 0.9370, 0.9560, 0.8850, 0.9520, [0.0650], 0.9860, 0.9850, 0.9620, Acc_f: 0.0650, Acc_r: 0.9493\n",
      "[train] epoch 6, batch 10, loss 2.9025673866271973\n",
      "0.8960, 0.9850, 0.9350, 0.9550, 0.8840, 0.9520, [0.0420], 0.9870, 0.9850, 0.9630, Acc_f: 0.0420, Acc_r: 0.9491\n",
      "[train] epoch 7, batch 10, loss 2.743762969970703\n",
      "0.8980, 0.9860, 0.9340, 0.9530, 0.8830, 0.9520, [0.0280], 0.9840, 0.9850, 0.9640, Acc_f: 0.0280, Acc_r: 0.9488\n",
      "[train] epoch 8, batch 10, loss 2.759169816970825\n",
      "0.8960, 0.9860, 0.9330, 0.9530, 0.8840, 0.9510, [0.0190], 0.9850, 0.9840, 0.9640, Acc_f: 0.0190, Acc_r: 0.9484\n",
      "[train] epoch 9, batch 10, loss 2.6250216960906982\n",
      "0.8940, 0.9860, 0.9320, 0.9520, 0.8880, 0.9500, [0.0160], 0.9850, 0.9830, 0.9640, Acc_f: 0.0160, Acc_r: 0.9482\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "[train] epoch 0, batch 10, loss 3.5961835384368896\n",
      "0.7200, 0.9870, 0.8600, 0.9110, 0.7740, 0.9510, [0.0240], 0.9920, 0.9890, 0.9560, Acc_f: 0.0240, Acc_r: 0.9044\n",
      "[train] epoch 1, batch 10, loss 2.710357427597046\n",
      "0.8320, 0.9870, 0.9080, 0.9280, 0.8080, 0.9540, [0.0090], 0.9860, 0.9890, 0.9680, Acc_f: 0.0090, Acc_r: 0.9289\n",
      "[train] epoch 2, batch 10, loss 2.6623172760009766\n",
      "0.8430, 0.9870, 0.8820, 0.9290, 0.8150, 0.9540, [0.0010], 0.9860, 0.9880, 0.9680, Acc_f: 0.0010, Acc_r: 0.9280\n",
      "[train] epoch 3, batch 10, loss 2.40962815284729\n",
      "0.8340, 0.9880, 0.8750, 0.9210, 0.8130, 0.9450, [0.0010], 0.9870, 0.9850, 0.9690, Acc_f: 0.0010, Acc_r: 0.9241\n",
      "[train] epoch 4, batch 10, loss 2.3571605682373047\n",
      "0.8310, 0.9880, 0.8820, 0.9240, 0.8090, 0.9420, [0.0010], 0.9870, 0.9840, 0.9680, Acc_f: 0.0010, Acc_r: 0.9239\n",
      "[train] epoch 5, batch 10, loss 2.371476650238037\n",
      "0.8000, 0.9890, 0.8810, 0.9210, 0.8080, 0.9420, [0.0010], 0.9870, 0.9840, 0.9700, Acc_f: 0.0010, Acc_r: 0.9202\n",
      "[train] epoch 6, batch 10, loss 2.365039348602295\n",
      "0.8090, 0.9890, 0.8440, 0.9270, 0.7990, 0.9400, [0.0000], 0.9870, 0.9850, 0.9670, Acc_f: 0.0000, Acc_r: 0.9163\n",
      "[train] epoch 7, batch 10, loss 2.2569162845611572\n",
      "0.7950, 0.9870, 0.8460, 0.9240, 0.8030, 0.9370, [0.0000], 0.9870, 0.9780, 0.9670, Acc_f: 0.0000, Acc_r: 0.9138\n",
      "[train] epoch 8, batch 10, loss 2.2564401626586914\n",
      "0.7730, 0.9850, 0.8300, 0.9370, 0.8490, 0.9370, [0.0000], 0.9870, 0.9790, 0.9670, Acc_f: 0.0000, Acc_r: 0.9160\n",
      "[train] epoch 9, batch 10, loss 2.22295880317688\n",
      "0.7680, 0.9870, 0.8030, 0.9270, 0.8390, 0.9350, [0.0000], 0.9860, 0.9800, 0.9680, Acc_f: 0.0000, Acc_r: 0.9103\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "[train] epoch 0, batch 10, loss 4.309385299682617\n",
      "0.8760, 0.9800, 0.8970, 0.9620, 0.8870, 0.9460, [0.2830], 0.9790, 0.9920, 0.9670, Acc_f: 0.2830, Acc_r: 0.9429\n",
      "[train] epoch 1, batch 10, loss 2.829252004623413\n",
      "0.8350, 0.9810, 0.8380, 0.9540, 0.8560, 0.9430, [0.0670], 0.9790, 0.9910, 0.9680, Acc_f: 0.0670, Acc_r: 0.9272\n",
      "[train] epoch 2, batch 10, loss 2.69862961769104\n",
      "0.8220, 0.9820, 0.8190, 0.9490, 0.8460, 0.9420, [0.0220], 0.9790, 0.9890, 0.9680, Acc_f: 0.0220, Acc_r: 0.9218\n",
      "[train] epoch 3, batch 10, loss 2.6149659156799316\n",
      "0.8140, 0.9830, 0.8060, 0.9430, 0.8500, 0.9420, [0.0110], 0.9790, 0.9890, 0.9680, Acc_f: 0.0110, Acc_r: 0.9193\n",
      "[train] epoch 4, batch 10, loss 2.5326316356658936\n",
      "0.8070, 0.9830, 0.7960, 0.9310, 0.8440, 0.9400, [0.0050], 0.9790, 0.9880, 0.9680, Acc_f: 0.0050, Acc_r: 0.9151\n",
      "[train] epoch 5, batch 10, loss 2.4632749557495117\n",
      "0.8050, 0.9850, 0.7770, 0.9330, 0.8470, 0.9370, [0.0050], 0.9790, 0.9870, 0.9680, Acc_f: 0.0050, Acc_r: 0.9131\n",
      "[train] epoch 6, batch 10, loss 2.4004480838775635\n",
      "0.8140, 0.9830, 0.7640, 0.9300, 0.8480, 0.9330, [0.0020], 0.9770, 0.9850, 0.9680, Acc_f: 0.0020, Acc_r: 0.9113\n",
      "[train] epoch 7, batch 10, loss 2.435741901397705\n",
      "0.8150, 0.9830, 0.7650, 0.9220, 0.8580, 0.9330, [0.0020], 0.9770, 0.9850, 0.9680, Acc_f: 0.0020, Acc_r: 0.9118\n",
      "[train] epoch 8, batch 10, loss 2.3415465354919434\n",
      "0.8150, 0.9830, 0.7680, 0.9210, 0.8710, 0.9320, [0.0020], 0.9770, 0.9850, 0.9670, Acc_f: 0.0020, Acc_r: 0.9132\n",
      "[train] epoch 9, batch 10, loss 2.346872568130493\n",
      "0.8070, 0.9830, 0.7450, 0.9170, 0.8580, 0.9280, [0.0010], 0.9770, 0.9850, 0.9670, Acc_f: 0.0010, Acc_r: 0.9074\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label + Subspace Acc_f: 1.57 \\pm 0.53\n",
      "Random label + Subspace Acc_r: 93.30 \\pm 1.12\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9482, 0.9289, 0.9218])\n",
    "Acc_f = 100*np.array([0.0160, 0.0090, 0.0220])\n",
    "\n",
    "print(f'Random label + Subspace Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label + Subspace Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "[train] epoch 0, batch 10, loss 2.2840707302093506\n",
      "0.8740, 0.9780, 0.9460, 0.9370, 0.9090, 0.9560, [0.4840], 0.9850, 0.9890, 0.9590, Acc_f: 0.4840, Acc_r: 0.9481\n",
      "[train] epoch 1, batch 10, loss 1.2306489944458008\n",
      "0.8870, 0.9790, 0.9550, 0.9400, 0.9070, 0.9520, [0.2470], 0.9850, 0.9890, 0.9590, Acc_f: 0.2470, Acc_r: 0.9503\n",
      "[train] epoch 2, batch 10, loss 0.9398791193962097\n",
      "0.8940, 0.9790, 0.9560, 0.9450, 0.8980, 0.9500, [0.1740], 0.9850, 0.9890, 0.9590, Acc_f: 0.1740, Acc_r: 0.9506\n",
      "[train] epoch 3, batch 10, loss 0.7715941071510315\n",
      "0.9060, 0.9790, 0.9560, 0.9450, 0.8930, 0.9500, [0.1340], 0.9860, 0.9890, 0.9590, Acc_f: 0.1340, Acc_r: 0.9514\n",
      "[train] epoch 4, batch 10, loss 0.5707158446311951\n",
      "0.9130, 0.9800, 0.9550, 0.9450, 0.8900, 0.9490, [0.0970], 0.9860, 0.9890, 0.9590, Acc_f: 0.0970, Acc_r: 0.9518\n",
      "[train] epoch 5, batch 10, loss 0.5738160014152527\n",
      "0.9160, 0.9800, 0.9530, 0.9460, 0.8900, 0.9490, [0.0620], 0.9860, 0.9890, 0.9600, Acc_f: 0.0620, Acc_r: 0.9521\n",
      "[train] epoch 6, batch 10, loss 0.46568563580513\n",
      "0.9200, 0.9800, 0.9530, 0.9470, 0.8910, 0.9490, [0.0490], 0.9860, 0.9890, 0.9600, Acc_f: 0.0490, Acc_r: 0.9528\n",
      "[train] epoch 7, batch 10, loss 0.4344524145126343\n",
      "0.9210, 0.9800, 0.9530, 0.9490, 0.8890, 0.9490, [0.0390], 0.9860, 0.9890, 0.9600, Acc_f: 0.0390, Acc_r: 0.9529\n",
      "[train] epoch 8, batch 10, loss 0.4402673542499542\n",
      "0.9230, 0.9790, 0.9530, 0.9490, 0.8890, 0.9480, [0.0310], 0.9860, 0.9890, 0.9600, Acc_f: 0.0310, Acc_r: 0.9529\n",
      "[train] epoch 9, batch 10, loss 0.39949509501457214\n",
      "0.9230, 0.9790, 0.9530, 0.9490, 0.8900, 0.9480, [0.0240], 0.9860, 0.9890, 0.9600, Acc_f: 0.0240, Acc_r: 0.9530\n",
      "[train] epoch 10, batch 10, loss 0.3726453185081482\n",
      "0.9240, 0.9790, 0.9520, 0.9490, 0.8910, 0.9480, [0.0200], 0.9860, 0.9890, 0.9600, Acc_f: 0.0200, Acc_r: 0.9531\n",
      "[train] epoch 11, batch 10, loss 0.34664031863212585\n",
      "0.9250, 0.9790, 0.9500, 0.9470, 0.8910, 0.9480, [0.0150], 0.9860, 0.9890, 0.9600, Acc_f: 0.0150, Acc_r: 0.9528\n",
      "[train] epoch 12, batch 10, loss 0.3712780773639679\n",
      "0.9270, 0.9780, 0.9510, 0.9470, 0.8910, 0.9480, [0.0110], 0.9850, 0.9890, 0.9600, Acc_f: 0.0110, Acc_r: 0.9529\n",
      "[train] epoch 13, batch 10, loss 0.33086955547332764\n",
      "0.9290, 0.9780, 0.9510, 0.9470, 0.8920, 0.9480, [0.0100], 0.9850, 0.9890, 0.9600, Acc_f: 0.0100, Acc_r: 0.9532\n",
      "[train] epoch 14, batch 10, loss 0.303688108921051\n",
      "0.9300, 0.9780, 0.9510, 0.9470, 0.8910, 0.9470, [0.0090], 0.9850, 0.9890, 0.9600, Acc_f: 0.0090, Acc_r: 0.9531\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "[train] epoch 0, batch 10, loss 0.9280670881271362\n",
      "0.8980, 0.9820, 0.9590, 0.9060, 0.8900, 0.9300, [0.0430], 0.9920, 0.9720, 0.9540, Acc_f: 0.0430, Acc_r: 0.9426\n",
      "[train] epoch 1, batch 10, loss 0.3911418616771698\n",
      "0.9260, 0.9810, 0.9420, 0.9250, 0.9220, 0.9320, [0.0180], 0.9920, 0.9710, 0.9550, Acc_f: 0.0180, Acc_r: 0.9496\n",
      "[train] epoch 2, batch 10, loss 0.4303034842014313\n",
      "0.9250, 0.9810, 0.9440, 0.9300, 0.9150, 0.9340, [0.0130], 0.9920, 0.9780, 0.9550, Acc_f: 0.0130, Acc_r: 0.9504\n",
      "[train] epoch 3, batch 10, loss 0.35234561562538147\n",
      "0.9240, 0.9810, 0.9410, 0.9320, 0.9180, 0.9350, [0.0110], 0.9920, 0.9810, 0.9550, Acc_f: 0.0110, Acc_r: 0.9510\n",
      "[train] epoch 4, batch 10, loss 0.3324027359485626\n",
      "0.9230, 0.9810, 0.9390, 0.9330, 0.9200, 0.9370, [0.0080], 0.9920, 0.9830, 0.9560, Acc_f: 0.0080, Acc_r: 0.9516\n",
      "[train] epoch 5, batch 10, loss 0.35158658027648926\n",
      "0.9230, 0.9810, 0.9400, 0.9350, 0.9210, 0.9380, [0.0070], 0.9920, 0.9830, 0.9570, Acc_f: 0.0070, Acc_r: 0.9522\n",
      "[train] epoch 6, batch 10, loss 0.3376046419143677\n",
      "0.9230, 0.9800, 0.9380, 0.9360, 0.9220, 0.9430, [0.0070], 0.9920, 0.9830, 0.9560, Acc_f: 0.0070, Acc_r: 0.9526\n",
      "[train] epoch 7, batch 10, loss 0.23726174235343933\n",
      "0.9250, 0.9800, 0.9390, 0.9350, 0.9220, 0.9430, [0.0060], 0.9910, 0.9850, 0.9570, Acc_f: 0.0060, Acc_r: 0.9530\n",
      "[train] epoch 8, batch 10, loss 0.27640408277511597\n",
      "0.9230, 0.9800, 0.9400, 0.9390, 0.9220, 0.9430, [0.0070], 0.9910, 0.9850, 0.9580, Acc_f: 0.0070, Acc_r: 0.9534\n",
      "[train] epoch 9, batch 10, loss 0.24315090477466583\n",
      "0.9240, 0.9800, 0.9400, 0.9380, 0.9230, 0.9440, [0.0070], 0.9910, 0.9870, 0.9590, Acc_f: 0.0070, Acc_r: 0.9540\n",
      "[train] epoch 10, batch 10, loss 0.23786485195159912\n",
      "0.9240, 0.9800, 0.9380, 0.9390, 0.9250, 0.9460, [0.0070], 0.9910, 0.9870, 0.9590, Acc_f: 0.0070, Acc_r: 0.9543\n",
      "[train] epoch 11, batch 10, loss 0.22861962020397186\n",
      "0.9230, 0.9800, 0.9400, 0.9390, 0.9240, 0.9470, [0.0060], 0.9910, 0.9870, 0.9590, Acc_f: 0.0060, Acc_r: 0.9544\n",
      "[train] epoch 12, batch 10, loss 0.24040678143501282\n",
      "0.9230, 0.9800, 0.9410, 0.9380, 0.9230, 0.9480, [0.0050], 0.9910, 0.9870, 0.9620, Acc_f: 0.0050, Acc_r: 0.9548\n",
      "[train] epoch 13, batch 10, loss 0.2054585963487625\n",
      "0.9240, 0.9800, 0.9400, 0.9420, 0.9210, 0.9490, [0.0060], 0.9910, 0.9870, 0.9620, Acc_f: 0.0060, Acc_r: 0.9551\n",
      "[train] epoch 14, batch 10, loss 0.21463097631931305\n",
      "0.9230, 0.9800, 0.9420, 0.9410, 0.9200, 0.9500, [0.0060], 0.9900, 0.9870, 0.9630, Acc_f: 0.0060, Acc_r: 0.9551\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "[train] epoch 0, batch 10, loss 0.9582839012145996\n",
      "0.9190, 0.9750, 0.9240, 0.9310, 0.9280, 0.9510, [0.2270], 0.9840, 0.9790, 0.9600, Acc_f: 0.2270, Acc_r: 0.9501\n",
      "[train] epoch 1, batch 10, loss 0.45996490120887756\n",
      "0.9370, 0.9750, 0.9250, 0.9190, 0.9220, 0.9500, [0.0660], 0.9850, 0.9760, 0.9510, Acc_f: 0.0660, Acc_r: 0.9489\n",
      "[train] epoch 2, batch 10, loss 0.3667939305305481\n",
      "0.9420, 0.9740, 0.9240, 0.9140, 0.9200, 0.9510, [0.0370], 0.9850, 0.9760, 0.9510, Acc_f: 0.0370, Acc_r: 0.9486\n",
      "[train] epoch 3, batch 10, loss 0.37386006116867065\n",
      "0.9420, 0.9730, 0.9240, 0.9150, 0.9170, 0.9500, [0.0230], 0.9850, 0.9760, 0.9490, Acc_f: 0.0230, Acc_r: 0.9479\n",
      "[train] epoch 4, batch 10, loss 0.3338853120803833\n",
      "0.9430, 0.9720, 0.9220, 0.9150, 0.9180, 0.9500, [0.0190], 0.9850, 0.9750, 0.9480, Acc_f: 0.0190, Acc_r: 0.9476\n",
      "[train] epoch 5, batch 10, loss 0.3321267366409302\n",
      "0.9440, 0.9720, 0.9240, 0.9150, 0.9160, 0.9500, [0.0140], 0.9850, 0.9740, 0.9480, Acc_f: 0.0140, Acc_r: 0.9476\n",
      "[train] epoch 6, batch 10, loss 0.3601209223270416\n",
      "0.9440, 0.9720, 0.9200, 0.9170, 0.9170, 0.9500, [0.0110], 0.9850, 0.9740, 0.9480, Acc_f: 0.0110, Acc_r: 0.9474\n",
      "[train] epoch 7, batch 10, loss 0.3344707489013672\n",
      "0.9440, 0.9720, 0.9180, 0.9210, 0.9170, 0.9500, [0.0090], 0.9850, 0.9740, 0.9480, Acc_f: 0.0090, Acc_r: 0.9477\n",
      "[train] epoch 8, batch 10, loss 0.2760794162750244\n",
      "0.9430, 0.9720, 0.9180, 0.9230, 0.9180, 0.9500, [0.0080], 0.9850, 0.9730, 0.9480, Acc_f: 0.0080, Acc_r: 0.9478\n",
      "[train] epoch 9, batch 10, loss 0.2792876362800598\n",
      "0.9430, 0.9720, 0.9190, 0.9240, 0.9160, 0.9500, [0.0070], 0.9850, 0.9740, 0.9500, Acc_f: 0.0070, Acc_r: 0.9481\n",
      "[train] epoch 10, batch 10, loss 0.2689383327960968\n",
      "0.9430, 0.9720, 0.9200, 0.9250, 0.9170, 0.9500, [0.0070], 0.9860, 0.9740, 0.9500, Acc_f: 0.0070, Acc_r: 0.9486\n",
      "[train] epoch 11, batch 10, loss 0.3003610670566559\n",
      "0.9420, 0.9720, 0.9200, 0.9260, 0.9170, 0.9500, [0.0070], 0.9860, 0.9730, 0.9500, Acc_f: 0.0070, Acc_r: 0.9484\n",
      "[train] epoch 12, batch 10, loss 0.2574321925640106\n",
      "0.9420, 0.9720, 0.9210, 0.9260, 0.9130, 0.9500, [0.0070], 0.9850, 0.9740, 0.9500, Acc_f: 0.0070, Acc_r: 0.9481\n",
      "[train] epoch 13, batch 10, loss 0.24409805238246918\n",
      "0.9420, 0.9720, 0.9200, 0.9260, 0.9130, 0.9500, [0.0060], 0.9860, 0.9740, 0.9500, Acc_f: 0.0060, Acc_r: 0.9481\n",
      "[train] epoch 14, batch 10, loss 0.2609729468822479\n",
      "0.9420, 0.9720, 0.9200, 0.9270, 0.9140, 0.9500, [0.0050], 0.9860, 0.9770, 0.9500, Acc_f: 0.0050, Acc_r: 0.9487\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.67 \\pm 0.17\n",
      "UNSC Acc_r: 95.23 \\pm 0.27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Acc_r = 100*np.array([0.9531, 0.9551, 0.9487])\n",
    "Acc_f = 100*np.array([0.0090, 0.0060, 0.0050])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "[train] epoch 0, batch 10, loss 1.9312938451766968\n",
      "0.1170, 0.9600, 0.8500, 0.7780, 0.2310, 0.8550, [0.0930], 0.9180, 0.9940, 0.9750, Acc_f: 0.0930, Acc_r: 0.7420\n",
      "[train] epoch 1, batch 10, loss 1.0576661825180054\n",
      "0.0660, 0.9600, 0.9080, 0.8080, 0.2010, 0.8280, [0.0690], 0.9180, 0.9870, 0.9750, Acc_f: 0.0690, Acc_r: 0.7390\n",
      "[train] epoch 2, batch 10, loss 1.120916724205017\n",
      "0.0570, 0.9590, 0.9000, 0.8630, 0.1910, 0.8150, [0.0640], 0.9280, 0.9780, 0.9740, Acc_f: 0.0640, Acc_r: 0.7406\n",
      "[train] epoch 3, batch 10, loss 0.9497131109237671\n",
      "0.0500, 0.9580, 0.8930, 0.8880, 0.1810, 0.8030, [0.0510], 0.9340, 0.9740, 0.9690, Acc_f: 0.0510, Acc_r: 0.7389\n",
      "[train] epoch 4, batch 10, loss 0.9455651640892029\n",
      "0.0380, 0.9570, 0.8920, 0.8950, 0.1650, 0.7810, [0.0440], 0.9340, 0.9660, 0.9680, Acc_f: 0.0440, Acc_r: 0.7329\n",
      "[train] epoch 5, batch 10, loss 0.9393824934959412\n",
      "0.0340, 0.9560, 0.8880, 0.8970, 0.1520, 0.7700, [0.0420], 0.9360, 0.9660, 0.9650, Acc_f: 0.0420, Acc_r: 0.7293\n",
      "[train] epoch 6, batch 10, loss 0.9012957811355591\n",
      "0.0300, 0.9570, 0.8890, 0.9030, 0.1430, 0.7700, [0.0410], 0.9410, 0.9570, 0.9650, Acc_f: 0.0410, Acc_r: 0.7283\n",
      "[train] epoch 7, batch 10, loss 0.7722598314285278\n",
      "0.0270, 0.9530, 0.8820, 0.9010, 0.1300, 0.7630, [0.0410], 0.9430, 0.9570, 0.9610, Acc_f: 0.0410, Acc_r: 0.7241\n",
      "[train] epoch 8, batch 10, loss 0.8592414855957031\n",
      "0.0210, 0.9520, 0.8810, 0.8980, 0.1170, 0.7590, [0.0390], 0.9420, 0.9530, 0.9600, Acc_f: 0.0390, Acc_r: 0.7203\n",
      "[train] epoch 9, batch 10, loss 0.8465537428855896\n",
      "0.0180, 0.9500, 0.8720, 0.8950, 0.1080, 0.7480, [0.0380], 0.9450, 0.9540, 0.9590, Acc_f: 0.0380, Acc_r: 0.7166\n",
      "[train] epoch 10, batch 10, loss 0.8549392819404602\n",
      "0.0150, 0.9490, 0.8660, 0.8870, 0.0990, 0.7360, [0.0350], 0.9430, 0.9580, 0.9580, Acc_f: 0.0350, Acc_r: 0.7123\n",
      "[train] epoch 11, batch 10, loss 0.778120219707489\n",
      "0.0170, 0.9470, 0.8650, 0.8930, 0.1000, 0.7310, [0.0370], 0.9460, 0.9540, 0.9580, Acc_f: 0.0370, Acc_r: 0.7123\n",
      "[train] epoch 12, batch 10, loss 0.6897382140159607\n",
      "0.0150, 0.9460, 0.8730, 0.8900, 0.0950, 0.7270, [0.0350], 0.9470, 0.9440, 0.9580, Acc_f: 0.0350, Acc_r: 0.7106\n",
      "[train] epoch 13, batch 10, loss 0.7604637742042542\n",
      "0.0140, 0.9430, 0.8720, 0.8870, 0.0920, 0.7180, [0.0380], 0.9450, 0.9420, 0.9580, Acc_f: 0.0380, Acc_r: 0.7079\n",
      "[train] epoch 14, batch 10, loss 0.7223082184791565\n",
      "0.0150, 0.9440, 0.8720, 0.8900, 0.0910, 0.7070, [0.0370], 0.9450, 0.9380, 0.9590, Acc_f: 0.0370, Acc_r: 0.7068\n",
      "[train] epoch 15, batch 10, loss 0.6777873635292053\n",
      "0.0150, 0.9420, 0.8690, 0.8890, 0.0850, 0.6940, [0.0340], 0.9430, 0.9340, 0.9590, Acc_f: 0.0340, Acc_r: 0.7033\n",
      "[train] epoch 16, batch 10, loss 0.7948279976844788\n",
      "0.0130, 0.9360, 0.8480, 0.8800, 0.0850, 0.6840, [0.0340], 0.9400, 0.9450, 0.9590, Acc_f: 0.0340, Acc_r: 0.6989\n",
      "[train] epoch 17, batch 10, loss 0.6650640964508057\n",
      "0.0090, 0.9340, 0.8660, 0.8710, 0.0830, 0.6780, [0.0330], 0.9410, 0.9300, 0.9590, Acc_f: 0.0330, Acc_r: 0.6968\n",
      "[train] epoch 18, batch 10, loss 0.7244254946708679\n",
      "0.0080, 0.9320, 0.8620, 0.8670, 0.0800, 0.6680, [0.0310], 0.9370, 0.9300, 0.9580, Acc_f: 0.0310, Acc_r: 0.6936\n",
      "[train] epoch 19, batch 10, loss 0.66758793592453\n",
      "0.0100, 0.9310, 0.8580, 0.8720, 0.0790, 0.6510, [0.0310], 0.9350, 0.9320, 0.9590, Acc_f: 0.0310, Acc_r: 0.6919\n",
      "============================================================\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "[train] epoch 0, batch 10, loss 1.8307621479034424\n",
      "0.1010, 0.9630, 0.9330, 0.6740, 0.2550, 0.8170, [0.0430], 0.9100, 0.9910, 0.9410, Acc_f: 0.0430, Acc_r: 0.7317\n",
      "[train] epoch 1, batch 10, loss 1.0479851961135864\n",
      "0.0470, 0.9650, 0.9610, 0.6920, 0.2180, 0.7720, [0.0290], 0.9220, 0.9790, 0.9450, Acc_f: 0.0290, Acc_r: 0.7223\n",
      "[train] epoch 2, batch 10, loss 1.0902334451675415\n",
      "0.0330, 0.9630, 0.9580, 0.7720, 0.2070, 0.7520, [0.0220], 0.9380, 0.9750, 0.9440, Acc_f: 0.0220, Acc_r: 0.7269\n",
      "[train] epoch 3, batch 10, loss 0.7737706899642944\n",
      "0.0250, 0.9630, 0.9630, 0.8060, 0.1840, 0.7360, [0.0130], 0.9450, 0.9680, 0.9460, Acc_f: 0.0130, Acc_r: 0.7262\n",
      "[train] epoch 4, batch 10, loss 0.7499836683273315\n",
      "0.0200, 0.9600, 0.9600, 0.8200, 0.1620, 0.7060, [0.0110], 0.9470, 0.9620, 0.9440, Acc_f: 0.0110, Acc_r: 0.7201\n",
      "[train] epoch 5, batch 10, loss 0.6507057547569275\n",
      "0.0130, 0.9570, 0.9500, 0.8330, 0.1510, 0.6860, [0.0090], 0.9470, 0.9610, 0.9430, Acc_f: 0.0090, Acc_r: 0.7157\n",
      "[train] epoch 6, batch 10, loss 0.77658611536026\n",
      "0.0130, 0.9530, 0.9490, 0.8390, 0.1410, 0.6750, [0.0080], 0.9520, 0.9600, 0.9440, Acc_f: 0.0080, Acc_r: 0.7140\n",
      "[train] epoch 7, batch 10, loss 0.8615567684173584\n",
      "0.0120, 0.9520, 0.9420, 0.8360, 0.1260, 0.6520, [0.0070], 0.9550, 0.9580, 0.9440, Acc_f: 0.0070, Acc_r: 0.7086\n",
      "[train] epoch 8, batch 10, loss 0.7798791527748108\n",
      "0.0100, 0.9510, 0.9310, 0.8380, 0.1240, 0.6310, [0.0070], 0.9560, 0.9580, 0.9480, Acc_f: 0.0070, Acc_r: 0.7052\n",
      "[train] epoch 9, batch 10, loss 0.732587456703186\n",
      "0.0070, 0.9490, 0.9480, 0.8320, 0.1200, 0.6210, [0.0060], 0.9590, 0.9500, 0.9500, Acc_f: 0.0060, Acc_r: 0.7040\n",
      "[train] epoch 10, batch 10, loss 0.6272302269935608\n",
      "0.0050, 0.9480, 0.9440, 0.8270, 0.1130, 0.6040, [0.0050], 0.9590, 0.9480, 0.9500, Acc_f: 0.0050, Acc_r: 0.6998\n",
      "[train] epoch 11, batch 10, loss 0.6848724484443665\n",
      "0.0040, 0.9460, 0.9430, 0.8230, 0.1070, 0.5880, [0.0050], 0.9600, 0.9480, 0.9490, Acc_f: 0.0050, Acc_r: 0.6964\n",
      "[train] epoch 12, batch 10, loss 0.6566336750984192\n",
      "0.0030, 0.9420, 0.9420, 0.8080, 0.1030, 0.5590, [0.0040], 0.9600, 0.9470, 0.9490, Acc_f: 0.0040, Acc_r: 0.6903\n",
      "[train] epoch 13, batch 10, loss 0.6198654174804688\n",
      "0.0020, 0.9390, 0.9370, 0.8230, 0.1010, 0.5360, [0.0040], 0.9590, 0.9440, 0.9490, Acc_f: 0.0040, Acc_r: 0.6878\n",
      "[train] epoch 14, batch 10, loss 0.7473649382591248\n",
      "0.0010, 0.9380, 0.9350, 0.8190, 0.0970, 0.5170, [0.0040], 0.9550, 0.9450, 0.9480, Acc_f: 0.0040, Acc_r: 0.6839\n",
      "[train] epoch 15, batch 10, loss 0.7325552701950073\n",
      "0.0000, 0.9360, 0.9320, 0.8180, 0.0910, 0.4970, [0.0030], 0.9540, 0.9430, 0.9470, Acc_f: 0.0030, Acc_r: 0.6798\n",
      "[train] epoch 16, batch 10, loss 0.6862399578094482\n",
      "0.0000, 0.9350, 0.9300, 0.8060, 0.0870, 0.4740, [0.0020], 0.9540, 0.9410, 0.9470, Acc_f: 0.0020, Acc_r: 0.6749\n",
      "[train] epoch 17, batch 10, loss 0.6188750267028809\n",
      "0.0000, 0.9300, 0.9270, 0.8070, 0.0850, 0.4540, [0.0020], 0.9530, 0.9410, 0.9450, Acc_f: 0.0020, Acc_r: 0.6713\n",
      "[train] epoch 18, batch 10, loss 0.5795126557350159\n",
      "0.0000, 0.9290, 0.9360, 0.7980, 0.0830, 0.4520, [0.0020], 0.9530, 0.9340, 0.9500, Acc_f: 0.0020, Acc_r: 0.6706\n",
      "[train] epoch 19, batch 10, loss 0.5382818579673767\n",
      "0.0000, 0.9270, 0.9340, 0.7900, 0.0800, 0.4320, [0.0020], 0.9530, 0.9380, 0.9450, Acc_f: 0.0020, Acc_r: 0.6666\n",
      "============================================================\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "[train] epoch 0, batch 10, loss 2.9942846298217773\n",
      "0.5540, 0.9490, 0.7060, 0.7360, 0.4470, 0.8540, [0.2060], 0.9420, 0.9990, 0.9630, Acc_f: 0.2060, Acc_r: 0.7944\n",
      "[train] epoch 1, batch 10, loss 1.4259921312332153\n",
      "0.5650, 0.9420, 0.7900, 0.6720, 0.3010, 0.7850, [0.0970], 0.9190, 0.9990, 0.9420, Acc_f: 0.0970, Acc_r: 0.7683\n",
      "[train] epoch 2, batch 10, loss 1.0563281774520874\n",
      "0.5200, 0.9360, 0.8190, 0.6070, 0.2270, 0.7370, [0.0490], 0.9130, 0.9990, 0.9290, Acc_f: 0.0490, Acc_r: 0.7430\n",
      "[train] epoch 3, batch 10, loss 1.1572065353393555\n",
      "0.4490, 0.9310, 0.8300, 0.5570, 0.1890, 0.7120, [0.0320], 0.9130, 0.9980, 0.9220, Acc_f: 0.0320, Acc_r: 0.7223\n",
      "[train] epoch 4, batch 10, loss 0.938096284866333\n",
      "0.3740, 0.9280, 0.8330, 0.5100, 0.1780, 0.6950, [0.0240], 0.9180, 0.9980, 0.9170, Acc_f: 0.0240, Acc_r: 0.7057\n",
      "[train] epoch 5, batch 10, loss 1.015512466430664\n",
      "0.3030, 0.9300, 0.8480, 0.4900, 0.1640, 0.6910, [0.0170], 0.9240, 0.9960, 0.9200, Acc_f: 0.0170, Acc_r: 0.6962\n",
      "[train] epoch 6, batch 10, loss 1.0032291412353516\n",
      "0.2330, 0.9230, 0.8380, 0.4430, 0.1450, 0.6630, [0.0140], 0.9290, 0.9960, 0.9140, Acc_f: 0.0140, Acc_r: 0.6760\n",
      "[train] epoch 7, batch 10, loss 0.9094220995903015\n",
      "0.1980, 0.9270, 0.8500, 0.4220, 0.1310, 0.6500, [0.0110], 0.9360, 0.9960, 0.9190, Acc_f: 0.0110, Acc_r: 0.6699\n",
      "[train] epoch 8, batch 10, loss 0.9455382227897644\n",
      "0.1580, 0.9240, 0.8510, 0.3960, 0.1230, 0.6370, [0.0100], 0.9400, 0.9950, 0.9200, Acc_f: 0.0100, Acc_r: 0.6604\n",
      "[train] epoch 9, batch 10, loss 0.9461691975593567\n",
      "0.1240, 0.9220, 0.8500, 0.3830, 0.1110, 0.6180, [0.0090], 0.9430, 0.9950, 0.9160, Acc_f: 0.0090, Acc_r: 0.6513\n",
      "[train] epoch 10, batch 10, loss 0.8213668465614319\n",
      "0.1080, 0.9220, 0.8410, 0.3640, 0.1030, 0.6050, [0.0070], 0.9430, 0.9940, 0.9070, Acc_f: 0.0070, Acc_r: 0.6430\n",
      "[train] epoch 11, batch 10, loss 0.7552803158760071\n",
      "0.1000, 0.9190, 0.8340, 0.3490, 0.0920, 0.5810, [0.0070], 0.9460, 0.9940, 0.8970, Acc_f: 0.0070, Acc_r: 0.6347\n",
      "[train] epoch 12, batch 10, loss 0.8422409892082214\n",
      "0.0850, 0.9200, 0.8390, 0.3370, 0.0910, 0.5690, [0.0070], 0.9460, 0.9940, 0.8930, Acc_f: 0.0070, Acc_r: 0.6304\n",
      "[train] epoch 13, batch 10, loss 0.8505348563194275\n",
      "0.0660, 0.9180, 0.8370, 0.3200, 0.0890, 0.5540, [0.0060], 0.9450, 0.9930, 0.8900, Acc_f: 0.0060, Acc_r: 0.6236\n",
      "[train] epoch 14, batch 10, loss 0.7974151372909546\n",
      "0.0590, 0.9170, 0.8310, 0.3130, 0.0850, 0.5410, [0.0060], 0.9430, 0.9920, 0.8820, Acc_f: 0.0060, Acc_r: 0.6181\n",
      "[train] epoch 15, batch 10, loss 0.6832866072654724\n",
      "0.0520, 0.9190, 0.8360, 0.3090, 0.0830, 0.5250, [0.0060], 0.9440, 0.9920, 0.8780, Acc_f: 0.0060, Acc_r: 0.6153\n",
      "[train] epoch 16, batch 10, loss 0.7311373353004456\n",
      "0.0480, 0.9190, 0.8290, 0.3020, 0.0800, 0.5160, [0.0060], 0.9410, 0.9920, 0.8710, Acc_f: 0.0060, Acc_r: 0.6109\n",
      "[train] epoch 17, batch 10, loss 0.7939513921737671\n",
      "0.0440, 0.9160, 0.8260, 0.2940, 0.0760, 0.5070, [0.0060], 0.9370, 0.9920, 0.8560, Acc_f: 0.0060, Acc_r: 0.6053\n",
      "[train] epoch 18, batch 10, loss 0.6900877356529236\n",
      "0.0410, 0.9190, 0.8300, 0.2880, 0.0760, 0.4990, [0.0050], 0.9360, 0.9910, 0.8500, Acc_f: 0.0050, Acc_r: 0.6033\n",
      "[train] epoch 19, batch 10, loss 0.9481807351112366\n",
      "0.0410, 0.9250, 0.8330, 0.2900, 0.0770, 0.4950, [0.0050], 0.9400, 0.9910, 0.8520, Acc_f: 0.0050, Acc_r: 0.6049\n"
     ]
    }
   ],
   "source": [
    "from agents.adv import FGSM\n",
    "\n",
    "def find_adjacent_cls(adv_agent, x, y):\n",
    "    x_adv = adv_agent.perturb(x, y)\n",
    "    adv_logits = model(x_adv)\n",
    "    adv_pred = torch.argmax(adv_logits.data, 1)\n",
    "    return adv_pred, x_adv\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    adv_agent = FGSM(deepcopy(model), bound=0.5, norm=False, random_start=True, device='cuda')\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    print('==='*20)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0005)\n",
    "\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    model.eval()\n",
    "    for ep in range(20):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            adv_pred, x_adv = find_adjacent_cls(adv_agent, x, y)\n",
    "            adv_y = torch.argmax(model(x_adv), dim=1).detach().cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, adv_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BU Acc_f: 2.97 \\pm 1.28\n",
      "BU Acc_r: 72.71 \\pm 0.44\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.7329, 0.7262, 0.7223])\n",
    "Acc_f = 100*np.array([0.0440, 0.0130, 0.0320])\n",
    "\n",
    "print(f'BU Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'BU Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "[train] epoch 0, batch 10, loss 4.493492603302002\n",
      "0.6980, 0.9780, 0.7870, 0.9150, 0.7260, 0.9500, [0.1230], 0.9810, 0.9910, 0.9620, Acc_f: 0.1230, Acc_r: 0.8876\n",
      "[train] epoch 1, batch 10, loss 2.921926259994507\n",
      "0.7990, 0.9850, 0.8450, 0.9450, 0.7630, 0.9530, [0.0420], 0.9730, 0.9860, 0.9660, Acc_f: 0.0420, Acc_r: 0.9128\n",
      "[train] epoch 2, batch 10, loss 2.778705596923828\n",
      "0.8170, 0.9870, 0.8540, 0.9470, 0.7750, 0.9550, [0.0240], 0.9640, 0.9850, 0.9730, Acc_f: 0.0240, Acc_r: 0.9174\n",
      "[train] epoch 3, batch 10, loss 2.756378650665283\n",
      "0.8250, 0.9880, 0.8450, 0.9460, 0.7610, 0.9530, [0.0140], 0.9570, 0.9840, 0.9760, Acc_f: 0.0140, Acc_r: 0.9150\n",
      "[train] epoch 4, batch 10, loss 2.516944646835327\n",
      "0.8180, 0.9890, 0.8400, 0.9400, 0.7630, 0.9490, [0.0080], 0.9530, 0.9840, 0.9770, Acc_f: 0.0080, Acc_r: 0.9126\n",
      "[train] epoch 5, batch 10, loss 2.4788084030151367\n",
      "0.8290, 0.9890, 0.8480, 0.9340, 0.7660, 0.9480, [0.0040], 0.9480, 0.9830, 0.9780, Acc_f: 0.0040, Acc_r: 0.9137\n",
      "[train] epoch 6, batch 10, loss 2.4331088066101074\n",
      "0.8180, 0.9890, 0.8390, 0.9300, 0.7690, 0.9490, [0.0020], 0.9430, 0.9840, 0.9800, Acc_f: 0.0020, Acc_r: 0.9112\n",
      "[train] epoch 7, batch 10, loss 2.4766101837158203\n",
      "0.7780, 0.9890, 0.8210, 0.9160, 0.7500, 0.9490, [0.0020], 0.9410, 0.9830, 0.9800, Acc_f: 0.0020, Acc_r: 0.9008\n",
      "[train] epoch 8, batch 10, loss 2.45048451423645\n",
      "0.7830, 0.9890, 0.8320, 0.9150, 0.7560, 0.9490, [0.0000], 0.9360, 0.9830, 0.9800, Acc_f: 0.0000, Acc_r: 0.9026\n",
      "[train] epoch 9, batch 10, loss 2.3394007682800293\n",
      "0.7760, 0.9890, 0.8260, 0.9110, 0.7590, 0.9490, [0.0000], 0.9330, 0.9830, 0.9830, Acc_f: 0.0000, Acc_r: 0.9010\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "[train] epoch 0, batch 10, loss 4.509137153625488\n",
      "0.7450, 0.9820, 0.8380, 0.8850, 0.7630, 0.9680, [0.0590], 0.9840, 0.9900, 0.9700, Acc_f: 0.0590, Acc_r: 0.9028\n",
      "[train] epoch 1, batch 10, loss 2.8311941623687744\n",
      "0.8210, 0.9870, 0.8720, 0.9080, 0.7500, 0.9700, [0.0230], 0.9670, 0.9860, 0.9770, Acc_f: 0.0230, Acc_r: 0.9153\n",
      "[train] epoch 2, batch 10, loss 2.7657155990600586\n",
      "0.8370, 0.9870, 0.8580, 0.9170, 0.7090, 0.9680, [0.0090], 0.9610, 0.9840, 0.9790, Acc_f: 0.0090, Acc_r: 0.9111\n",
      "[train] epoch 3, batch 10, loss 2.57645583152771\n",
      "0.8430, 0.9870, 0.8610, 0.9170, 0.7040, 0.9640, [0.0030], 0.9570, 0.9840, 0.9800, Acc_f: 0.0030, Acc_r: 0.9108\n",
      "[train] epoch 4, batch 10, loss 2.5159835815429688\n",
      "0.8350, 0.9870, 0.8600, 0.9120, 0.7130, 0.9620, [0.0020], 0.9530, 0.9840, 0.9810, Acc_f: 0.0020, Acc_r: 0.9097\n",
      "[train] epoch 5, batch 10, loss 2.4261035919189453\n",
      "0.8370, 0.9870, 0.8610, 0.9100, 0.7060, 0.9570, [0.0010], 0.9460, 0.9830, 0.9830, Acc_f: 0.0010, Acc_r: 0.9078\n",
      "[train] epoch 6, batch 10, loss 2.4106574058532715\n",
      "0.8410, 0.9870, 0.8720, 0.9040, 0.7050, 0.9520, [0.0010], 0.9430, 0.9790, 0.9840, Acc_f: 0.0010, Acc_r: 0.9074\n",
      "[train] epoch 7, batch 10, loss 2.428351402282715\n",
      "0.8270, 0.9870, 0.8660, 0.8990, 0.6970, 0.9510, [0.0010], 0.9410, 0.9810, 0.9840, Acc_f: 0.0010, Acc_r: 0.9037\n",
      "[train] epoch 8, batch 10, loss 2.3739733695983887\n",
      "0.8320, 0.9870, 0.8770, 0.9000, 0.7160, 0.9460, [0.0010], 0.9380, 0.9770, 0.9850, Acc_f: 0.0010, Acc_r: 0.9064\n",
      "[train] epoch 9, batch 10, loss 2.348412036895752\n",
      "0.8360, 0.9870, 0.8710, 0.9050, 0.7010, 0.9430, [0.0010], 0.9340, 0.9750, 0.9850, Acc_f: 0.0010, Acc_r: 0.9041\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "[train] epoch 0, batch 10, loss 5.095587730407715\n",
      "0.8880, 0.9820, 0.9100, 0.9550, 0.8890, 0.9460, [0.3750], 0.9740, 0.9920, 0.9660, Acc_f: 0.3750, Acc_r: 0.9447\n",
      "[train] epoch 1, batch 10, loss 3.1571097373962402\n",
      "0.8440, 0.9830, 0.8270, 0.9390, 0.8510, 0.9160, [0.1280], 0.9710, 0.9950, 0.9710, Acc_f: 0.1280, Acc_r: 0.9219\n",
      "[train] epoch 2, batch 10, loss 2.7921485900878906\n",
      "0.8280, 0.9840, 0.7860, 0.9310, 0.8380, 0.9120, [0.0540], 0.9690, 0.9920, 0.9720, Acc_f: 0.0540, Acc_r: 0.9124\n",
      "[train] epoch 3, batch 10, loss 2.886348009109497\n",
      "0.8210, 0.9860, 0.7610, 0.9320, 0.8220, 0.9200, [0.0290], 0.9680, 0.9910, 0.9740, Acc_f: 0.0290, Acc_r: 0.9083\n",
      "[train] epoch 4, batch 10, loss 2.6902761459350586\n",
      "0.8230, 0.9870, 0.7490, 0.9360, 0.8250, 0.9270, [0.0190], 0.9670, 0.9900, 0.9750, Acc_f: 0.0190, Acc_r: 0.9088\n",
      "[train] epoch 5, batch 10, loss 2.611440420150757\n",
      "0.8200, 0.9870, 0.7290, 0.9300, 0.8240, 0.9290, [0.0150], 0.9680, 0.9900, 0.9750, Acc_f: 0.0150, Acc_r: 0.9058\n",
      "[train] epoch 6, batch 10, loss 2.67956280708313\n",
      "0.8140, 0.9870, 0.6980, 0.9260, 0.8120, 0.9290, [0.0100], 0.9660, 0.9900, 0.9750, Acc_f: 0.0100, Acc_r: 0.8997\n",
      "[train] epoch 7, batch 10, loss 2.6460773944854736\n",
      "0.8220, 0.9880, 0.6890, 0.9210, 0.8220, 0.9320, [0.0070], 0.9660, 0.9890, 0.9750, Acc_f: 0.0070, Acc_r: 0.9004\n",
      "[train] epoch 8, batch 10, loss 2.57619309425354\n",
      "0.8200, 0.9880, 0.6770, 0.9190, 0.8270, 0.9320, [0.0070], 0.9660, 0.9870, 0.9750, Acc_f: 0.0070, Acc_r: 0.8990\n",
      "[train] epoch 9, batch 10, loss 2.5062758922576904\n",
      "0.8210, 0.9880, 0.6660, 0.9170, 0.8320, 0.9330, [0.0060], 0.9660, 0.9870, 0.9750, Acc_f: 0.0060, Acc_r: 0.8983\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Acc_f: 3.37 \\pm 1.44\n",
      "RL Acc_r: 91.50 \\pm 0.20\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9174, 0.9153, 0.9124])\n",
    "Acc_f = 100*np.array([0.0240, 0.0230, 0.0540])\n",
    "\n",
    "print(f'RL Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'RL Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "[train] epoch 0, batch 10, loss -0.11080976575613022\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8570, 0.9780, 0.9140, 0.9320, 0.8450, 0.9580, [0.8130], 0.9850, 0.9880, 0.9570, Acc_f: 0.8130, Acc_r: 0.9349\n",
      "[train] epoch 1, batch 10, loss -0.056481387466192245\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8600, 0.9780, 0.9160, 0.9300, 0.8480, 0.9580, [0.8050], 0.9850, 0.9880, 0.9570, Acc_f: 0.8050, Acc_r: 0.9356\n",
      "[train] epoch 2, batch 10, loss -0.053473953157663345\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8640, 0.9780, 0.9190, 0.9280, 0.8480, 0.9580, [0.7970], 0.9850, 0.9880, 0.9570, Acc_f: 0.7970, Acc_r: 0.9361\n",
      "[train] epoch 3, batch 10, loss -0.05863038823008537\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8670, 0.9780, 0.9190, 0.9280, 0.8490, 0.9580, [0.7880], 0.9850, 0.9880, 0.9570, Acc_f: 0.7880, Acc_r: 0.9366\n",
      "[train] epoch 4, batch 10, loss -0.0699896290898323\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8680, 0.9780, 0.9230, 0.9260, 0.8470, 0.9580, [0.7790], 0.9850, 0.9880, 0.9560, Acc_f: 0.7790, Acc_r: 0.9366\n",
      "[train] epoch 5, batch 10, loss -0.07848202437162399\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8690, 0.9780, 0.9250, 0.9240, 0.8470, 0.9570, [0.7700], 0.9850, 0.9880, 0.9560, Acc_f: 0.7700, Acc_r: 0.9366\n",
      "[train] epoch 6, batch 10, loss -0.1059817224740982\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8740, 0.9780, 0.9300, 0.9250, 0.8440, 0.9570, [0.7620], 0.9860, 0.9880, 0.9570, Acc_f: 0.7620, Acc_r: 0.9377\n",
      "[train] epoch 7, batch 10, loss -0.11866949498653412\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8770, 0.9770, 0.9330, 0.9230, 0.8420, 0.9550, [0.7380], 0.9860, 0.9880, 0.9570, Acc_f: 0.7380, Acc_r: 0.9376\n",
      "[train] epoch 8, batch 10, loss -0.14130140841007233\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8840, 0.9770, 0.9360, 0.9240, 0.8340, 0.9550, [0.7120], 0.9860, 0.9880, 0.9570, Acc_f: 0.7120, Acc_r: 0.9379\n",
      "[train] epoch 9, batch 10, loss -0.17916572093963623\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8930, 0.9760, 0.9410, 0.9250, 0.8240, 0.9540, [0.6770], 0.9860, 0.9880, 0.9580, Acc_f: 0.6770, Acc_r: 0.9383\n",
      "[train] epoch 10, batch 10, loss -0.26402947306632996\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9000, 0.9760, 0.9460, 0.9190, 0.8030, 0.9520, [0.6490], 0.9860, 0.9880, 0.9580, Acc_f: 0.6490, Acc_r: 0.9364\n",
      "[train] epoch 11, batch 10, loss -0.4197585880756378\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9060, 0.9770, 0.9490, 0.9020, 0.7730, 0.9490, [0.5880], 0.9860, 0.9880, 0.9580, Acc_f: 0.5880, Acc_r: 0.9320\n",
      "[train] epoch 12, batch 10, loss -0.7115438580513\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9050, 0.9750, 0.9440, 0.8860, 0.7130, 0.9470, [0.4940], 0.9850, 0.9910, 0.9600, Acc_f: 0.4940, Acc_r: 0.9229\n",
      "[train] epoch 13, batch 10, loss -1.476686716079712\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9040, 0.9750, 0.9270, 0.8550, 0.5930, 0.9400, [0.3700], 0.9850, 0.9930, 0.9600, Acc_f: 0.3700, Acc_r: 0.9036\n",
      "[train] epoch 14, batch 10, loss -2.9206831455230713\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8800, 0.9670, 0.8760, 0.7790, 0.3970, 0.9230, [0.2360], 0.9830, 0.9960, 0.9600, Acc_f: 0.2360, Acc_r: 0.8623\n",
      "[train] epoch 15, batch 10, loss -5.899458408355713\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.6190, 0.9300, 0.6780, 0.5840, 0.1970, 0.8770, [0.0990], 0.9570, 0.9960, 0.9550, Acc_f: 0.0990, Acc_r: 0.7548\n",
      "[train] epoch 16, batch 10, loss -13.176453590393066\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.1060, 0.6070, 0.2020, 0.2010, 0.0520, 0.5120, [0.0140], 0.4410, 0.9990, 0.4400, Acc_f: 0.0140, Acc_r: 0.3956\n",
      "[train] epoch 17, batch 10, loss -25.249391555786133\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.0000, 0.0260, 0.0040, 0.0050, 0.0040, 0.0080, [0.0000], 0.0010, 1.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1164\n",
      "[train] epoch 18, batch 10, loss -52.131690979003906\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 1.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 10, loss -205.30111694335938\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 1.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "------------ Trail 1 ------------\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "[train] epoch 0, batch 10, loss -0.073306605219841\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8490, 0.9820, 0.9190, 0.9370, 0.8780, 0.9630, [0.8050], 0.9880, 0.9850, 0.9620, Acc_f: 0.8050, Acc_r: 0.9403\n",
      "[train] epoch 1, batch 10, loss -0.043573617935180664\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8500, 0.9820, 0.9200, 0.9370, 0.8760, 0.9630, [0.8000], 0.9880, 0.9850, 0.9620, Acc_f: 0.8000, Acc_r: 0.9403\n",
      "[train] epoch 2, batch 10, loss -0.05379940941929817\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8560, 0.9820, 0.9230, 0.9370, 0.8750, 0.9630, [0.7910], 0.9880, 0.9860, 0.9620, Acc_f: 0.7910, Acc_r: 0.9413\n",
      "[train] epoch 3, batch 10, loss -0.042598918080329895\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8610, 0.9830, 0.9240, 0.9380, 0.8780, 0.9630, [0.7790], 0.9890, 0.9860, 0.9610, Acc_f: 0.7790, Acc_r: 0.9426\n",
      "[train] epoch 4, batch 10, loss -0.060663361102342606\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8640, 0.9830, 0.9280, 0.9360, 0.8770, 0.9620, [0.7710], 0.9890, 0.9860, 0.9610, Acc_f: 0.7710, Acc_r: 0.9429\n",
      "[train] epoch 5, batch 10, loss -0.0622747465968132\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8660, 0.9830, 0.9320, 0.9350, 0.8760, 0.9620, [0.7630], 0.9890, 0.9860, 0.9600, Acc_f: 0.7630, Acc_r: 0.9432\n",
      "[train] epoch 6, batch 10, loss -0.07448393851518631\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8700, 0.9830, 0.9360, 0.9370, 0.8720, 0.9620, [0.7490], 0.9900, 0.9860, 0.9600, Acc_f: 0.7490, Acc_r: 0.9440\n",
      "[train] epoch 7, batch 10, loss -0.09081320464611053\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8740, 0.9830, 0.9420, 0.9370, 0.8710, 0.9620, [0.7350], 0.9900, 0.9860, 0.9580, Acc_f: 0.7350, Acc_r: 0.9448\n",
      "[train] epoch 8, batch 10, loss -0.11511809378862381\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8780, 0.9830, 0.9460, 0.9360, 0.8720, 0.9600, [0.7180], 0.9900, 0.9860, 0.9580, Acc_f: 0.7180, Acc_r: 0.9454\n",
      "[train] epoch 9, batch 10, loss -0.15695728361606598\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8800, 0.9830, 0.9500, 0.9340, 0.8680, 0.9590, [0.6910], 0.9900, 0.9860, 0.9570, Acc_f: 0.6910, Acc_r: 0.9452\n",
      "[train] epoch 10, batch 10, loss -0.2159988135099411\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8890, 0.9830, 0.9500, 0.9290, 0.8510, 0.9580, [0.6410], 0.9910, 0.9880, 0.9580, Acc_f: 0.6410, Acc_r: 0.9441\n",
      "[train] epoch 11, batch 10, loss -0.3669154942035675\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8960, 0.9830, 0.9580, 0.9240, 0.8300, 0.9540, [0.5830], 0.9910, 0.9890, 0.9580, Acc_f: 0.5830, Acc_r: 0.9426\n",
      "[train] epoch 12, batch 10, loss -0.6343544721603394\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9030, 0.9840, 0.9670, 0.9100, 0.7810, 0.9520, [0.4830], 0.9890, 0.9900, 0.9570, Acc_f: 0.4830, Acc_r: 0.9370\n",
      "[train] epoch 13, batch 10, loss -1.29249107837677\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9180, 0.9830, 0.9660, 0.8770, 0.6750, 0.9440, [0.3370], 0.9890, 0.9930, 0.9580, Acc_f: 0.3370, Acc_r: 0.9226\n",
      "[train] epoch 14, batch 10, loss -3.2019011974334717\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9130, 0.9780, 0.9690, 0.8100, 0.4650, 0.9330, [0.1980], 0.9870, 0.9930, 0.9610, Acc_f: 0.1980, Acc_r: 0.8899\n",
      "[train] epoch 15, batch 10, loss -6.531900405883789\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9000, 0.9730, 0.9660, 0.6290, 0.2670, 0.9160, [0.0650], 0.9750, 0.9920, 0.9460, Acc_f: 0.0650, Acc_r: 0.8404\n",
      "[train] epoch 16, batch 10, loss -10.46535587310791\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8740, 0.9510, 0.9730, 0.3370, 0.1350, 0.8670, [0.0140], 0.9110, 0.9910, 0.8350, Acc_f: 0.0140, Acc_r: 0.7638\n",
      "[train] epoch 17, batch 10, loss -16.083988189697266\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.7850, 0.9000, 0.9860, 0.0860, 0.0460, 0.6880, [0.0020], 0.5360, 0.9760, 0.3850, Acc_f: 0.0020, Acc_r: 0.5987\n",
      "[train] epoch 18, batch 10, loss -24.807554244995117\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.2320, 0.5230, 0.9980, 0.0050, 0.0060, 0.1730, [0.0000], 0.0940, 0.6980, 0.0020, Acc_f: 0.0000, Acc_r: 0.3034\n",
      "[train] epoch 19, batch 10, loss -50.97227478027344\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.0010, 0.0030, 1.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0080, 0.0000, Acc_f: 0.0000, Acc_r: 0.1124\n",
      "[train] epoch 20, batch 10, loss -196.8780059814453\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 10, loss nan\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, [0.0000], 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "------------ Trail 2 ------------\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "[train] epoch 0, batch 10, loss -0.3083973526954651\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8590, 0.9760, 0.9000, 0.9400, 0.8630, 0.9570, [0.7790], 0.9820, 0.9850, 0.9610, Acc_f: 0.7790, Acc_r: 0.9359\n",
      "[train] epoch 1, batch 10, loss -0.26242145895957947\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8600, 0.9760, 0.9020, 0.9400, 0.8670, 0.9570, [0.7690], 0.9830, 0.9850, 0.9610, Acc_f: 0.7690, Acc_r: 0.9368\n",
      "[train] epoch 2, batch 10, loss -0.25052812695503235\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8650, 0.9760, 0.9030, 0.9400, 0.8660, 0.9570, [0.7570], 0.9830, 0.9860, 0.9610, Acc_f: 0.7570, Acc_r: 0.9374\n",
      "[train] epoch 3, batch 10, loss -0.26882076263427734\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8710, 0.9770, 0.9080, 0.9390, 0.8650, 0.9570, [0.7490], 0.9840, 0.9860, 0.9610, Acc_f: 0.7490, Acc_r: 0.9387\n",
      "[train] epoch 4, batch 10, loss -0.34527111053466797\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8730, 0.9770, 0.9130, 0.9400, 0.8600, 0.9570, [0.7390], 0.9840, 0.9850, 0.9610, Acc_f: 0.7390, Acc_r: 0.9389\n",
      "[train] epoch 5, batch 10, loss -0.2846825122833252\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8760, 0.9770, 0.9170, 0.9400, 0.8590, 0.9570, [0.7240], 0.9850, 0.9850, 0.9610, Acc_f: 0.7240, Acc_r: 0.9397\n",
      "[train] epoch 6, batch 10, loss -0.3764650225639343\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8780, 0.9770, 0.9190, 0.9370, 0.8660, 0.9570, [0.7050], 0.9850, 0.9850, 0.9610, Acc_f: 0.7050, Acc_r: 0.9406\n",
      "[train] epoch 7, batch 10, loss -0.39867544174194336\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8810, 0.9770, 0.9240, 0.9360, 0.8650, 0.9550, [0.6900], 0.9850, 0.9850, 0.9610, Acc_f: 0.6900, Acc_r: 0.9410\n",
      "[train] epoch 8, batch 10, loss -0.39179813861846924\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8840, 0.9770, 0.9310, 0.9380, 0.8600, 0.9550, [0.6700], 0.9860, 0.9860, 0.9590, Acc_f: 0.6700, Acc_r: 0.9418\n",
      "[train] epoch 9, batch 10, loss -0.4413652718067169\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8910, 0.9770, 0.9370, 0.9360, 0.8600, 0.9530, [0.6450], 0.9860, 0.9860, 0.9590, Acc_f: 0.6450, Acc_r: 0.9428\n",
      "[train] epoch 10, batch 10, loss -0.5516433715820312\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8940, 0.9770, 0.9400, 0.9340, 0.8550, 0.9530, [0.6140], 0.9860, 0.9870, 0.9590, Acc_f: 0.6140, Acc_r: 0.9428\n",
      "[train] epoch 11, batch 10, loss -0.5785308480262756\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.8990, 0.9770, 0.9440, 0.9330, 0.8520, 0.9530, [0.5950], 0.9860, 0.9870, 0.9580, Acc_f: 0.5950, Acc_r: 0.9432\n",
      "[train] epoch 12, batch 10, loss -0.7125177979469299\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9100, 0.9770, 0.9460, 0.9300, 0.8520, 0.9520, [0.5580], 0.9860, 0.9880, 0.9580, Acc_f: 0.5580, Acc_r: 0.9443\n",
      "[train] epoch 13, batch 10, loss -0.7729268074035645\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9120, 0.9770, 0.9490, 0.9230, 0.8480, 0.9520, [0.5210], 0.9860, 0.9880, 0.9580, Acc_f: 0.5210, Acc_r: 0.9437\n",
      "[train] epoch 14, batch 10, loss -0.8470319509506226\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9140, 0.9770, 0.9490, 0.9240, 0.8370, 0.9500, [0.4830], 0.9850, 0.9880, 0.9580, Acc_f: 0.4830, Acc_r: 0.9424\n",
      "[train] epoch 15, batch 10, loss -0.9650465250015259\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9190, 0.9790, 0.9530, 0.9220, 0.8260, 0.9490, [0.4400], 0.9850, 0.9880, 0.9570, Acc_f: 0.4400, Acc_r: 0.9420\n",
      "[train] epoch 16, batch 10, loss -1.0771466493606567\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9230, 0.9800, 0.9520, 0.9200, 0.8100, 0.9480, [0.4000], 0.9840, 0.9880, 0.9580, Acc_f: 0.4000, Acc_r: 0.9403\n",
      "[train] epoch 17, batch 10, loss -1.407192587852478\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9250, 0.9800, 0.9540, 0.9140, 0.7720, 0.9480, [0.3450], 0.9840, 0.9890, 0.9570, Acc_f: 0.3450, Acc_r: 0.9359\n",
      "[train] epoch 18, batch 10, loss -1.5439770221710205\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9340, 0.9800, 0.9550, 0.9060, 0.7380, 0.9430, [0.3030], 0.9830, 0.9910, 0.9560, Acc_f: 0.3030, Acc_r: 0.9318\n",
      "[train] epoch 19, batch 10, loss -1.7155702114105225\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9350, 0.9800, 0.9580, 0.8870, 0.6990, 0.9390, [0.2360], 0.9800, 0.9930, 0.9580, Acc_f: 0.2360, Acc_r: 0.9254\n",
      "[train] epoch 20, batch 10, loss -2.1181447505950928\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9380, 0.9790, 0.9540, 0.8710, 0.6500, 0.9330, [0.2000], 0.9780, 0.9930, 0.9600, Acc_f: 0.2000, Acc_r: 0.9173\n",
      "[train] epoch 21, batch 10, loss -2.530926465988159\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9400, 0.9790, 0.9500, 0.8450, 0.5880, 0.9260, [0.1550], 0.9750, 0.9920, 0.9620, Acc_f: 0.1550, Acc_r: 0.9063\n",
      "[train] epoch 22, batch 10, loss -3.161137819290161\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9420, 0.9780, 0.9490, 0.8060, 0.5450, 0.9170, [0.1250], 0.9720, 0.9930, 0.9630, Acc_f: 0.1250, Acc_r: 0.8961\n",
      "[train] epoch 23, batch 10, loss -3.4791462421417236\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9420, 0.9780, 0.9470, 0.7650, 0.4930, 0.9080, [0.0900], 0.9710, 0.9930, 0.9630, Acc_f: 0.0900, Acc_r: 0.8844\n",
      "[train] epoch 24, batch 10, loss -4.249629974365234\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9380, 0.9770, 0.9400, 0.7140, 0.4320, 0.8980, [0.0680], 0.9630, 0.9940, 0.9650, Acc_f: 0.0680, Acc_r: 0.8690\n",
      "[train] epoch 25, batch 10, loss -5.047976970672607\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9410, 0.9760, 0.9330, 0.6520, 0.3660, 0.8830, [0.0440], 0.9600, 0.9940, 0.9640, Acc_f: 0.0440, Acc_r: 0.8521\n",
      "[train] epoch 26, batch 10, loss -5.6663360595703125\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9460, 0.9730, 0.9220, 0.5700, 0.3090, 0.8680, [0.0280], 0.9530, 0.9940, 0.9620, Acc_f: 0.0280, Acc_r: 0.8330\n",
      "[train] epoch 27, batch 10, loss -6.708719253540039\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9470, 0.9700, 0.9060, 0.4910, 0.2580, 0.8480, [0.0180], 0.9370, 0.9930, 0.9490, Acc_f: 0.0180, Acc_r: 0.8110\n",
      "[train] epoch 28, batch 10, loss -7.668573379516602\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9490, 0.9630, 0.8710, 0.4160, 0.2180, 0.8040, [0.0110], 0.9060, 0.9930, 0.9300, Acc_f: 0.0110, Acc_r: 0.7833\n",
      "[train] epoch 29, batch 10, loss -8.785694122314453\n",
      ">> unlearned model testing acc by class SGD <<\n",
      "0.9500, 0.9570, 0.8430, 0.3230, 0.1810, 0.7690, [0.0070], 0.8520, 0.9930, 0.8660, Acc_f: 0.0070, Acc_r: 0.7482\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.00005)\n",
    "    \n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    for ep in range(30):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = -criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        print('>> unlearned model testing acc by class SGD <<')\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Acc_f: 6.40 \\pm 2.90\n",
      "GA Acc_r: 80.94 \\pm 3.87\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.7548, 0.8404, 0.8330])\n",
    "Acc_f = 100*np.array([0.0990, 0.0650, 0.0280])\n",
    "\n",
    "print(f'GA Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'GA Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hessian(model, train_loader):\n",
    "    model.eval()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(device), orig_target.to(device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad2_acc += torch.mean(prob[:, y]) * p.grad.data.pow(2) \n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc /= len(train_loader)\n",
    "    \n",
    "def get_mean_var(args, p, alpha=1e-7):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3) \n",
    "    if p.size(0) == args.num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var \n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == args.num_classes:\n",
    "        mu[args.unlearn_class] = 0\n",
    "        var[args.unlearn_class] = 0.0001\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        var *= 10 \n",
    "    return mu, var\n",
    "\n",
    "def fisher_new(model, train_loader):\n",
    "    for p in model.parameters():\n",
    "        p.data0 = copy.deepcopy(p.data.clone())\n",
    "    hessian(model, train_loader)\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        mu, var = get_mean_var(args, p)\n",
    "        p.data = mu + var.sqrt() * torch.empty_like(p.data).normal_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:05<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9390, 0.9820, 0.9510, 0.9380, 0.8440, 0.9570, [0.0310], 0.9820, 0.9920, 0.9560, Acc_f: 0.0310, Acc_r: 0.9490\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:05<00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9240, 0.9820, 0.9620, 0.9340, 0.8560, 0.9690, [0.0140], 0.9780, 0.9880, 0.9680, Acc_f: 0.0140, Acc_r: 0.9512\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:05<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280, 0.9810, 0.9400, 0.9550, 0.8930, 0.9600, [0.0000], 0.9780, 0.9810, 0.9630, Acc_f: 0.0000, Acc_r: 0.9532\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    fisher_model = copy.deepcopy(model)\n",
    "    fisher_new(fisher_model, remain_train_loader)\n",
    "    test_by_class(fisher_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Acc_f: 1.50 \\pm 1.27\n",
      "Fisher Acc_r: 95.11 \\pm 0.17\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9490, 0.9512, 0.9532])\n",
    "Acc_f = 100*np.array([0.0310, 0.0140, 0.0000])\n",
    "\n",
    "print(f'Fisher Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Fisher Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SalUn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# create saliency map\n",
    "def save_gradient_ratio(unlearn_train_loader, model, criterion, args, seed):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        args.unlearn_lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    gradients = {}\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        gradients[name] = 0\n",
    "\n",
    "    for i, (image, target) in enumerate(unlearn_train_loader):\n",
    "        image = image.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output_clean = model(image)\n",
    "        loss = - criterion(output_clean, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    gradients[name] += param.grad.data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name in gradients:\n",
    "            gradients[name] = torch.abs_(gradients[name])\n",
    "\n",
    "    threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    for i in threshold_list:\n",
    "        print(i)\n",
    "        sorted_dict_positions = {}\n",
    "        hard_dict = {}\n",
    "\n",
    "        # Concatenate all tensors into a single tensor\n",
    "        all_elements = - torch.cat([tensor.flatten() for tensor in gradients.values()])\n",
    "\n",
    "        # Calculate the threshold index for the top 10% elements\n",
    "        threshold_index = int(len(all_elements) * i)\n",
    "\n",
    "        # Calculate positions of all elements\n",
    "        positions = torch.argsort(all_elements)\n",
    "        ranks = torch.argsort(positions)\n",
    "\n",
    "        start_index = 0\n",
    "        for key, tensor in gradients.items():\n",
    "            num_elements = tensor.numel()\n",
    "            # tensor_positions = positions[start_index: start_index + num_elements]\n",
    "            tensor_ranks = ranks[start_index : start_index + num_elements]\n",
    "\n",
    "            sorted_positions = tensor_ranks.reshape(tensor.shape)\n",
    "            sorted_dict_positions[key] = sorted_positions\n",
    "\n",
    "            # Set the corresponding elements to 1\n",
    "            threshold_tensor = torch.zeros_like(tensor_ranks)\n",
    "            threshold_tensor[tensor_ranks < threshold_index] = 1\n",
    "            threshold_tensor = threshold_tensor.reshape(tensor.shape)\n",
    "            hard_dict[key] = threshold_tensor\n",
    "            start_index += num_elements\n",
    "\n",
    "        torch.save(hard_dict, f'./save/{args.dataset}/{args.model_name}/mask_threshold_{seed}_{i}.pt')\n",
    "\n",
    "\n",
    "args.unlearn_lr=0.01\n",
    "args.momentum=0.9\n",
    "args.weight_decay=5e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    save_gradient_ratio(unlearn_train_loader, model, criterion, args, args.seeds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490, 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8260, Acc_r: 0.9326\n",
      "[train] epoch 0, batch 10, loss 7.825351715087891\n",
      "0.8770, 0.9790, 0.9350, 0.9340, 0.8280, 0.9550, [0.5690], 0.9850, 0.9910, 0.9600, Acc_f: 0.5690, Acc_r: 0.9382\n",
      "[train] epoch 1, batch 10, loss 4.5068488121032715\n",
      "0.8540, 0.9760, 0.8840, 0.9220, 0.7520, 0.9450, [0.3040], 0.9850, 0.9920, 0.9620, Acc_f: 0.3040, Acc_r: 0.9191\n",
      "[train] epoch 2, batch 10, loss 3.8723034858703613\n",
      "0.8230, 0.9770, 0.8670, 0.9190, 0.7440, 0.9440, [0.2000], 0.9850, 0.9920, 0.9630, Acc_f: 0.2000, Acc_r: 0.9127\n",
      "[train] epoch 3, batch 10, loss 3.56961989402771\n",
      "0.8110, 0.9790, 0.8650, 0.9230, 0.7530, 0.9420, [0.1510], 0.9850, 0.9880, 0.9620, Acc_f: 0.1510, Acc_r: 0.9120\n",
      "[train] epoch 4, batch 10, loss 3.3138904571533203\n",
      "0.8090, 0.9810, 0.8650, 0.9250, 0.7560, 0.9430, [0.1110], 0.9850, 0.9860, 0.9620, Acc_f: 0.1110, Acc_r: 0.9124\n",
      "[train] epoch 5, batch 10, loss 3.21722412109375\n",
      "0.8040, 0.9830, 0.8630, 0.9280, 0.7550, 0.9450, [0.0770], 0.9840, 0.9860, 0.9630, Acc_f: 0.0770, Acc_r: 0.9123\n",
      "[train] epoch 6, batch 10, loss 3.0761637687683105\n",
      "0.8000, 0.9830, 0.8590, 0.9330, 0.7560, 0.9490, [0.0630], 0.9790, 0.9860, 0.9640, Acc_f: 0.0630, Acc_r: 0.9121\n",
      "[train] epoch 7, batch 10, loss 3.0114941596984863\n",
      "0.8010, 0.9840, 0.8620, 0.9360, 0.7600, 0.9500, [0.0570], 0.9760, 0.9860, 0.9650, Acc_f: 0.0570, Acc_r: 0.9133\n",
      "[train] epoch 8, batch 10, loss 2.8338582515716553\n",
      "0.8040, 0.9850, 0.8580, 0.9380, 0.7630, 0.9500, [0.0480], 0.9750, 0.9860, 0.9660, Acc_f: 0.0480, Acc_r: 0.9139\n",
      "[train] epoch 9, batch 10, loss 3.016874074935913\n",
      "0.8030, 0.9850, 0.8620, 0.9410, 0.7670, 0.9530, [0.0410], 0.9730, 0.9860, 0.9710, Acc_f: 0.0410, Acc_r: 0.9157\n",
      "[train] epoch 10, batch 10, loss 2.8974809646606445\n",
      "0.8070, 0.9870, 0.8640, 0.9420, 0.7670, 0.9540, [0.0390], 0.9690, 0.9860, 0.9710, Acc_f: 0.0390, Acc_r: 0.9163\n",
      "[train] epoch 11, batch 10, loss 2.8068790435791016\n",
      "0.8080, 0.9870, 0.8650, 0.9420, 0.7670, 0.9530, [0.0360], 0.9650, 0.9860, 0.9710, Acc_f: 0.0360, Acc_r: 0.9160\n",
      "[train] epoch 12, batch 10, loss 2.8499276638031006\n",
      "0.8120, 0.9870, 0.8640, 0.9420, 0.7710, 0.9530, [0.0260], 0.9640, 0.9860, 0.9730, Acc_f: 0.0260, Acc_r: 0.9169\n",
      "[train] epoch 13, batch 10, loss 2.760056257247925\n",
      "0.8130, 0.9870, 0.8610, 0.9420, 0.7700, 0.9540, [0.0210], 0.9640, 0.9850, 0.9740, Acc_f: 0.0210, Acc_r: 0.9167\n",
      "[train] epoch 14, batch 10, loss 2.835183620452881\n",
      "0.8150, 0.9870, 0.8580, 0.9430, 0.7710, 0.9540, [0.0210], 0.9640, 0.9850, 0.9750, Acc_f: 0.0210, Acc_r: 0.9169\n",
      "[train] epoch 15, batch 10, loss 2.6521310806274414\n",
      "0.8200, 0.9870, 0.8570, 0.9440, 0.7710, 0.9530, [0.0200], 0.9640, 0.9850, 0.9760, Acc_f: 0.0200, Acc_r: 0.9174\n",
      "[train] epoch 16, batch 10, loss 2.7488434314727783\n",
      "0.8220, 0.9870, 0.8580, 0.9440, 0.7680, 0.9540, [0.0160], 0.9620, 0.9840, 0.9760, Acc_f: 0.0160, Acc_r: 0.9172\n",
      "[train] epoch 17, batch 10, loss 2.5836217403411865\n",
      "0.8210, 0.9870, 0.8540, 0.9440, 0.7690, 0.9530, [0.0150], 0.9590, 0.9840, 0.9760, Acc_f: 0.0150, Acc_r: 0.9163\n",
      "[train] epoch 18, batch 10, loss 2.5775020122528076\n",
      "0.8240, 0.9870, 0.8550, 0.9440, 0.7670, 0.9530, [0.0130], 0.9570, 0.9840, 0.9760, Acc_f: 0.0130, Acc_r: 0.9163\n",
      "[train] epoch 19, batch 10, loss 2.6694467067718506\n",
      "0.8230, 0.9870, 0.8540, 0.9430, 0.7660, 0.9520, [0.0110], 0.9550, 0.9840, 0.9760, Acc_f: 0.0110, Acc_r: 0.9156\n",
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.8460, 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8120, Acc_r: 0.9394\n",
      "[train] epoch 0, batch 10, loss 7.855729103088379\n",
      "0.8870, 0.9840, 0.9530, 0.9350, 0.8890, 0.9590, [0.5380], 0.9870, 0.9900, 0.9680, Acc_f: 0.5380, Acc_r: 0.9502\n",
      "[train] epoch 1, batch 10, loss 4.657577991485596\n",
      "0.8870, 0.9850, 0.9510, 0.9150, 0.8240, 0.9540, [0.2770], 0.9870, 0.9920, 0.9660, Acc_f: 0.2770, Acc_r: 0.9401\n",
      "[train] epoch 2, batch 10, loss 3.8003270626068115\n",
      "0.8760, 0.9820, 0.9360, 0.9120, 0.7880, 0.9500, [0.1510], 0.9870, 0.9910, 0.9640, Acc_f: 0.1510, Acc_r: 0.9318\n",
      "[train] epoch 3, batch 10, loss 3.312488317489624\n",
      "0.8610, 0.9830, 0.9140, 0.9010, 0.7560, 0.9510, [0.0970], 0.9870, 0.9890, 0.9640, Acc_f: 0.0970, Acc_r: 0.9229\n",
      "[train] epoch 4, batch 10, loss 3.220288038253784\n",
      "0.8600, 0.9840, 0.9040, 0.8960, 0.7420, 0.9530, [0.0720], 0.9870, 0.9870, 0.9670, Acc_f: 0.0720, Acc_r: 0.9200\n",
      "[train] epoch 5, batch 10, loss 3.0870211124420166\n",
      "0.8490, 0.9850, 0.8930, 0.8970, 0.7230, 0.9570, [0.0580], 0.9850, 0.9860, 0.9700, Acc_f: 0.0580, Acc_r: 0.9161\n",
      "[train] epoch 6, batch 10, loss 2.9498698711395264\n",
      "0.8430, 0.9850, 0.8880, 0.8990, 0.7160, 0.9590, [0.0480], 0.9810, 0.9860, 0.9730, Acc_f: 0.0480, Acc_r: 0.9144\n",
      "[train] epoch 7, batch 10, loss 3.1195435523986816\n",
      "0.8410, 0.9850, 0.8860, 0.9030, 0.7120, 0.9610, [0.0410], 0.9770, 0.9850, 0.9740, Acc_f: 0.0410, Acc_r: 0.9138\n",
      "[train] epoch 8, batch 10, loss 2.9102022647857666\n",
      "0.8390, 0.9860, 0.8820, 0.9020, 0.7050, 0.9600, [0.0320], 0.9760, 0.9850, 0.9760, Acc_f: 0.0320, Acc_r: 0.9123\n",
      "[train] epoch 9, batch 10, loss 2.8438172340393066\n",
      "0.8360, 0.9860, 0.8780, 0.9040, 0.6990, 0.9590, [0.0290], 0.9730, 0.9850, 0.9770, Acc_f: 0.0290, Acc_r: 0.9108\n",
      "[train] epoch 10, batch 10, loss 2.811492919921875\n",
      "0.8390, 0.9860, 0.8810, 0.9070, 0.7030, 0.9600, [0.0250], 0.9710, 0.9850, 0.9790, Acc_f: 0.0250, Acc_r: 0.9123\n",
      "[train] epoch 11, batch 10, loss 2.6728177070617676\n",
      "0.8410, 0.9860, 0.8850, 0.9090, 0.7020, 0.9600, [0.0190], 0.9700, 0.9850, 0.9790, Acc_f: 0.0190, Acc_r: 0.9130\n",
      "[train] epoch 12, batch 10, loss 2.762984037399292\n",
      "0.8420, 0.9860, 0.8820, 0.9090, 0.6980, 0.9610, [0.0160], 0.9690, 0.9850, 0.9790, Acc_f: 0.0160, Acc_r: 0.9123\n",
      "[train] epoch 13, batch 10, loss 2.732935667037964\n",
      "0.8400, 0.9860, 0.8800, 0.9090, 0.6850, 0.9590, [0.0130], 0.9670, 0.9850, 0.9800, Acc_f: 0.0130, Acc_r: 0.9101\n",
      "[train] epoch 14, batch 10, loss 2.5553812980651855\n",
      "0.8400, 0.9860, 0.8790, 0.9120, 0.6820, 0.9590, [0.0130], 0.9660, 0.9850, 0.9800, Acc_f: 0.0130, Acc_r: 0.9099\n",
      "[train] epoch 15, batch 10, loss 2.6997196674346924\n",
      "0.8370, 0.9860, 0.8790, 0.9130, 0.6780, 0.9570, [0.0080], 0.9660, 0.9850, 0.9800, Acc_f: 0.0080, Acc_r: 0.9090\n",
      "[train] epoch 16, batch 10, loss 2.5723183155059814\n",
      "0.8380, 0.9860, 0.8790, 0.9130, 0.6740, 0.9560, [0.0080], 0.9650, 0.9850, 0.9800, Acc_f: 0.0080, Acc_r: 0.9084\n",
      "[train] epoch 17, batch 10, loss 2.7134504318237305\n",
      "0.8380, 0.9860, 0.8790, 0.9120, 0.6720, 0.9570, [0.0060], 0.9640, 0.9840, 0.9810, Acc_f: 0.0060, Acc_r: 0.9081\n",
      "[train] epoch 18, batch 10, loss 2.706233501434326\n",
      "0.8370, 0.9860, 0.8800, 0.9120, 0.6760, 0.9560, [0.0060], 0.9630, 0.9840, 0.9810, Acc_f: 0.0060, Acc_r: 0.9083\n",
      "[train] epoch 19, batch 10, loss 2.6058285236358643\n",
      "0.8350, 0.9860, 0.8780, 0.9120, 0.6770, 0.9560, [0.0050], 0.9630, 0.9840, 0.9820, Acc_f: 0.0050, Acc_r: 0.9081\n",
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.8580, 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.7910, Acc_r: 0.9353\n",
      "[train] epoch 0, batch 10, loss 8.033873558044434\n",
      "0.8710, 0.9780, 0.9110, 0.9450, 0.8770, 0.9550, [0.7200], 0.9820, 0.9860, 0.9610, Acc_f: 0.7200, Acc_r: 0.9407\n",
      "[train] epoch 1, batch 10, loss 5.497828006744385\n",
      "0.8830, 0.9810, 0.9230, 0.9470, 0.8810, 0.9510, [0.6120], 0.9830, 0.9910, 0.9610, Acc_f: 0.6120, Acc_r: 0.9446\n",
      "[train] epoch 2, batch 10, loss 5.0681657791137695\n",
      "0.8910, 0.9820, 0.9230, 0.9450, 0.8850, 0.9480, [0.5180], 0.9840, 0.9920, 0.9620, Acc_f: 0.5180, Acc_r: 0.9458\n",
      "[train] epoch 3, batch 10, loss 4.358201026916504\n",
      "0.8870, 0.9840, 0.9180, 0.9390, 0.8780, 0.9400, [0.4120], 0.9830, 0.9940, 0.9630, Acc_f: 0.4120, Acc_r: 0.9429\n",
      "[train] epoch 4, batch 10, loss 3.9643561840057373\n",
      "0.8810, 0.9830, 0.9090, 0.9370, 0.8700, 0.9250, [0.3240], 0.9830, 0.9940, 0.9630, Acc_f: 0.3240, Acc_r: 0.9383\n",
      "[train] epoch 5, batch 10, loss 3.5203051567077637\n",
      "0.8650, 0.9830, 0.8910, 0.9320, 0.8630, 0.9130, [0.2560], 0.9830, 0.9960, 0.9630, Acc_f: 0.2560, Acc_r: 0.9321\n",
      "[train] epoch 6, batch 10, loss 3.485337257385254\n",
      "0.8500, 0.9830, 0.8710, 0.9270, 0.8490, 0.9070, [0.2110], 0.9830, 0.9950, 0.9640, Acc_f: 0.2110, Acc_r: 0.9254\n",
      "[train] epoch 7, batch 10, loss 3.3184611797332764\n",
      "0.8450, 0.9840, 0.8490, 0.9190, 0.8410, 0.9020, [0.1630], 0.9830, 0.9940, 0.9660, Acc_f: 0.1630, Acc_r: 0.9203\n",
      "[train] epoch 8, batch 10, loss 3.3197078704833984\n",
      "0.8320, 0.9840, 0.8350, 0.9140, 0.8370, 0.8990, [0.1350], 0.9820, 0.9940, 0.9660, Acc_f: 0.1350, Acc_r: 0.9159\n",
      "[train] epoch 9, batch 10, loss 3.047060966491699\n",
      "0.8310, 0.9840, 0.8300, 0.9100, 0.8340, 0.8990, [0.1120], 0.9790, 0.9940, 0.9660, Acc_f: 0.1120, Acc_r: 0.9141\n",
      "[train] epoch 10, batch 10, loss 3.1439616680145264\n",
      "0.8250, 0.9850, 0.8230, 0.9060, 0.8270, 0.9000, [0.0990], 0.9790, 0.9940, 0.9660, Acc_f: 0.0990, Acc_r: 0.9117\n",
      "[train] epoch 11, batch 10, loss 2.888875961303711\n",
      "0.8260, 0.9850, 0.8150, 0.9070, 0.8250, 0.9010, [0.0880], 0.9790, 0.9940, 0.9660, Acc_f: 0.0880, Acc_r: 0.9109\n",
      "[train] epoch 12, batch 10, loss 2.90915846824646\n",
      "0.8260, 0.9870, 0.8150, 0.9070, 0.8250, 0.9030, [0.0750], 0.9770, 0.9940, 0.9670, Acc_f: 0.0750, Acc_r: 0.9112\n",
      "[train] epoch 13, batch 10, loss 2.8626089096069336\n",
      "0.8250, 0.9870, 0.8070, 0.9070, 0.8220, 0.9040, [0.0660], 0.9760, 0.9940, 0.9670, Acc_f: 0.0660, Acc_r: 0.9099\n",
      "[train] epoch 14, batch 10, loss 2.798062562942505\n",
      "0.8250, 0.9880, 0.8060, 0.9070, 0.8210, 0.9080, [0.0570], 0.9750, 0.9940, 0.9680, Acc_f: 0.0570, Acc_r: 0.9102\n",
      "[train] epoch 15, batch 10, loss 2.9196128845214844\n",
      "0.8250, 0.9880, 0.8050, 0.9120, 0.8240, 0.9080, [0.0550], 0.9740, 0.9940, 0.9680, Acc_f: 0.0550, Acc_r: 0.9109\n",
      "[train] epoch 16, batch 10, loss 2.789137601852417\n",
      "0.8240, 0.9880, 0.7970, 0.9120, 0.8210, 0.9090, [0.0510], 0.9730, 0.9940, 0.9690, Acc_f: 0.0510, Acc_r: 0.9097\n",
      "[train] epoch 17, batch 10, loss 2.701045274734497\n",
      "0.8240, 0.9880, 0.7970, 0.9140, 0.8210, 0.9110, [0.0430], 0.9710, 0.9940, 0.9690, Acc_f: 0.0430, Acc_r: 0.9099\n",
      "[train] epoch 18, batch 10, loss 2.872138500213623\n",
      "0.8200, 0.9890, 0.7950, 0.9190, 0.8170, 0.9130, [0.0410], 0.9710, 0.9940, 0.9690, Acc_f: 0.0410, Acc_r: 0.9097\n",
      "[train] epoch 19, batch 10, loss 2.7488110065460205\n",
      "0.8210, 0.9890, 0.7910, 0.9190, 0.8170, 0.9150, [0.0380], 0.9710, 0.9940, 0.9700, Acc_f: 0.0380, Acc_r: 0.9097\n"
     ]
    }
   ],
   "source": [
    "from agents.svc_mia import SVC_MIA\n",
    "indice = remain_train_loader.sampler.indices\n",
    "neg_size = len(test_loader.sampler) + len(val_loader.sampler.indices)\n",
    "balanced_indice = np.random.choice(indice, size=neg_size, replace=False)\n",
    "balanced_sampler = torch.utils.data.SubsetRandomSampler(balanced_indice)\n",
    "balanced_train_loader = torch.utils.data.DataLoader(remain_train_loader.dataset,\n",
    "                                                    batch_size=args.batch_size,\n",
    "                                                    sampler=balanced_sampler)\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"=======\"*50)\n",
    "    mask = torch.load(f'./save/{args.dataset}/{args.model_name}/mask_threshold_{args.seeds[i]}_{threshold}.pt')\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(20):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for name, param in sgd_mr_model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= mask[name]\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalUn Acc_f: 1.80 \\pm 1.44\n",
      "SalUn Acc_r: 91.11 \\pm 0.32\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9156, 0.9081, 0.9097])\n",
    "Acc_f = 100*np.array([0.0110, 0.0050, 0.0380])\n",
    "\n",
    "print(f'SalUn Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'SalUn Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8375, Acc_r: 0.9430\n",
      "[0.8460], 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8290, Acc_r: 0.9511\n",
      "[0.8580], 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.8245, Acc_r: 0.9450\n",
      "------------ Retrained model ------------\n",
      "[0.0000], 0.9820, 0.9530, 0.9600, 0.9170, 0.9430, [0.0000], 0.9860, 0.9880, 0.9620, Acc_f: 0.0000, Acc_r: 0.9614\n",
      "[0.0000], 0.9830, 0.9600, 0.9550, 0.9130, 0.9420, [0.0000], 0.9840, 0.9890, 0.9640, Acc_f: 0.0000, Acc_r: 0.9612\n",
      "[0.0000], 0.9800, 0.9520, 0.9540, 0.9160, 0.9570, [0.0000], 0.9820, 0.9900, 0.9660, Acc_f: 0.0000, Acc_r: 0.9621\n",
      "Original model Acc_f: 83.03 \\pm 0.54\n",
      "Original model Acc_r: 94.64 \\pm 0.35\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 96.16 \\pm 0.04\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 2\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 2/16\n",
      "Layer 2 : 52/576\n",
      "Layer 3 : 224/512\n",
      "Layer 4 : 71/1024\n",
      "Layer 5 : 69/2048\n",
      "Layer 6 : 3/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 130/576\n",
      "Layer 3 : 366/512\n",
      "Layer 4 : 165/1024\n",
      "Layer 5 : 211/2048\n",
      "Layer 6 : 8/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 148/576\n",
      "Layer 3 : 421/512\n",
      "Layer 4 : 260/1024\n",
      "Layer 5 : 371/2048\n",
      "Layer 6 : 13/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/16\n",
      "Layer 2 : 175/576\n",
      "Layer 3 : 431/512\n",
      "Layer 4 : 353/1024\n",
      "Layer 5 : 532/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 240/576\n",
      "Layer 3 : 471/512\n",
      "Layer 4 : 450/1024\n",
      "Layer 5 : 648/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 271/576\n",
      "Layer 3 : 483/512\n",
      "Layer 4 : 547/1024\n",
      "Layer 5 : 747/2048\n",
      "Layer 6 : 22/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 318/576\n",
      "Layer 3 : 489/512\n",
      "Layer 4 : 647/1024\n",
      "Layer 5 : 911/2048\n",
      "Layer 6 : 37/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 373/576\n",
      "Layer 3 : 500/512\n",
      "Layer 4 : 747/1024\n",
      "Layer 5 : 1072/2048\n",
      "Layer 6 : 60/2048\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.973\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 2/16\n",
      "Layer 2 : 60/576\n",
      "Layer 3 : 250/512\n",
      "Layer 4 : 79/1024\n",
      "Layer 5 : 67/2048\n",
      "Layer 6 : 3/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 139/576\n",
      "Layer 3 : 378/512\n",
      "Layer 4 : 175/1024\n",
      "Layer 5 : 211/2048\n",
      "Layer 6 : 8/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 152/576\n",
      "Layer 3 : 424/512\n",
      "Layer 4 : 272/1024\n",
      "Layer 5 : 368/2048\n",
      "Layer 6 : 13/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/16\n",
      "Layer 2 : 180/576\n",
      "Layer 3 : 435/512\n",
      "Layer 4 : 366/1024\n",
      "Layer 5 : 524/2048\n",
      "Layer 6 : 15/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 236/576\n",
      "Layer 3 : 472/512\n",
      "Layer 4 : 464/1024\n",
      "Layer 5 : 638/2048\n",
      "Layer 6 : 17/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 270/576\n",
      "Layer 3 : 485/512\n",
      "Layer 4 : 560/1024\n",
      "Layer 5 : 730/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 331/576\n",
      "Layer 3 : 491/512\n",
      "Layer 4 : 663/1024\n",
      "Layer 5 : 898/2048\n",
      "Layer 6 : 36/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 372/576\n",
      "Layer 3 : 500/512\n",
      "Layer 4 : 762/1024\n",
      "Layer 5 : 1052/2048\n",
      "Layer 6 : 56/2048\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.973\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 2/16\n",
      "Layer 2 : 47/576\n",
      "Layer 3 : 194/512\n",
      "Layer 4 : 61/1024\n",
      "Layer 5 : 37/2048\n",
      "Layer 6 : 3/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 117/576\n",
      "Layer 3 : 315/512\n",
      "Layer 4 : 141/1024\n",
      "Layer 5 : 138/2048\n",
      "Layer 6 : 10/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 131/576\n",
      "Layer 3 : 376/512\n",
      "Layer 4 : 227/1024\n",
      "Layer 5 : 263/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.982\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 158/576\n",
      "Layer 3 : 396/512\n",
      "Layer 4 : 309/1024\n",
      "Layer 5 : 377/2048\n",
      "Layer 6 : 20/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 220/576\n",
      "Layer 3 : 455/512\n",
      "Layer 4 : 403/1024\n",
      "Layer 5 : 477/2048\n",
      "Layer 6 : 23/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 256/576\n",
      "Layer 3 : 472/512\n",
      "Layer 4 : 493/1024\n",
      "Layer 5 : 553/2048\n",
      "Layer 6 : 25/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 309/576\n",
      "Layer 3 : 480/512\n",
      "Layer 4 : 590/1024\n",
      "Layer 5 : 701/2048\n",
      "Layer 6 : 40/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 354/576\n",
      "Layer 3 : 493/512\n",
      "Layer 4 : 683/1024\n",
      "Layer 5 : 832/2048\n",
      "Layer 6 : 59/2048\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.targets)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100,  125, 125, 250,  256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], 0.9780, 0.9090, 0.9310, 0.8380, 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8375, Acc_r: 0.9430\n",
      "[train] epoch 0, batch 21, loss 2.983308792114258\n",
      "[0.2620], 0.9790, 0.9590, 0.9310, 0.8840, 0.9510, [0.6050], 0.9840, 0.9880, 0.9630, Acc_f: 0.4335, Acc_r: 0.9549\n",
      "[train] epoch 1, batch 21, loss 1.239094614982605\n",
      "[0.0970], 0.9790, 0.9700, 0.9350, 0.8890, 0.9440, [0.3320], 0.9850, 0.9890, 0.9630, Acc_f: 0.2145, Acc_r: 0.9567\n",
      "[train] epoch 2, batch 21, loss 0.6775612235069275\n",
      "[0.0550], 0.9800, 0.9670, 0.9400, 0.8870, 0.9370, [0.1860], 0.9830, 0.9900, 0.9640, Acc_f: 0.1205, Acc_r: 0.9560\n",
      "[train] epoch 3, batch 21, loss 0.5368953347206116\n",
      "[0.0330], 0.9790, 0.9630, 0.9440, 0.8800, 0.9310, [0.1210], 0.9830, 0.9900, 0.9660, Acc_f: 0.0770, Acc_r: 0.9545\n",
      "[train] epoch 4, batch 21, loss 0.5600632429122925\n",
      "[0.0230], 0.9790, 0.9620, 0.9460, 0.8770, 0.9280, [0.0650], 0.9820, 0.9900, 0.9660, Acc_f: 0.0440, Acc_r: 0.9537\n",
      "[train] epoch 5, batch 21, loss 0.289654403924942\n",
      "[0.0180], 0.9780, 0.9630, 0.9500, 0.8750, 0.9260, [0.0520], 0.9820, 0.9910, 0.9660, Acc_f: 0.0350, Acc_r: 0.9539\n",
      "[train] epoch 6, batch 21, loss 0.44652241468429565\n",
      "[0.0150], 0.9770, 0.9630, 0.9510, 0.8760, 0.9260, [0.0440], 0.9820, 0.9910, 0.9670, Acc_f: 0.0295, Acc_r: 0.9541\n",
      "[train] epoch 7, batch 21, loss 0.4708428680896759\n",
      "[0.0130], 0.9760, 0.9630, 0.9510, 0.8740, 0.9250, [0.0350], 0.9820, 0.9910, 0.9670, Acc_f: 0.0240, Acc_r: 0.9536\n",
      "[train] epoch 8, batch 21, loss 0.4323054552078247\n",
      "[0.0110], 0.9760, 0.9640, 0.9510, 0.8720, 0.9250, [0.0290], 0.9820, 0.9910, 0.9670, Acc_f: 0.0200, Acc_r: 0.9535\n",
      "[train] epoch 9, batch 21, loss 0.3385828733444214\n",
      "[0.0110], 0.9760, 0.9640, 0.9510, 0.8720, 0.9230, [0.0260], 0.9820, 0.9910, 0.9670, Acc_f: 0.0185, Acc_r: 0.9532\n",
      "[train] epoch 10, batch 21, loss 0.5126517415046692\n",
      "[0.0090], 0.9760, 0.9650, 0.9510, 0.8720, 0.9230, [0.0230], 0.9820, 0.9910, 0.9670, Acc_f: 0.0160, Acc_r: 0.9534\n",
      "[train] epoch 11, batch 21, loss 0.41121193766593933\n",
      "[0.0070], 0.9760, 0.9650, 0.9510, 0.8730, 0.9230, [0.0180], 0.9820, 0.9910, 0.9670, Acc_f: 0.0125, Acc_r: 0.9535\n",
      "[train] epoch 12, batch 21, loss 0.5287587642669678\n",
      "[0.0070], 0.9760, 0.9650, 0.9530, 0.8730, 0.9230, [0.0150], 0.9820, 0.9910, 0.9670, Acc_f: 0.0110, Acc_r: 0.9537\n",
      "[train] epoch 13, batch 21, loss 0.3610524833202362\n",
      "[0.0060], 0.9760, 0.9640, 0.9510, 0.8740, 0.9230, [0.0120], 0.9820, 0.9910, 0.9670, Acc_f: 0.0090, Acc_r: 0.9535\n",
      "[train] epoch 14, batch 21, loss 0.23067958652973175\n",
      "[0.0050], 0.9760, 0.9650, 0.9500, 0.8740, 0.9240, [0.0110], 0.9820, 0.9910, 0.9670, Acc_f: 0.0080, Acc_r: 0.9536\n",
      "[0.8460], 0.9820, 0.9160, 0.9370, 0.8760, 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8290, Acc_r: 0.9511\n",
      "[train] epoch 0, batch 21, loss 2.353658676147461\n",
      "[0.0720], 0.9800, 0.9690, 0.8740, 0.9120, 0.9480, [0.3050], 0.9910, 0.9850, 0.9640, Acc_f: 0.1885, Acc_r: 0.9529\n",
      "[train] epoch 1, batch 21, loss 0.7164808511734009\n",
      "[0.0470], 0.9800, 0.9640, 0.9030, 0.9160, 0.9460, [0.1200], 0.9910, 0.9880, 0.9610, Acc_f: 0.0835, Acc_r: 0.9561\n",
      "[train] epoch 2, batch 21, loss 0.5656227469444275\n",
      "[0.0290], 0.9800, 0.9620, 0.9300, 0.9070, 0.9430, [0.0610], 0.9910, 0.9880, 0.9600, Acc_f: 0.0450, Acc_r: 0.9576\n",
      "[train] epoch 3, batch 21, loss 0.41773760318756104\n",
      "[0.0150], 0.9800, 0.9640, 0.9390, 0.8950, 0.9440, [0.0380], 0.9900, 0.9860, 0.9590, Acc_f: 0.0265, Acc_r: 0.9571\n",
      "[train] epoch 4, batch 21, loss 0.6014023423194885\n",
      "[0.0060], 0.9800, 0.9640, 0.9390, 0.8950, 0.9430, [0.0240], 0.9900, 0.9860, 0.9600, Acc_f: 0.0150, Acc_r: 0.9571\n",
      "[train] epoch 5, batch 21, loss 0.4417436122894287\n",
      "[0.0030], 0.9800, 0.9630, 0.9410, 0.8960, 0.9430, [0.0210], 0.9900, 0.9860, 0.9600, Acc_f: 0.0120, Acc_r: 0.9574\n",
      "[train] epoch 6, batch 21, loss 0.49425607919692993\n",
      "[0.0010], 0.9800, 0.9620, 0.9400, 0.8970, 0.9420, [0.0170], 0.9900, 0.9850, 0.9600, Acc_f: 0.0090, Acc_r: 0.9570\n",
      "[train] epoch 7, batch 21, loss 0.4540104269981384\n",
      "[0.0010], 0.9800, 0.9620, 0.9390, 0.8960, 0.9420, [0.0140], 0.9900, 0.9850, 0.9600, Acc_f: 0.0075, Acc_r: 0.9567\n",
      "[train] epoch 8, batch 21, loss 0.38059234619140625\n",
      "[0.0000], 0.9800, 0.9620, 0.9400, 0.8990, 0.9420, [0.0140], 0.9900, 0.9850, 0.9600, Acc_f: 0.0070, Acc_r: 0.9572\n",
      "[train] epoch 9, batch 21, loss 0.4824222922325134\n",
      "[0.0000], 0.9800, 0.9620, 0.9410, 0.9010, 0.9420, [0.0130], 0.9900, 0.9850, 0.9600, Acc_f: 0.0065, Acc_r: 0.9576\n",
      "[train] epoch 10, batch 21, loss 0.4032033085823059\n",
      "[0.0000], 0.9800, 0.9620, 0.9420, 0.9040, 0.9420, [0.0100], 0.9900, 0.9850, 0.9600, Acc_f: 0.0050, Acc_r: 0.9581\n",
      "[train] epoch 11, batch 21, loss 0.46453890204429626\n",
      "[0.0000], 0.9800, 0.9620, 0.9440, 0.9030, 0.9410, [0.0090], 0.9900, 0.9850, 0.9600, Acc_f: 0.0045, Acc_r: 0.9581\n",
      "[train] epoch 12, batch 21, loss 0.24980388581752777\n",
      "[0.0000], 0.9800, 0.9620, 0.9430, 0.9020, 0.9410, [0.0090], 0.9900, 0.9850, 0.9600, Acc_f: 0.0045, Acc_r: 0.9579\n",
      "[train] epoch 13, batch 21, loss 0.3365212678909302\n",
      "[0.0000], 0.9800, 0.9610, 0.9440, 0.9050, 0.9410, [0.0090], 0.9900, 0.9850, 0.9600, Acc_f: 0.0045, Acc_r: 0.9583\n",
      "[train] epoch 14, batch 21, loss 0.4831351637840271\n",
      "[0.0000], 0.9800, 0.9640, 0.9430, 0.9050, 0.9410, [0.0090], 0.9900, 0.9850, 0.9600, Acc_f: 0.0045, Acc_r: 0.9585\n",
      "[0.8580], 0.9760, 0.8940, 0.9400, 0.8640, 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.8245, Acc_r: 0.9450\n",
      "[train] epoch 0, batch 21, loss 2.9642891883850098\n",
      "[0.6250], 0.9750, 0.9230, 0.9300, 0.9040, 0.9510, [0.6710], 0.9810, 0.9850, 0.9640, Acc_f: 0.6480, Acc_r: 0.9516\n",
      "[train] epoch 1, batch 21, loss 1.2237685918807983\n",
      "[0.2070], 0.9750, 0.9340, 0.9130, 0.9180, 0.9440, [0.4620], 0.9820, 0.9840, 0.9640, Acc_f: 0.3345, Acc_r: 0.9517\n",
      "[train] epoch 2, batch 21, loss 0.9152353405952454\n",
      "[0.0980], 0.9740, 0.9360, 0.9130, 0.9240, 0.9410, [0.3090], 0.9820, 0.9820, 0.9650, Acc_f: 0.2035, Acc_r: 0.9521\n",
      "[train] epoch 3, batch 21, loss 0.8927108645439148\n",
      "[0.0610], 0.9740, 0.9380, 0.9130, 0.9240, 0.9400, [0.2090], 0.9820, 0.9810, 0.9650, Acc_f: 0.1350, Acc_r: 0.9521\n",
      "[train] epoch 4, batch 21, loss 0.574057400226593\n",
      "[0.0440], 0.9740, 0.9390, 0.9230, 0.9210, 0.9360, [0.1440], 0.9810, 0.9810, 0.9630, Acc_f: 0.0940, Acc_r: 0.9522\n",
      "[train] epoch 5, batch 21, loss 0.6772652268409729\n",
      "[0.0250], 0.9750, 0.9380, 0.9270, 0.9190, 0.9360, [0.0990], 0.9810, 0.9810, 0.9630, Acc_f: 0.0620, Acc_r: 0.9525\n",
      "[train] epoch 6, batch 21, loss 0.5230110287666321\n",
      "[0.0210], 0.9740, 0.9390, 0.9300, 0.9170, 0.9320, [0.0760], 0.9820, 0.9800, 0.9630, Acc_f: 0.0485, Acc_r: 0.9521\n",
      "[train] epoch 7, batch 21, loss 0.5186712145805359\n",
      "[0.0190], 0.9720, 0.9400, 0.9340, 0.9140, 0.9300, [0.0630], 0.9820, 0.9800, 0.9630, Acc_f: 0.0410, Acc_r: 0.9519\n",
      "[train] epoch 8, batch 21, loss 0.49774226546287537\n",
      "[0.0170], 0.9720, 0.9410, 0.9350, 0.9130, 0.9300, [0.0500], 0.9820, 0.9800, 0.9640, Acc_f: 0.0335, Acc_r: 0.9521\n",
      "[train] epoch 9, batch 21, loss 0.4857568144798279\n",
      "[0.0160], 0.9720, 0.9410, 0.9390, 0.9110, 0.9270, [0.0380], 0.9820, 0.9790, 0.9640, Acc_f: 0.0270, Acc_r: 0.9519\n",
      "[train] epoch 10, batch 21, loss 0.3692178428173065\n",
      "[0.0150], 0.9710, 0.9410, 0.9420, 0.9080, 0.9260, [0.0350], 0.9810, 0.9790, 0.9640, Acc_f: 0.0250, Acc_r: 0.9515\n",
      "[train] epoch 11, batch 21, loss 0.33209556341171265\n",
      "[0.0110], 0.9710, 0.9410, 0.9420, 0.9030, 0.9250, [0.0260], 0.9800, 0.9790, 0.9640, Acc_f: 0.0185, Acc_r: 0.9506\n",
      "[train] epoch 12, batch 21, loss 0.4825897216796875\n",
      "[0.0100], 0.9710, 0.9410, 0.9420, 0.9010, 0.9250, [0.0230], 0.9800, 0.9780, 0.9640, Acc_f: 0.0165, Acc_r: 0.9503\n",
      "[train] epoch 13, batch 21, loss 0.34464749693870544\n",
      "[0.0080], 0.9710, 0.9410, 0.9420, 0.8990, 0.9250, [0.0200], 0.9800, 0.9780, 0.9640, Acc_f: 0.0140, Acc_r: 0.9500\n",
      "[train] epoch 14, batch 21, loss 0.28443580865859985\n",
      "[0.0070], 0.9710, 0.9410, 0.9410, 0.8980, 0.9230, [0.0160], 0.9800, 0.9770, 0.9630, Acc_f: 0.0115, Acc_r: 0.9492\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.80 \\pm 0.29\n",
      "UNSC Acc_r: 95.38 \\pm 0.38\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9536, 0.9585, 0.9492])\n",
    "Acc_f = 100*np.array([0.0080, 0.0045, 0.0115])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7400000000000043, 0.2612789058968726)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (100*np.array([0.9536, 0.9585, 0.9492])-100*np.array([0.9430, 0.9511, 0.9450]))\n",
    "a.mean(), a.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], 0.9780, 0.9090, 0.9310, [0.8380], 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8377, Acc_r: 0.9580\n",
      "[0.8460], 0.9820, 0.9160, 0.9370, [0.8760], 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8447, Acc_r: 0.9619\n",
      "[0.8580], 0.9760, 0.8940, 0.9400, [0.8640], 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.8377, Acc_r: 0.9566\n",
      "------------ Retrained model ------------\n",
      "[0.0000], 0.9810, 0.9850, 0.9630, [0.0000], 0.9600, [0.0000], 0.9770, 0.9910, 0.9670, Acc_f: 0.0000, Acc_r: 0.9749\n",
      "[0.0000], 0.9820, 0.9820, 0.9610, [0.0000], 0.9520, [0.0000], 0.9870, 0.9900, 0.9580, Acc_f: 0.0000, Acc_r: 0.9731\n",
      "[0.0000], 0.9830, 0.9850, 0.9670, [0.0000], 0.9470, [0.0000], 0.9860, 0.9920, 0.9630, Acc_f: 0.0000, Acc_r: 0.9747\n",
      "Original model Acc_f: 84.00 \\pm 0.33\n",
      "Original model Acc_r: 95.88 \\pm 0.22\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 97.42 \\pm 0.08\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 3\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.973\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 2/16\n",
      "Layer 2 : 53/576\n",
      "Layer 3 : 222/512\n",
      "Layer 4 : 73/1024\n",
      "Layer 5 : 70/2048\n",
      "Layer 6 : 3/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 136/576\n",
      "Layer 3 : 367/512\n",
      "Layer 4 : 166/1024\n",
      "Layer 5 : 215/2048\n",
      "Layer 6 : 9/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 149/576\n",
      "Layer 3 : 418/512\n",
      "Layer 4 : 261/1024\n",
      "Layer 5 : 378/2048\n",
      "Layer 6 : 13/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 228/576\n",
      "Layer 3 : 467/512\n",
      "Layer 4 : 361/1024\n",
      "Layer 5 : 502/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 268/576\n",
      "Layer 3 : 482/512\n",
      "Layer 4 : 463/1024\n",
      "Layer 5 : 614/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 321/576\n",
      "Layer 3 : 490/512\n",
      "Layer 4 : 570/1024\n",
      "Layer 5 : 789/2048\n",
      "Layer 6 : 36/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 370/576\n",
      "Layer 3 : 499/512\n",
      "Layer 4 : 673/1024\n",
      "Layer 5 : 951/2048\n",
      "Layer 6 : 55/2048\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.973\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 2/16\n",
      "Layer 2 : 51/576\n",
      "Layer 3 : 228/512\n",
      "Layer 4 : 77/1024\n",
      "Layer 5 : 63/2048\n",
      "Layer 6 : 2/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 138/576\n",
      "Layer 3 : 377/512\n",
      "Layer 4 : 171/1024\n",
      "Layer 5 : 208/2048\n",
      "Layer 6 : 7/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 155/576\n",
      "Layer 3 : 425/512\n",
      "Layer 4 : 266/1024\n",
      "Layer 5 : 364/2048\n",
      "Layer 6 : 12/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 234/576\n",
      "Layer 3 : 470/512\n",
      "Layer 4 : 366/1024\n",
      "Layer 5 : 488/2048\n",
      "Layer 6 : 15/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 259/576\n",
      "Layer 3 : 484/512\n",
      "Layer 4 : 465/1024\n",
      "Layer 5 : 592/2048\n",
      "Layer 6 : 17/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 321/576\n",
      "Layer 3 : 490/512\n",
      "Layer 4 : 571/1024\n",
      "Layer 5 : 765/2048\n",
      "Layer 6 : 34/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 369/576\n",
      "Layer 3 : 501/512\n",
      "Layer 4 : 677/1024\n",
      "Layer 5 : 926/2048\n",
      "Layer 6 : 53/2048\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.973\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 2/16\n",
      "Layer 2 : 45/576\n",
      "Layer 3 : 187/512\n",
      "Layer 4 : 60/1024\n",
      "Layer 5 : 32/2048\n",
      "Layer 6 : 2/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 123/576\n",
      "Layer 3 : 324/512\n",
      "Layer 4 : 142/1024\n",
      "Layer 5 : 136/2048\n",
      "Layer 6 : 10/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 136/576\n",
      "Layer 3 : 382/512\n",
      "Layer 4 : 228/1024\n",
      "Layer 5 : 260/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 215/576\n",
      "Layer 3 : 453/512\n",
      "Layer 4 : 324/1024\n",
      "Layer 5 : 367/2048\n",
      "Layer 6 : 20/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 246/576\n",
      "Layer 3 : 469/512\n",
      "Layer 4 : 417/1024\n",
      "Layer 5 : 445/2048\n",
      "Layer 6 : 22/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 308/576\n",
      "Layer 3 : 479/512\n",
      "Layer 4 : 519/1024\n",
      "Layer 5 : 604/2048\n",
      "Layer 6 : 39/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 359/576\n",
      "Layer 3 : 493/512\n",
      "Layer 4 : 618/1024\n",
      "Layer 5 : 745/2048\n",
      "Layer 6 : 59/2048\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.targets)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[100, 100,  125, 125, 250,  256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], 0.9780, 0.9090, 0.9310, [0.8380], 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8377, Acc_r: 0.9580\n",
      "[train] epoch 0, batch 31, loss 3.2238874435424805\n",
      "[0.2820], 0.9790, 0.9750, 0.9440, [0.6410], 0.9490, [0.6190], 0.9820, 0.9890, 0.9660, Acc_f: 0.5140, Acc_r: 0.9691\n",
      "[train] epoch 1, batch 31, loss 1.3306982517242432\n",
      "[0.1020], 0.9780, 0.9860, 0.9550, [0.3860], 0.9370, [0.3550], 0.9800, 0.9890, 0.9720, Acc_f: 0.2810, Acc_r: 0.9710\n",
      "[train] epoch 2, batch 31, loss 0.9686804413795471\n",
      "[0.0580], 0.9780, 0.9850, 0.9630, [0.2160], 0.9300, [0.1870], 0.9780, 0.9910, 0.9730, Acc_f: 0.1537, Acc_r: 0.9711\n",
      "[train] epoch 3, batch 31, loss 0.6143273711204529\n",
      "[0.0350], 0.9760, 0.9800, 0.9710, [0.1210], 0.9260, [0.0920], 0.9770, 0.9910, 0.9730, Acc_f: 0.0827, Acc_r: 0.9706\n",
      "[train] epoch 4, batch 31, loss 0.602303683757782\n",
      "[0.0240], 0.9750, 0.9790, 0.9720, [0.0810], 0.9240, [0.0530], 0.9760, 0.9910, 0.9730, Acc_f: 0.0527, Acc_r: 0.9700\n",
      "[train] epoch 5, batch 31, loss 0.5115252733230591\n",
      "[0.0160], 0.9750, 0.9800, 0.9730, [0.0550], 0.9220, [0.0320], 0.9760, 0.9910, 0.9730, Acc_f: 0.0343, Acc_r: 0.9700\n",
      "[train] epoch 6, batch 31, loss 0.517425537109375\n",
      "[0.0150], 0.9750, 0.9800, 0.9730, [0.0390], 0.9220, [0.0250], 0.9750, 0.9930, 0.9740, Acc_f: 0.0263, Acc_r: 0.9703\n",
      "[train] epoch 7, batch 31, loss 0.41188445687294006\n",
      "[0.0100], 0.9750, 0.9800, 0.9730, [0.0320], 0.9210, [0.0180], 0.9750, 0.9940, 0.9740, Acc_f: 0.0200, Acc_r: 0.9703\n",
      "[train] epoch 8, batch 31, loss 0.44313350319862366\n",
      "[0.0090], 0.9750, 0.9810, 0.9720, [0.0240], 0.9200, [0.0150], 0.9750, 0.9940, 0.9740, Acc_f: 0.0160, Acc_r: 0.9701\n",
      "[train] epoch 9, batch 31, loss 0.391767680644989\n",
      "[0.0070], 0.9750, 0.9820, 0.9700, [0.0200], 0.9200, [0.0150], 0.9750, 0.9940, 0.9740, Acc_f: 0.0140, Acc_r: 0.9700\n",
      "[train] epoch 10, batch 31, loss 0.32788658142089844\n",
      "[0.0070], 0.9750, 0.9830, 0.9700, [0.0160], 0.9200, [0.0130], 0.9750, 0.9940, 0.9750, Acc_f: 0.0120, Acc_r: 0.9703\n",
      "[train] epoch 11, batch 31, loss 0.4291916489601135\n",
      "[0.0060], 0.9750, 0.9830, 0.9700, [0.0130], 0.9180, [0.0120], 0.9750, 0.9940, 0.9750, Acc_f: 0.0103, Acc_r: 0.9700\n",
      "[train] epoch 12, batch 31, loss 0.4104449152946472\n",
      "[0.0050], 0.9750, 0.9830, 0.9700, [0.0110], 0.9180, [0.0080], 0.9750, 0.9940, 0.9750, Acc_f: 0.0080, Acc_r: 0.9700\n",
      "[train] epoch 13, batch 31, loss 0.4469741880893707\n",
      "[0.0050], 0.9750, 0.9840, 0.9690, [0.0090], 0.9180, [0.0070], 0.9750, 0.9940, 0.9760, Acc_f: 0.0070, Acc_r: 0.9701\n",
      "[train] epoch 14, batch 31, loss 0.3920903205871582\n",
      "[0.0050], 0.9750, 0.9840, 0.9680, [0.0080], 0.9190, [0.0040], 0.9750, 0.9940, 0.9760, Acc_f: 0.0057, Acc_r: 0.9701\n",
      "[0.8460], 0.9820, 0.9160, 0.9370, [0.8760], 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8447, Acc_r: 0.9619\n",
      "[train] epoch 0, batch 31, loss 2.094083070755005\n",
      "[0.0670], 0.9830, 0.9890, 0.9130, [0.3310], 0.9440, [0.2840], 0.9900, 0.9890, 0.9610, Acc_f: 0.2273, Acc_r: 0.9670\n",
      "[train] epoch 1, batch 31, loss 0.6969335675239563\n",
      "[0.0270], 0.9780, 0.9850, 0.9590, [0.1130], 0.9410, [0.0920], 0.9900, 0.9930, 0.9540, Acc_f: 0.0773, Acc_r: 0.9714\n",
      "[train] epoch 2, batch 31, loss 0.42578965425491333\n",
      "[0.0110], 0.9760, 0.9850, 0.9650, [0.0500], 0.9350, [0.0350], 0.9910, 0.9920, 0.9520, Acc_f: 0.0320, Acc_r: 0.9709\n",
      "[train] epoch 3, batch 31, loss 0.49943846464157104\n",
      "[0.0060], 0.9790, 0.9850, 0.9650, [0.0250], 0.9310, [0.0230], 0.9910, 0.9910, 0.9520, Acc_f: 0.0180, Acc_r: 0.9706\n",
      "[train] epoch 4, batch 31, loss 0.4115030765533447\n",
      "[0.0040], 0.9820, 0.9850, 0.9650, [0.0170], 0.9280, [0.0150], 0.9910, 0.9880, 0.9530, Acc_f: 0.0120, Acc_r: 0.9703\n",
      "[train] epoch 5, batch 31, loss 0.5172455310821533\n",
      "[0.0040], 0.9820, 0.9850, 0.9650, [0.0120], 0.9260, [0.0120], 0.9920, 0.9860, 0.9530, Acc_f: 0.0093, Acc_r: 0.9699\n",
      "[train] epoch 6, batch 31, loss 0.4402763843536377\n",
      "[0.0030], 0.9820, 0.9850, 0.9630, [0.0110], 0.9240, [0.0090], 0.9920, 0.9840, 0.9540, Acc_f: 0.0077, Acc_r: 0.9691\n",
      "[train] epoch 7, batch 31, loss 0.470797061920166\n",
      "[0.0030], 0.9820, 0.9850, 0.9630, [0.0070], 0.9240, [0.0070], 0.9910, 0.9840, 0.9540, Acc_f: 0.0057, Acc_r: 0.9690\n",
      "[train] epoch 8, batch 31, loss 0.3769501745700836\n",
      "[0.0030], 0.9820, 0.9850, 0.9620, [0.0060], 0.9220, [0.0060], 0.9910, 0.9840, 0.9540, Acc_f: 0.0050, Acc_r: 0.9686\n",
      "[train] epoch 9, batch 31, loss 0.40981942415237427\n",
      "[0.0030], 0.9820, 0.9850, 0.9640, [0.0050], 0.9220, [0.0060], 0.9910, 0.9840, 0.9540, Acc_f: 0.0047, Acc_r: 0.9689\n",
      "[train] epoch 10, batch 31, loss 0.39831510186195374\n",
      "[0.0010], 0.9810, 0.9870, 0.9620, [0.0050], 0.9200, [0.0060], 0.9910, 0.9840, 0.9550, Acc_f: 0.0040, Acc_r: 0.9686\n",
      "[train] epoch 11, batch 31, loss 0.3881295919418335\n",
      "[0.0010], 0.9800, 0.9870, 0.9630, [0.0040], 0.9190, [0.0060], 0.9910, 0.9850, 0.9550, Acc_f: 0.0037, Acc_r: 0.9686\n",
      "[train] epoch 12, batch 31, loss 0.3480183482170105\n",
      "[0.0010], 0.9800, 0.9870, 0.9640, [0.0040], 0.9190, [0.0060], 0.9910, 0.9850, 0.9550, Acc_f: 0.0037, Acc_r: 0.9687\n",
      "[train] epoch 13, batch 31, loss 0.3645164370536804\n",
      "[0.0010], 0.9800, 0.9870, 0.9630, [0.0030], 0.9190, [0.0060], 0.9910, 0.9850, 0.9550, Acc_f: 0.0033, Acc_r: 0.9686\n",
      "[train] epoch 14, batch 31, loss 0.3518267273902893\n",
      "[0.0010], 0.9810, 0.9870, 0.9620, [0.0030], 0.9190, [0.0060], 0.9910, 0.9850, 0.9550, Acc_f: 0.0033, Acc_r: 0.9686\n",
      "[0.8580], 0.9760, 0.8940, 0.9400, [0.8640], 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.8377, Acc_r: 0.9566\n",
      "[train] epoch 0, batch 31, loss 2.385800361633301\n",
      "[0.5800], 0.9770, 0.9760, 0.9290, [0.5910], 0.9490, [0.6380], 0.9780, 0.9810, 0.9680, Acc_f: 0.6030, Acc_r: 0.9654\n",
      "[train] epoch 1, batch 31, loss 1.2186046838760376\n",
      "[0.2040], 0.9750, 0.9900, 0.9250, [0.2860], 0.9380, [0.3960], 0.9730, 0.9760, 0.9710, Acc_f: 0.2953, Acc_r: 0.9640\n",
      "[train] epoch 2, batch 31, loss 0.8498081564903259\n",
      "[0.1220], 0.9740, 0.9930, 0.9340, [0.1410], 0.9300, [0.2550], 0.9720, 0.9750, 0.9710, Acc_f: 0.1727, Acc_r: 0.9641\n",
      "[train] epoch 3, batch 31, loss 0.6358769536018372\n",
      "[0.0870], 0.9720, 0.9920, 0.9460, [0.0860], 0.9250, [0.1520], 0.9700, 0.9750, 0.9710, Acc_f: 0.1083, Acc_r: 0.9644\n",
      "[train] epoch 4, batch 31, loss 0.4964432716369629\n",
      "[0.0540], 0.9700, 0.9870, 0.9580, [0.0560], 0.9180, [0.0930], 0.9690, 0.9750, 0.9710, Acc_f: 0.0677, Acc_r: 0.9640\n",
      "[train] epoch 5, batch 31, loss 0.3688911497592926\n",
      "[0.0370], 0.9670, 0.9870, 0.9640, [0.0350], 0.9150, [0.0590], 0.9680, 0.9750, 0.9710, Acc_f: 0.0437, Acc_r: 0.9639\n",
      "[train] epoch 6, batch 31, loss 0.3094099164009094\n",
      "[0.0230], 0.9670, 0.9850, 0.9650, [0.0240], 0.9140, [0.0360], 0.9680, 0.9750, 0.9710, Acc_f: 0.0277, Acc_r: 0.9636\n",
      "[train] epoch 7, batch 31, loss 0.3469932973384857\n",
      "[0.0170], 0.9670, 0.9850, 0.9680, [0.0170], 0.9130, [0.0250], 0.9680, 0.9750, 0.9710, Acc_f: 0.0197, Acc_r: 0.9639\n",
      "[train] epoch 8, batch 31, loss 0.3374651074409485\n",
      "[0.0130], 0.9650, 0.9850, 0.9700, [0.0110], 0.9110, [0.0190], 0.9670, 0.9750, 0.9710, Acc_f: 0.0143, Acc_r: 0.9634\n",
      "[train] epoch 9, batch 31, loss 0.30984365940093994\n",
      "[0.0120], 0.9630, 0.9840, 0.9700, [0.0080], 0.9090, [0.0160], 0.9670, 0.9750, 0.9710, Acc_f: 0.0120, Acc_r: 0.9627\n",
      "[train] epoch 10, batch 31, loss 0.24042688310146332\n",
      "[0.0120], 0.9630, 0.9840, 0.9690, [0.0070], 0.9080, [0.0130], 0.9670, 0.9760, 0.9700, Acc_f: 0.0107, Acc_r: 0.9624\n",
      "[train] epoch 11, batch 31, loss 0.27652865648269653\n",
      "[0.0110], 0.9620, 0.9840, 0.9690, [0.0070], 0.9080, [0.0100], 0.9670, 0.9740, 0.9700, Acc_f: 0.0093, Acc_r: 0.9620\n",
      "[train] epoch 12, batch 31, loss 0.2646307945251465\n",
      "[0.0100], 0.9620, 0.9840, 0.9680, [0.0060], 0.9070, [0.0080], 0.9670, 0.9730, 0.9690, Acc_f: 0.0080, Acc_r: 0.9614\n",
      "[train] epoch 13, batch 31, loss 0.24266792833805084\n",
      "[0.0090], 0.9610, 0.9840, 0.9680, [0.0060], 0.9070, [0.0050], 0.9670, 0.9740, 0.9690, Acc_f: 0.0067, Acc_r: 0.9614\n",
      "[train] epoch 14, batch 31, loss 0.2774367928504944\n",
      "[0.0090], 0.9610, 0.9840, 0.9680, [0.0060], 0.9070, [0.0040], 0.9670, 0.9740, 0.9680, Acc_f: 0.0063, Acc_r: 0.9613\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 1.16 \\pm 0.59\n",
      "UNSC Acc_r: 96.80 \\pm 0.29\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9701, 0.9699, 0.9639])\n",
    "Acc_f = 100*np.array([0.0057, 0.0093, 0.0197])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], [0.9780], 0.9090, 0.9310, [0.8380], 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8728, Acc_r: 0.9547\n",
      "[0.8460], [0.9820], 0.9160, 0.9370, [0.8760], 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8790, Acc_r: 0.9585\n",
      "[0.8580], [0.9760], 0.8940, 0.9400, [0.8640], 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.8722, Acc_r: 0.9533\n",
      "------------ Retrained model ------------\n",
      "[0.0000], [0.0000], 0.9810, 0.9590, [0.0000], 0.9360, [0.0000], 0.9800, 0.9910, 0.9680, Acc_f: 0.0000, Acc_r: 0.9692\n",
      "[0.0000], [0.0000], 0.9820, 0.9610, [0.0000], 0.9340, [0.0000], 0.9720, 0.9900, 0.9740, Acc_f: 0.0000, Acc_r: 0.9688\n",
      "[0.0000], [0.0000], 0.9800, 0.9650, [0.0000], 0.9330, [0.0000], 0.9850, 0.9940, 0.9590, Acc_f: 0.0000, Acc_r: 0.9693\n",
      "Original model Acc_f: 87.47 \\pm 0.31\n",
      "Original model Acc_r: 95.55 \\pm 0.22\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 96.91 \\pm 0.02\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 4\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/16\n",
      "Layer 2 : 118/576\n",
      "Layer 3 : 303/512\n",
      "Layer 4 : 77/1024\n",
      "Layer 5 : 86/2048\n",
      "Layer 6 : 5/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/16\n",
      "Layer 2 : 152/576\n",
      "Layer 3 : 411/512\n",
      "Layer 4 : 157/1024\n",
      "Layer 5 : 179/2048\n",
      "Layer 6 : 11/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 231/576\n",
      "Layer 3 : 465/512\n",
      "Layer 4 : 241/1024\n",
      "Layer 5 : 261/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 263/576\n",
      "Layer 3 : 480/512\n",
      "Layer 4 : 327/1024\n",
      "Layer 5 : 336/2048\n",
      "Layer 6 : 17/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 317/576\n",
      "Layer 3 : 488/512\n",
      "Layer 4 : 416/1024\n",
      "Layer 5 : 439/2048\n",
      "Layer 6 : 32/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 365/576\n",
      "Layer 3 : 499/512\n",
      "Layer 4 : 506/1024\n",
      "Layer 5 : 542/2048\n",
      "Layer 6 : 49/2048\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/16\n",
      "Layer 2 : 123/576\n",
      "Layer 3 : 309/512\n",
      "Layer 4 : 79/1024\n",
      "Layer 5 : 85/2048\n",
      "Layer 6 : 5/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 151/576\n",
      "Layer 3 : 413/512\n",
      "Layer 4 : 160/1024\n",
      "Layer 5 : 178/2048\n",
      "Layer 6 : 11/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/16\n",
      "Layer 2 : 228/576\n",
      "Layer 3 : 467/512\n",
      "Layer 4 : 243/1024\n",
      "Layer 5 : 259/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/16\n",
      "Layer 2 : 258/576\n",
      "Layer 3 : 483/512\n",
      "Layer 4 : 328/1024\n",
      "Layer 5 : 333/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 323/576\n",
      "Layer 3 : 490/512\n",
      "Layer 4 : 418/1024\n",
      "Layer 5 : 435/2048\n",
      "Layer 6 : 32/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 373/576\n",
      "Layer 3 : 500/512\n",
      "Layer 4 : 509/1024\n",
      "Layer 5 : 537/2048\n",
      "Layer 6 : 49/2048\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/16\n",
      "Layer 2 : 107/576\n",
      "Layer 3 : 251/512\n",
      "Layer 4 : 69/1024\n",
      "Layer 5 : 62/2048\n",
      "Layer 6 : 6/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 134/576\n",
      "Layer 3 : 364/512\n",
      "Layer 4 : 142/1024\n",
      "Layer 5 : 138/2048\n",
      "Layer 6 : 13/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 212/576\n",
      "Layer 3 : 447/512\n",
      "Layer 4 : 222/1024\n",
      "Layer 5 : 207/2048\n",
      "Layer 6 : 17/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 245/576\n",
      "Layer 3 : 467/512\n",
      "Layer 4 : 302/1024\n",
      "Layer 5 : 268/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 310/576\n",
      "Layer 3 : 478/512\n",
      "Layer 4 : 387/1024\n",
      "Layer 5 : 362/2048\n",
      "Layer 6 : 37/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.997\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 11/16\n",
      "Layer 2 : 359/576\n",
      "Layer 3 : 493/512\n",
      "Layer 4 : 474/1024\n",
      "Layer 5 : 453/2048\n",
      "Layer 6 : 55/2048\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.targets)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], [0.9780], 0.9090, 0.9310, [0.8380], 0.9580, [0.8260], 0.9850, 0.9880, 0.9570, Acc_f: 0.8728, Acc_r: 0.9547\n",
      "[train] epoch 0, batch 42, loss 1.382326364517212\n",
      "[0.1160], [0.0200], 0.9560, 0.9760, [0.6420], 0.9570, [0.5340], 0.9850, 0.9890, 0.9630, Acc_f: 0.3280, Acc_r: 0.9710\n",
      "[train] epoch 1, batch 42, loss 0.6873600482940674\n",
      "[0.0310], [0.0040], 0.9730, 0.9760, [0.1330], 0.9560, [0.1190], 0.9840, 0.9890, 0.9640, Acc_f: 0.0718, Acc_r: 0.9737\n",
      "[train] epoch 2, batch 42, loss 0.5258454084396362\n",
      "[0.0130], [0.0010], 0.9750, 0.9760, [0.0530], 0.9530, [0.0570], 0.9840, 0.9900, 0.9650, Acc_f: 0.0310, Acc_r: 0.9738\n",
      "[train] epoch 3, batch 42, loss 0.5510820746421814\n",
      "[0.0090], [0.0000], 0.9770, 0.9760, [0.0220], 0.9530, [0.0400], 0.9840, 0.9900, 0.9650, Acc_f: 0.0178, Acc_r: 0.9742\n",
      "[train] epoch 4, batch 42, loss 0.4113083481788635\n",
      "[0.0050], [0.0000], 0.9780, 0.9760, [0.0110], 0.9520, [0.0320], 0.9830, 0.9900, 0.9650, Acc_f: 0.0120, Acc_r: 0.9740\n",
      "[train] epoch 5, batch 42, loss 0.4051724374294281\n",
      "[0.0050], [0.0000], 0.9780, 0.9760, [0.0070], 0.9520, [0.0250], 0.9830, 0.9900, 0.9650, Acc_f: 0.0093, Acc_r: 0.9740\n",
      "[train] epoch 6, batch 42, loss 0.4493251442909241\n",
      "[0.0040], [0.0000], 0.9780, 0.9760, [0.0020], 0.9520, [0.0180], 0.9830, 0.9890, 0.9640, Acc_f: 0.0060, Acc_r: 0.9737\n",
      "[train] epoch 7, batch 42, loss 0.3974882662296295\n",
      "[0.0020], [0.0000], 0.9780, 0.9780, [0.0020], 0.9510, [0.0140], 0.9830, 0.9890, 0.9630, Acc_f: 0.0045, Acc_r: 0.9737\n",
      "[train] epoch 8, batch 42, loss 0.3438018262386322\n",
      "[0.0020], [0.0000], 0.9780, 0.9770, [0.0020], 0.9520, [0.0100], 0.9830, 0.9890, 0.9610, Acc_f: 0.0035, Acc_r: 0.9733\n",
      "[train] epoch 9, batch 42, loss 0.3441908359527588\n",
      "[0.0020], [0.0000], 0.9770, 0.9770, [0.0010], 0.9520, [0.0080], 0.9830, 0.9890, 0.9600, Acc_f: 0.0027, Acc_r: 0.9730\n",
      "[train] epoch 10, batch 42, loss 0.3408215343952179\n",
      "[0.0010], [0.0000], 0.9770, 0.9770, [0.0010], 0.9520, [0.0060], 0.9830, 0.9890, 0.9600, Acc_f: 0.0020, Acc_r: 0.9730\n",
      "[train] epoch 11, batch 42, loss 0.25638943910598755\n",
      "[0.0010], [0.0000], 0.9770, 0.9780, [0.0010], 0.9520, [0.0050], 0.9830, 0.9890, 0.9600, Acc_f: 0.0018, Acc_r: 0.9732\n",
      "[train] epoch 12, batch 42, loss 0.33703580498695374\n",
      "[0.0000], [0.0000], 0.9770, 0.9770, [0.0010], 0.9520, [0.0050], 0.9830, 0.9890, 0.9600, Acc_f: 0.0015, Acc_r: 0.9730\n",
      "[train] epoch 13, batch 42, loss 0.30394235253334045\n",
      "[0.0000], [0.0000], 0.9780, 0.9760, [0.0010], 0.9520, [0.0040], 0.9830, 0.9890, 0.9600, Acc_f: 0.0013, Acc_r: 0.9730\n",
      "[train] epoch 14, batch 42, loss 0.27600303292274475\n",
      "[0.0000], [0.0000], 0.9780, 0.9760, [0.0010], 0.9530, [0.0040], 0.9840, 0.9890, 0.9590, Acc_f: 0.0013, Acc_r: 0.9732\n",
      "[0.8460], [0.9820], 0.9160, 0.9370, [0.8760], 0.9640, [0.8120], 0.9880, 0.9850, 0.9610, Acc_f: 0.8790, Acc_r: 0.9585\n",
      "[train] epoch 0, batch 42, loss 1.0895923376083374\n",
      "[0.0000], [0.0010], 0.9820, 0.9570, [0.0160], 0.9070, [0.0090], 0.9900, 0.9820, 0.9550, Acc_f: 0.0065, Acc_r: 0.9622\n",
      "[train] epoch 1, batch 42, loss 0.5199585556983948\n",
      "[0.0000], [0.0010], 0.9870, 0.9660, [0.0020], 0.9240, [0.0040], 0.9880, 0.9840, 0.9610, Acc_f: 0.0018, Acc_r: 0.9683\n",
      "[train] epoch 2, batch 42, loss 0.4333127737045288\n",
      "[0.0000], [0.0000], 0.9870, 0.9730, [0.0020], 0.9360, [0.0030], 0.9880, 0.9840, 0.9620, Acc_f: 0.0013, Acc_r: 0.9717\n",
      "[train] epoch 3, batch 42, loss 0.5274847149848938\n",
      "[0.0000], [0.0000], 0.9880, 0.9680, [0.0010], 0.9370, [0.0030], 0.9880, 0.9820, 0.9600, Acc_f: 0.0010, Acc_r: 0.9705\n",
      "[train] epoch 4, batch 42, loss 0.4133588671684265\n",
      "[0.0000], [0.0000], 0.9880, 0.9730, [0.0010], 0.9400, [0.0030], 0.9890, 0.9820, 0.9610, Acc_f: 0.0010, Acc_r: 0.9722\n",
      "[train] epoch 5, batch 42, loss 0.2876504063606262\n",
      "[0.0000], [0.0000], 0.9880, 0.9710, [0.0000], 0.9380, [0.0030], 0.9900, 0.9830, 0.9600, Acc_f: 0.0008, Acc_r: 0.9717\n",
      "[train] epoch 6, batch 42, loss 0.26598480343818665\n",
      "[0.0000], [0.0000], 0.9880, 0.9720, [0.0010], 0.9410, [0.0030], 0.9900, 0.9820, 0.9610, Acc_f: 0.0010, Acc_r: 0.9723\n",
      "[train] epoch 7, batch 42, loss 0.32132086157798767\n",
      "[0.0000], [0.0000], 0.9880, 0.9730, [0.0010], 0.9400, [0.0030], 0.9910, 0.9850, 0.9580, Acc_f: 0.0010, Acc_r: 0.9725\n",
      "[train] epoch 8, batch 42, loss 0.40577012300491333\n",
      "[0.0000], [0.0000], 0.9880, 0.9730, [0.0010], 0.9390, [0.0030], 0.9910, 0.9840, 0.9590, Acc_f: 0.0010, Acc_r: 0.9723\n",
      "[train] epoch 9, batch 42, loss 0.2714749872684479\n",
      "[0.0000], [0.0000], 0.9880, 0.9720, [0.0010], 0.9410, [0.0030], 0.9910, 0.9830, 0.9580, Acc_f: 0.0010, Acc_r: 0.9722\n",
      "[train] epoch 10, batch 42, loss 0.29505085945129395\n",
      "[0.0000], [0.0000], 0.9880, 0.9720, [0.0010], 0.9430, [0.0030], 0.9920, 0.9830, 0.9570, Acc_f: 0.0010, Acc_r: 0.9725\n",
      "[train] epoch 11, batch 42, loss 0.27681201696395874\n",
      "[0.0000], [0.0000], 0.9880, 0.9710, [0.0010], 0.9420, [0.0030], 0.9920, 0.9830, 0.9580, Acc_f: 0.0010, Acc_r: 0.9723\n",
      "[train] epoch 12, batch 42, loss 0.260284960269928\n",
      "[0.0000], [0.0000], 0.9870, 0.9720, [0.0010], 0.9450, [0.0020], 0.9920, 0.9830, 0.9570, Acc_f: 0.0008, Acc_r: 0.9727\n",
      "[train] epoch 13, batch 42, loss 0.2937214970588684\n",
      "[0.0000], [0.0000], 0.9880, 0.9670, [0.0010], 0.9440, [0.0020], 0.9920, 0.9820, 0.9550, Acc_f: 0.0008, Acc_r: 0.9713\n",
      "[train] epoch 14, batch 42, loss 0.2345839887857437\n",
      "[0.0000], [0.0000], 0.9870, 0.9710, [0.0010], 0.9440, [0.0020], 0.9920, 0.9840, 0.9570, Acc_f: 0.0008, Acc_r: 0.9725\n",
      "[0.8580], [0.9760], 0.8940, 0.9400, [0.8640], 0.9570, [0.7910], 0.9830, 0.9850, 0.9610, Acc_f: 0.8722, Acc_r: 0.9533\n",
      "[train] epoch 0, batch 42, loss 0.6787095665931702\n",
      "[0.0150], [0.0060], 0.9840, 0.9600, [0.0890], 0.9200, [0.1440], 0.9610, 0.9900, 0.9780, Acc_f: 0.0635, Acc_r: 0.9655\n",
      "[train] epoch 1, batch 42, loss 0.3596137762069702\n",
      "[0.0060], [0.0020], 0.9850, 0.9650, [0.0070], 0.9190, [0.0120], 0.9590, 0.9800, 0.9800, Acc_f: 0.0067, Acc_r: 0.9647\n",
      "[train] epoch 2, batch 42, loss 0.3543928861618042\n",
      "[0.0020], [0.0020], 0.9840, 0.9670, [0.0030], 0.9180, [0.0040], 0.9580, 0.9820, 0.9810, Acc_f: 0.0027, Acc_r: 0.9650\n",
      "[train] epoch 3, batch 42, loss 0.3257165849208832\n",
      "[0.0010], [0.0020], 0.9840, 0.9690, [0.0020], 0.9190, [0.0010], 0.9580, 0.9820, 0.9810, Acc_f: 0.0015, Acc_r: 0.9655\n",
      "[train] epoch 4, batch 42, loss 0.35707688331604004\n",
      "[0.0010], [0.0010], 0.9850, 0.9680, [0.0010], 0.9230, [0.0010], 0.9580, 0.9810, 0.9810, Acc_f: 0.0010, Acc_r: 0.9660\n",
      "[train] epoch 5, batch 42, loss 0.327644944190979\n",
      "[0.0010], [0.0010], 0.9850, 0.9690, [0.0010], 0.9250, [0.0010], 0.9590, 0.9820, 0.9800, Acc_f: 0.0010, Acc_r: 0.9667\n",
      "[train] epoch 6, batch 42, loss 0.2081671506166458\n",
      "[0.0010], [0.0010], 0.9850, 0.9670, [0.0010], 0.9260, [0.0010], 0.9590, 0.9820, 0.9790, Acc_f: 0.0010, Acc_r: 0.9663\n",
      "[train] epoch 7, batch 42, loss 0.279092013835907\n",
      "[0.0010], [0.0010], 0.9860, 0.9660, [0.0000], 0.9260, [0.0010], 0.9590, 0.9850, 0.9790, Acc_f: 0.0008, Acc_r: 0.9668\n",
      "[train] epoch 8, batch 42, loss 0.22389991581439972\n",
      "[0.0010], [0.0010], 0.9860, 0.9660, [0.0000], 0.9270, [0.0000], 0.9600, 0.9840, 0.9770, Acc_f: 0.0005, Acc_r: 0.9667\n",
      "[train] epoch 9, batch 42, loss 0.3450276553630829\n",
      "[0.0010], [0.0010], 0.9860, 0.9670, [0.0000], 0.9300, [0.0000], 0.9600, 0.9850, 0.9770, Acc_f: 0.0005, Acc_r: 0.9675\n",
      "[train] epoch 10, batch 42, loss 0.27509409189224243\n",
      "[0.0010], [0.0010], 0.9860, 0.9660, [0.0000], 0.9320, [0.0000], 0.9600, 0.9850, 0.9770, Acc_f: 0.0005, Acc_r: 0.9677\n",
      "[train] epoch 11, batch 42, loss 0.3784233629703522\n",
      "[0.0010], [0.0010], 0.9860, 0.9660, [0.0000], 0.9320, [0.0000], 0.9600, 0.9860, 0.9770, Acc_f: 0.0005, Acc_r: 0.9678\n",
      "[train] epoch 12, batch 42, loss 0.22925446927547455\n",
      "[0.0000], [0.0010], 0.9860, 0.9680, [0.0000], 0.9340, [0.0000], 0.9610, 0.9860, 0.9760, Acc_f: 0.0003, Acc_r: 0.9685\n",
      "[train] epoch 13, batch 42, loss 0.1941591054201126\n",
      "[0.0000], [0.0010], 0.9860, 0.9680, [0.0000], 0.9350, [0.0000], 0.9630, 0.9860, 0.9760, Acc_f: 0.0003, Acc_r: 0.9690\n",
      "[train] epoch 14, batch 42, loss 0.278984010219574\n",
      "[0.0000], [0.0000], 0.9860, 0.9660, [0.0000], 0.9360, [0.0000], 0.9620, 0.9870, 0.9760, Acc_f: 0.0000, Acc_r: 0.9688\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.35 \\pm 0.41\n",
      "UNSC Acc_r: 97.19 \\pm 0.21\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9740, 0.9727, 0.9690])\n",
    "Acc_f = 100*np.array([0.0093, 0.0008, 0.0003])\n",
    "\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearn 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], [0.9780], 0.9090, 0.9310, [0.8380], 0.9580, [0.8260], 0.9850, 0.9880, [0.9570], Acc_f: 0.8896, Acc_r: 0.9542\n",
      "[0.8460], [0.9820], 0.9160, 0.9370, [0.8760], 0.9640, [0.8120], 0.9880, 0.9850, [0.9610], Acc_f: 0.8954, Acc_r: 0.9580\n",
      "[0.8580], [0.9760], 0.8940, 0.9400, [0.8640], 0.9570, [0.7910], 0.9830, 0.9850, [0.9610], Acc_f: 0.8900, Acc_r: 0.9518\n",
      "------------ Retrained model ------------\n",
      "[0.0000], [0.0000], 0.9900, 0.8950, [0.0000], 0.8220, [0.0000], 0.9820, 0.9720, [0.0000], Acc_f: 0.0000, Acc_r: 0.9322\n",
      "[0.0000], [0.0000], 0.9630, 0.9560, [0.0000], 0.9350, [0.0000], 0.9840, 0.8570, [0.0000], Acc_f: 0.0000, Acc_r: 0.9390\n",
      "[0.0000], [0.0000], 0.9800, 0.9580, [0.0000], 0.9480, [0.0000], 0.9990, 0.9960, [0.0000], Acc_f: 0.0000, Acc_r: 0.9762\n",
      "Original model Acc_f: 89.17 \\pm 0.26\n",
      "Original model Acc_r: 95.47 \\pm 0.26\n",
      "Retrained model Acc_f: 0.00 \\pm 0.00\n",
      "Retrained model Acc_r: 94.91 \\pm 1.93\n"
     ]
    }
   ],
   "source": [
    "num_unlearn = 5\n",
    "arxiv_name = dict[num_unlearn]['arxiv_name']\n",
    "args.unlearn_class = dict[num_unlearn]['unlean_class']\n",
    "Acc_r, Acc_f = np.zeros((2,3)), np.zeros((2,3))\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    Acc_f[0][i], Acc_r[0][i] = test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "        Acc_f[1][i], Acc_r[1][i] = test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break\n",
    "\n",
    "print(f'Original model Acc_f: {100*Acc_f[0].mean():.2f} \\pm {100*Acc_f[0].std():.2f}')\n",
    "print(f'Original model Acc_r: {100*Acc_r[0].mean():.2f} \\pm {100*Acc_r[0].std():.2f}')\n",
    "\n",
    "print(f'Retrained model Acc_f: {100*Acc_f[1].mean():.2f} \\pm {100*Acc_f[1].std():.2f}')\n",
    "print(f'Retrained model Acc_r: {100*Acc_r[1].mean():.2f} \\pm {100*Acc_r[1].std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Trail 0 ------------\n",
      "Threshold:  0.976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 4/16\n",
      "Layer 2 : 125/576\n",
      "Layer 3 : 318/512\n",
      "Layer 4 : 78/1024\n",
      "Layer 5 : 83/2048\n",
      "Layer 6 : 5/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 146/576\n",
      "Layer 3 : 412/512\n",
      "Layer 4 : 158/1024\n",
      "Layer 5 : 176/2048\n",
      "Layer 6 : 11/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/16\n",
      "Layer 2 : 227/576\n",
      "Layer 3 : 464/512\n",
      "Layer 4 : 244/1024\n",
      "Layer 5 : 261/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/16\n",
      "Layer 2 : 257/576\n",
      "Layer 3 : 479/512\n",
      "Layer 4 : 329/1024\n",
      "Layer 5 : 335/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 318/576\n",
      "Layer 3 : 488/512\n",
      "Layer 4 : 418/1024\n",
      "Layer 5 : 438/2048\n",
      "Layer 6 : 33/2048\n",
      "----------------------------------------\n",
      "------------ Trail 1 ------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 130/576\n",
      "Layer 3 : 319/512\n",
      "Layer 4 : 78/1024\n",
      "Layer 5 : 86/2048\n",
      "Layer 6 : 5/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 155/576\n",
      "Layer 3 : 416/512\n",
      "Layer 4 : 160/1024\n",
      "Layer 5 : 181/2048\n",
      "Layer 6 : 11/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/16\n",
      "Layer 2 : 226/576\n",
      "Layer 3 : 467/512\n",
      "Layer 4 : 245/1024\n",
      "Layer 5 : 262/2048\n",
      "Layer 6 : 14/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 8/16\n",
      "Layer 2 : 257/576\n",
      "Layer 3 : 482/512\n",
      "Layer 4 : 330/1024\n",
      "Layer 5 : 333/2048\n",
      "Layer 6 : 16/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 318/576\n",
      "Layer 3 : 489/512\n",
      "Layer 4 : 419/1024\n",
      "Layer 5 : 434/2048\n",
      "Layer 6 : 30/2048\n",
      "----------------------------------------\n",
      "------------ Trail 2 ------------\n",
      "Threshold:  0.976\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 111/576\n",
      "Layer 3 : 256/512\n",
      "Layer 4 : 69/1024\n",
      "Layer 5 : 64/2048\n",
      "Layer 6 : 6/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.979\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/16\n",
      "Layer 2 : 139/576\n",
      "Layer 3 : 372/512\n",
      "Layer 4 : 144/1024\n",
      "Layer 5 : 143/2048\n",
      "Layer 6 : 13/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.985\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 214/576\n",
      "Layer 3 : 449/512\n",
      "Layer 4 : 224/1024\n",
      "Layer 5 : 211/2048\n",
      "Layer 6 : 17/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.991\n",
      "Skip Updating GPM for layer: 1\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 9/16\n",
      "Layer 2 : 247/576\n",
      "Layer 3 : 466/512\n",
      "Layer 4 : 305/1024\n",
      "Layer 5 : 271/2048\n",
      "Layer 6 : 19/2048\n",
      "----------------------------------------\n",
      "Threshold:  0.994\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 10/16\n",
      "Layer 2 : 311/576\n",
      "Layer 3 : 479/512\n",
      "Layer 4 : 391/1024\n",
      "Layer 5 : 365/2048\n",
      "Layer 6 : 34/2048\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "Proj_mat_lst =[]\n",
    "train_targets_list = np.array(train_loader.dataset.targets)[train_loader.sampler.indices]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    print(f'------------ Trail {i} ------------')\n",
    "    merged_feat_mat = []\n",
    "    for cls_id in range(10): \n",
    "        cls_indices = np.where(np.isin(train_targets_list, cls_id))[0]\n",
    "        cls_indices = train_loader.sampler.indices[cls_indices]\n",
    "        cls_sampler = torch.utils.data.SubsetRandomSampler(cls_indices)\n",
    "        cls_loader_dict = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                                batch_size=args.batch_size, \n",
    "                                                                sampler=cls_sampler)\n",
    "        if cls_id in args.unlearn_class:\n",
    "            continue\n",
    "        for batch, (x, y) in enumerate(cls_loader_dict ):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            mat_list = get_representation_matrix(model, \n",
    "                                                x, \n",
    "                                                batch_list=[24, 100, 100, 100, 125, 125, 125, 250, 250, 256, 256])\n",
    "            break\n",
    "        threshold = 0.97 + 0.003*cls_id\n",
    "        merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "        proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "        Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8490], [0.9780], 0.9090, 0.9310, [0.8380], 0.9580, [0.8260], 0.9850, 0.9880, [0.9570], Acc_f: 0.8896, Acc_r: 0.9542\n",
      "[train] epoch 0, batch 52, loss 1.0092248916625977\n",
      "[0.1350], [0.0240], 0.9600, 0.9760, [0.6360], 0.9630, [0.5310], 1.0000, 0.9890, [0.1110], Acc_f: 0.2874, Acc_r: 0.9776\n",
      "[train] epoch 1, batch 52, loss 0.5859055519104004\n",
      "[0.0310], [0.0060], 0.9730, 0.9760, [0.1400], 0.9650, [0.0910], 0.9990, 0.9890, [0.0080], Acc_f: 0.0552, Acc_r: 0.9804\n",
      "[train] epoch 2, batch 52, loss 0.4705379009246826\n",
      "[0.0160], [0.0040], 0.9720, 0.9770, [0.0580], 0.9650, [0.0430], 0.9990, 0.9890, [0.0010], Acc_f: 0.0244, Acc_r: 0.9804\n",
      "[train] epoch 3, batch 52, loss 0.42829957604408264\n",
      "[0.0110], [0.0020], 0.9750, 0.9770, [0.0310], 0.9640, [0.0280], 0.9990, 0.9890, [0.0000], Acc_f: 0.0144, Acc_r: 0.9808\n",
      "[train] epoch 4, batch 52, loss 0.38154730200767517\n",
      "[0.0060], [0.0020], 0.9750, 0.9770, [0.0140], 0.9630, [0.0190], 0.9990, 0.9890, [0.0000], Acc_f: 0.0082, Acc_r: 0.9806\n",
      "[train] epoch 5, batch 52, loss 0.4136338531970978\n",
      "[0.0050], [0.0010], 0.9750, 0.9770, [0.0070], 0.9630, [0.0140], 0.9990, 0.9890, [0.0000], Acc_f: 0.0054, Acc_r: 0.9806\n",
      "[train] epoch 6, batch 52, loss 0.35197874903678894\n",
      "[0.0040], [0.0010], 0.9750, 0.9770, [0.0060], 0.9620, [0.0110], 0.9990, 0.9890, [0.0000], Acc_f: 0.0044, Acc_r: 0.9804\n",
      "[train] epoch 7, batch 52, loss 0.3741694390773773\n",
      "[0.0030], [0.0010], 0.9750, 0.9770, [0.0030], 0.9620, [0.0080], 0.9990, 0.9890, [0.0000], Acc_f: 0.0030, Acc_r: 0.9804\n",
      "[train] epoch 8, batch 52, loss 0.29840290546417236\n",
      "[0.0030], [0.0010], 0.9750, 0.9770, [0.0020], 0.9620, [0.0050], 1.0000, 0.9890, [0.0000], Acc_f: 0.0022, Acc_r: 0.9806\n",
      "[train] epoch 9, batch 52, loss 0.30176541209220886\n",
      "[0.0020], [0.0010], 0.9760, 0.9770, [0.0000], 0.9620, [0.0050], 1.0000, 0.9890, [0.0000], Acc_f: 0.0016, Acc_r: 0.9808\n",
      "[train] epoch 10, batch 52, loss 0.25782909989356995\n",
      "[0.0020], [0.0010], 0.9750, 0.9770, [0.0000], 0.9620, [0.0050], 1.0000, 0.9890, [0.0000], Acc_f: 0.0016, Acc_r: 0.9806\n",
      "[train] epoch 11, batch 52, loss 0.2881695032119751\n",
      "[0.0020], [0.0010], 0.9750, 0.9770, [0.0000], 0.9620, [0.0040], 1.0000, 0.9880, [0.0000], Acc_f: 0.0014, Acc_r: 0.9804\n",
      "[train] epoch 12, batch 52, loss 0.3167096972465515\n",
      "[0.0010], [0.0000], 0.9760, 0.9770, [0.0000], 0.9620, [0.0040], 1.0000, 0.9880, [0.0000], Acc_f: 0.0010, Acc_r: 0.9806\n",
      "[train] epoch 13, batch 52, loss 0.2688273787498474\n",
      "[0.0000], [0.0000], 0.9770, 0.9780, [0.0000], 0.9620, [0.0040], 1.0000, 0.9880, [0.0000], Acc_f: 0.0008, Acc_r: 0.9810\n",
      "[train] epoch 14, batch 52, loss 0.25685107707977295\n",
      "[0.0000], [0.0000], 0.9770, 0.9780, [0.0000], 0.9600, [0.0030], 1.0000, 0.9880, [0.0000], Acc_f: 0.0006, Acc_r: 0.9806\n",
      "[0.8460], [0.9820], 0.9160, 0.9370, [0.8760], 0.9640, [0.8120], 0.9880, 0.9850, [0.9610], Acc_f: 0.8954, Acc_r: 0.9580\n",
      "[train] epoch 0, batch 52, loss 0.6950298547744751\n",
      "[0.0010], [0.0000], 0.9830, 0.9390, [0.0100], 0.9010, [0.0090], 1.0000, 0.9780, [0.0020], Acc_f: 0.0044, Acc_r: 0.9602\n",
      "[train] epoch 1, batch 52, loss 0.38550853729248047\n",
      "[0.0010], [0.0010], 0.9840, 0.9670, [0.0010], 0.9500, [0.0040], 1.0000, 0.9800, [0.0010], Acc_f: 0.0016, Acc_r: 0.9762\n",
      "[train] epoch 2, batch 52, loss 0.3872988820075989\n",
      "[0.0010], [0.0010], 0.9870, 0.9650, [0.0010], 0.9540, [0.0040], 1.0000, 0.9800, [0.0000], Acc_f: 0.0014, Acc_r: 0.9772\n",
      "[train] epoch 3, batch 52, loss 0.3775535225868225\n",
      "[0.0010], [0.0000], 0.9860, 0.9670, [0.0000], 0.9550, [0.0040], 1.0000, 0.9810, [0.0000], Acc_f: 0.0010, Acc_r: 0.9778\n",
      "[train] epoch 4, batch 52, loss 0.3993874788284302\n",
      "[0.0010], [0.0000], 0.9870, 0.9670, [0.0000], 0.9540, [0.0030], 1.0000, 0.9830, [0.0000], Acc_f: 0.0008, Acc_r: 0.9782\n",
      "[train] epoch 5, batch 52, loss 0.28714311122894287\n",
      "[0.0000], [0.0000], 0.9880, 0.9670, [0.0000], 0.9550, [0.0030], 1.0000, 0.9820, [0.0000], Acc_f: 0.0006, Acc_r: 0.9784\n",
      "[train] epoch 6, batch 52, loss 0.2937231659889221\n",
      "[0.0000], [0.0000], 0.9860, 0.9690, [0.0000], 0.9540, [0.0030], 1.0000, 0.9820, [0.0000], Acc_f: 0.0006, Acc_r: 0.9782\n",
      "[train] epoch 7, batch 52, loss 0.28822264075279236\n",
      "[0.0000], [0.0000], 0.9870, 0.9690, [0.0000], 0.9550, [0.0030], 1.0000, 0.9820, [0.0000], Acc_f: 0.0006, Acc_r: 0.9786\n",
      "[train] epoch 8, batch 52, loss 0.2816002666950226\n",
      "[0.0000], [0.0000], 0.9860, 0.9700, [0.0000], 0.9540, [0.0030], 1.0000, 0.9820, [0.0000], Acc_f: 0.0006, Acc_r: 0.9784\n",
      "[train] epoch 9, batch 52, loss 0.24671314656734467\n",
      "[0.0000], [0.0000], 0.9860, 0.9670, [0.0000], 0.9530, [0.0030], 1.0000, 0.9820, [0.0000], Acc_f: 0.0006, Acc_r: 0.9776\n",
      "[train] epoch 10, batch 52, loss 0.2595643997192383\n",
      "[0.0000], [0.0000], 0.9860, 0.9700, [0.0000], 0.9550, [0.0030], 1.0000, 0.9830, [0.0000], Acc_f: 0.0006, Acc_r: 0.9788\n",
      "[train] epoch 11, batch 52, loss 0.226190984249115\n",
      "[0.0000], [0.0000], 0.9870, 0.9660, [0.0000], 0.9540, [0.0030], 1.0000, 0.9830, [0.0000], Acc_f: 0.0006, Acc_r: 0.9780\n",
      "[train] epoch 12, batch 52, loss 0.27280953526496887\n",
      "[0.0000], [0.0000], 0.9860, 0.9680, [0.0000], 0.9530, [0.0030], 1.0000, 0.9830, [0.0000], Acc_f: 0.0006, Acc_r: 0.9780\n",
      "[train] epoch 13, batch 52, loss 0.240848109126091\n",
      "[0.0000], [0.0000], 0.9860, 0.9690, [0.0000], 0.9540, [0.0020], 1.0000, 0.9830, [0.0000], Acc_f: 0.0004, Acc_r: 0.9784\n",
      "[train] epoch 14, batch 52, loss 0.21308885514736176\n",
      "[0.0000], [0.0000], 0.9860, 0.9670, [0.0000], 0.9540, [0.0020], 1.0000, 0.9840, [0.0000], Acc_f: 0.0004, Acc_r: 0.9782\n",
      "[0.8580], [0.9760], 0.8940, 0.9400, [0.8640], 0.9570, [0.7910], 0.9830, 0.9850, [0.9610], Acc_f: 0.8900, Acc_r: 0.9518\n",
      "[train] epoch 0, batch 52, loss 0.630524754524231\n",
      "[0.0200], [0.0070], 0.9880, 0.9510, [0.1180], 0.9170, [0.1480], 1.0000, 0.9850, [0.0700], Acc_f: 0.0726, Acc_r: 0.9682\n",
      "[train] epoch 1, batch 52, loss 0.35939309000968933\n",
      "[0.0090], [0.0030], 0.9850, 0.9670, [0.0080], 0.9410, [0.0170], 1.0000, 0.9820, [0.0040], Acc_f: 0.0082, Acc_r: 0.9750\n",
      "[train] epoch 2, batch 52, loss 0.3139919340610504\n",
      "[0.0050], [0.0020], 0.9860, 0.9680, [0.0040], 0.9460, [0.0040], 1.0000, 0.9780, [0.0000], Acc_f: 0.0030, Acc_r: 0.9756\n",
      "[train] epoch 3, batch 52, loss 0.33718064427375793\n",
      "[0.0020], [0.0020], 0.9850, 0.9680, [0.0020], 0.9480, [0.0010], 1.0000, 0.9790, [0.0000], Acc_f: 0.0014, Acc_r: 0.9760\n",
      "[train] epoch 4, batch 52, loss 0.2779383957386017\n",
      "[0.0010], [0.0020], 0.9850, 0.9680, [0.0020], 0.9500, [0.0010], 1.0000, 0.9790, [0.0000], Acc_f: 0.0012, Acc_r: 0.9764\n",
      "[train] epoch 5, batch 52, loss 0.2568967044353485\n",
      "[0.0000], [0.0010], 0.9850, 0.9690, [0.0010], 0.9490, [0.0010], 1.0000, 0.9790, [0.0000], Acc_f: 0.0006, Acc_r: 0.9764\n",
      "[train] epoch 6, batch 52, loss 0.2637025713920593\n",
      "[0.0000], [0.0010], 0.9850, 0.9680, [0.0010], 0.9480, [0.0010], 1.0000, 0.9790, [0.0000], Acc_f: 0.0006, Acc_r: 0.9760\n",
      "[train] epoch 7, batch 52, loss 0.22976432740688324\n",
      "[0.0000], [0.0000], 0.9850, 0.9680, [0.0010], 0.9480, [0.0010], 1.0000, 0.9800, [0.0000], Acc_f: 0.0004, Acc_r: 0.9762\n",
      "[train] epoch 8, batch 52, loss 0.2680216431617737\n",
      "[0.0000], [0.0000], 0.9860, 0.9670, [0.0010], 0.9480, [0.0000], 1.0000, 0.9820, [0.0000], Acc_f: 0.0002, Acc_r: 0.9766\n",
      "[train] epoch 9, batch 52, loss 0.22816407680511475\n",
      "[0.0000], [0.0000], 0.9860, 0.9640, [0.0010], 0.9480, [0.0000], 1.0000, 0.9820, [0.0000], Acc_f: 0.0002, Acc_r: 0.9760\n",
      "[train] epoch 10, batch 52, loss 0.2311047613620758\n",
      "[0.0000], [0.0000], 0.9860, 0.9670, [0.0000], 0.9470, [0.0000], 1.0000, 0.9820, [0.0000], Acc_f: 0.0000, Acc_r: 0.9764\n",
      "[train] epoch 11, batch 52, loss 0.18091563880443573\n",
      "[0.0000], [0.0000], 0.9860, 0.9680, [0.0000], 0.9490, [0.0000], 1.0000, 0.9820, [0.0000], Acc_f: 0.0000, Acc_r: 0.9770\n",
      "[train] epoch 12, batch 52, loss 0.18766966462135315\n",
      "[0.0000], [0.0000], 0.9860, 0.9680, [0.0000], 0.9480, [0.0000], 1.0000, 0.9820, [0.0000], Acc_f: 0.0000, Acc_r: 0.9768\n",
      "[train] epoch 13, batch 52, loss 0.233585387468338\n",
      "[0.0000], [0.0000], 0.9860, 0.9660, [0.0000], 0.9480, [0.0000], 1.0000, 0.9830, [0.0000], Acc_f: 0.0000, Acc_r: 0.9766\n",
      "[train] epoch 14, batch 52, loss 0.22107627987861633\n",
      "[0.0000], [0.0000], 0.9860, 0.9660, [0.0000], 0.9480, [0.0000], 1.0000, 0.9830, [0.0000], Acc_f: 0.0000, Acc_r: 0.9766\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_label(args, model, x):\n",
    "    masked_output = model(x)\n",
    "    masked_output[:, args.unlearn_class] = -np.inf\n",
    "    pseudo_labels = torch.topk(masked_output, k=1, dim=1).indices\n",
    "    return pseudo_labels.reshape(-1)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/original_model_{arxiv_name_o}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.005)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_pseudo_label(args, model, x)\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSC Acc_f: 0.04 \\pm 0.02\n",
      "UNSC Acc_r: 97.88 \\pm 0.15\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9806, 0.9788, 0.9770])\n",
    "Acc_f = 100*np.array([0.0006, 0.0006, 0.0001])\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
